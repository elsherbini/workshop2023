[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Please install R and RStudio on your laptop. If you already have R and Rstudio installed, please make sure they are up-to date. Please install R version 4.3.1 and RStudio version 2023.09.0 Click here for instructions on installing R and RStudio"
  },
  {
    "objectID": "resources.html#before-the-workshop-installing-r-and-rstudio",
    "href": "resources.html#before-the-workshop-installing-r-and-rstudio",
    "title": "Resources",
    "section": "",
    "text": "Please install R and RStudio on your laptop. If you already have R and Rstudio installed, please make sure they are up-to date. Please install R version 4.3.1 and RStudio version 2023.09.0 Click here for instructions on installing R and RStudio"
  },
  {
    "objectID": "resources.html#textbooks",
    "href": "resources.html#textbooks",
    "title": "Resources",
    "section": "Textbooks",
    "text": "Textbooks\n\nR for Data Science: Import, Tidy, Transform, Visualize, and Model Data by Garrett Grolemund and Hadley Wickham\nLink"
  },
  {
    "objectID": "resources.html#definitions-from-glosario",
    "href": "resources.html#definitions-from-glosario",
    "title": "Resources",
    "section": "Definitions from Glosario",
    "text": "Definitions from Glosario\nFurther definitions at Glosario\nArgument: one of possibly several expressions that are passed to a function. Oftentimes parameter and arugument are used interchangably, even though techincally parameter refers to the variable and argument refers to the value.\nAssignment operator: Symbol that assigns values on the right to an object on the left. Looks like &lt;-. Keyboard shortcut is Alt + -\nComment: Text written in a script that is not treated as code to be run, but rather as text that describes what the code is doing. These are usually short notes, beginning with a #\nComprehensive R Archive Network (CRAN): A public repository of R packages.\nData frame: A two-dimensional data structure for storing tabular data in memory. Rows represent records and columns represent variables.\nFunction: A code block which gathers a sequence of operations into a whole, preserving it for ongoing use by defining a set of tasks that takes zero or more required and optional arguments as inputs and returns expected outputs (return values), if any. Functions enable repeating these defined tasks with one command, known as a function call.\nNA: A special value used to represent data that is not available.\nPipe operator: The %&gt;% used to make the output of one function the input of the next.\nPackage: A collection of code, data, and documentation that can be distributed and re-used. Also referred to in some languages as a library or module.\nParameter: A variable specified in a function definition whose value is passed to the function when the function is called. Parameters and arguments are distinct, but related concepts. Parameters are variables and arguments are the values assigned to those variables. In practice though these terms are often used interchangeably.\nPositional argument: An argument to a function that gets its value according to its place in the function’s definition, as opposed to a named argument that is explicitly matched by name.\nReproducible research: The practice of describing and documenting research results in such a way that another researcher or person can re-run the analysis code on the same data to obtain the same result.\nTibble: A modern replacement for R’s data frame, which stores tabular data in columns and rows, defined and used in the tidyverse. Almost always when you are working with a data frame, you are actually working with a tibble.\nTidy data: Tabular data that satisfies three conditions that facilitate initial cleaning, and later exploration and analysis—(1) each variable forms a column, (2) each observation forms a row, and (3) each type of observation unit forms a table.\nTidyverse: A collection of R packages for operating on tabular data in consistent ways.\nVariable: A name in a program that has some data associated with it. A variable’s value can be changed after definition.\nVector: A sequence of values that have all the same type. Vectors are the fundamental data structure in R; a scalar is just a vector with exactly one element."
  },
  {
    "objectID": "materials/3-workshop3/genomics_adventure_wrapper/index.html",
    "href": "materials/3-workshop3/genomics_adventure_wrapper/index.html",
    "title": "Genomics Adventure in Durban",
    "section": "",
    "text": "The Genomics Adventure module has been run many times by Guy Leonard in a workshop held in Cesky Krumlov. Now the adventure continues in Durban, SA.",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Interactive 1</b>:Genomics Adventure"
    ]
  },
  {
    "objectID": "materials/3-workshop3/genomics_adventure_wrapper/index.html#before-we-start-run-the-following-command",
    "href": "materials/3-workshop3/genomics_adventure_wrapper/index.html#before-we-start-run-the-following-command",
    "title": "Genomics Adventure in Durban",
    "section": "Before we start, run the following command:",
    "text": "Before we start, run the following command:\ncd /home/genomics/workshop_materials/genomics_adventure/;\nmv sequencing_data .sequencing_data_backup;\nmv reference_sequences .reference_sequences_backup;\nmkdir -p sequencing_data/ecoli;\nmkdir -p reference_sequences/ecoli;\ncp .sequencing_data_backup/ecoli/*SRR857279* sequencing_data/ecoli;\ncp .reference_sequences_backup/ecoli/GCF_000005845.2_ASM584v2_genomic.fna reference_sequences/ecoli;\ncp .reference_sequences_backup/ecoli/GCF_000005845.2_ASM584v2_genomic.gff reference_sequences/ecoli;",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Interactive 1</b>:Genomics Adventure"
    ]
  },
  {
    "objectID": "materials/3-workshop3/genomics_adventure_wrapper/index.html#now-follow-the-adventure-here",
    "href": "materials/3-workshop3/genomics_adventure_wrapper/index.html#now-follow-the-adventure-here",
    "title": "Genomics Adventure in Durban",
    "section": "Now follow the adventure here:",
    "text": "Now follow the adventure here:\nhttps://github.com/guyleonard/genomics_adventure/blob/release/README.md#a-few-notes-on-styles",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Interactive 1</b>:Genomics Adventure"
    ]
  },
  {
    "objectID": "materials/3-workshop3/genomics_adventure_wrapper/index.html#to-download-your-fastqc-results",
    "href": "materials/3-workshop3/genomics_adventure_wrapper/index.html#to-download-your-fastqc-results",
    "title": "Genomics Adventure in Durban",
    "section": "to download your fastqc results",
    "text": "to download your fastqc results\n# replace &lt;your-ip-address&gt; with the correct IP address\nscp genomics@&lt;your-ip-address&gt;:`/home/genomics/workshop_materials/genomics_adventure/sequencing_data/ecoli/*.html` ./",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Interactive 1</b>:Genomics Adventure"
    ]
  },
  {
    "objectID": "materials/3-workshop3/7-best-practices-and-send-off/index.html",
    "href": "materials/3-workshop3/7-best-practices-and-send-off/index.html",
    "title": "Best Practices and Send-Off",
    "section": "",
    "text": "Say goodbye to the workshop and reflect on your new-found superpowers",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 7</b>:Best Practices"
    ]
  },
  {
    "objectID": "materials/3-workshop3/7-best-practices-and-send-off/index.html#slides",
    "href": "materials/3-workshop3/7-best-practices-and-send-off/index.html#slides",
    "title": "Best Practices and Send-Off",
    "section": "Slides",
    "text": "Slides\nMake slides full screen\n\n\nVSCode\nVSCode is a code text editor application that provides an easy-to-use interface for viewing and writing code/scripts. This section will detail how to 1) install VSCode as an application on your computer, 2) install VSCode extensions and 3) setup launching VSCode as an interface to view and edit your files on the O2 cluster.\n\nInstall VSCode application\nTo install VSCode on your computer, use this link.\n\n\nInstall VSCode extensions\nVSCode has a number of extensions that allow each user to customize their own coding experience. We recommend the installation of three extensions:\n\nRainbow CSV - identifies “,” as the separator in CSV files, highlights each column with a different color\nVSCode-pdf - view PDF files directly within the VSCode interface\nRemote-SSH - allows users to login and access their own HPC cluster and view their files through the VSCode interface (see next section)\n\nTo install extensions:\n\nNavigate to the toolbar on the far left of the VSCode interface and click on the icon that resembles four squares.\nUse the search bar that has the caption “Search Extensions in Marketplace” to find extensions that you like and install each of them individually.\nAn additional step for Remote-SSH installation involves changing the connect timeout setting. As O2 login requires Duo authentication, there is often a slight delay and more time is needed to complete the authentication and login properly. To adjust this, please follow steps 4-6.\nNavigate to the extension page for Remote-SSH (on the toolbar on the far left, select the icon with four squares, then input remote-ssh into the search bar) \n\nSelect the gear button and press Settings \n\nInput 120 seconds for the Remote.SSH: Connect Timeout parameter \n\n\n\n\nSetup remote access with VSCode\nWhile logging into O2 on the terminal only provides a command-line interface and linux commands are needed to modify files, the main advantage of accessing O2 through VSCode is it allows users to easily view and edit their files in an interactive manner within the VSCode application.\nAfter installation of VSCode and the Remote-SSH extension, establish a SSH connection to O2 through VSCode:\n\nPress the bottom left purple icon to ‘Open a Remote Window’. This will prompt a few options to be displayed at the top near the search bar \nSelect ‘Add New SSH Host’ and then input the following and enter\n\n\n##| eval: false\n&lt;user_name&gt;@&lt;server address&gt;\n\n\nSelect &lt;path to config&gt;/.ssh/config as the SSH configuration file to update\nInput password for login\nLogin complete (Note: login may take longer at the first instance because VSCode will download its own server.)\n\nAfter setup, directly click the bottom left purple icon each time and input login credentials to launch O2 on VSCode each time.\n\n\nAdditional steps to not need pasword\nAfter setting up VSCode, Remote-SSH extension and establishing a connection to O2 login node through VSCode, follow these steps to launch VSCode on a remote server.\n\nGenerate SSH key on your own computer’s terminal with the following command. When prompted for file name, press enter to use the default file name. Enter a passphrase to protect your SSH keys.\n\n\n##| eval: false\n## input this line into your computer's terminal\nssh-keygen -t rsa\n\n## sample output (See O2 documentation for reference)\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/USERHOME/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /USERHOME/.ssh/id_rsa.\nYour public key has been saved in /USERHOME/.ssh/id_rsa.pub.\n\n\nCopy your computer’s SSH public key onto the O2 SSH authorized_keys file, which allows your computer and O2 to recognize each other\n\n\nLinux or Mac:\n\n\n##| eval: false\n## input this line into your computer's terminal\nssh-copy-id -i $HOME/.ssh/id_rsa.pub &lt;user_name&gt;@&lt;server address&gt;\n## input O2 password and complete Duo authentication\n\n\nWindows:\n\n\n##| eval: false\n## input this line into your computer's terminal\nGet-Content \"$env:USERPROFILE\\.ssh\\id_rsa.pub\" | ssh &lt;user_name&gt;@&lt;server address&gt; \"mkdir -p ~/.ssh && cat &gt;&gt; ~/.ssh/authorized_keys\"\n## input O2 password and complete Duo authentication\n\n\n\n\nSnakemake\nWe could (and should!) do a whole workshop on using Snakemake for bioinformatics pipelines.\nFor now, take a look at their excellent documentation here: (https://snakemake.readthedocs.io/en/stable/index.html)[https://snakemake.readthedocs.io/en/stable/index.html]\nAnd consider that AI tools are quite good at snakemake.\n\n\nAI tools in 2025\nhttps://chatgpt.com/ - Quite good but costs money to use the more powerful models.\nhttps://chat.deepseek.com/ - Same performance as chatGPT’s o1 model, but for free as of today.\nhttps://www.phind.com/ - This is an interesting one to watch, especially if you plan to use AI to help do the tinking part of your research, not just the coding part. Provides sources, and tries to synthesize more stuff together. Performs a lot better if you pay a subscription.",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 7</b>:Best Practices"
    ]
  },
  {
    "objectID": "materials/3-workshop3/6-untargetted-viromics/index.html",
    "href": "materials/3-workshop3/6-untargetted-viromics/index.html",
    "title": "Workshop III – Bioinformatics and Viral Genomics",
    "section": "",
    "text": "Download Slides",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 6</b>:Untargetted Viromics"
    ]
  },
  {
    "objectID": "materials/3-workshop3/6-untargetted-viromics/index.html#slides",
    "href": "materials/3-workshop3/6-untargetted-viromics/index.html#slides",
    "title": "Workshop III – Bioinformatics and Viral Genomics",
    "section": "",
    "text": "Download Slides",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 6</b>:Untargetted Viromics"
    ]
  },
  {
    "objectID": "materials/3-workshop3/6-untargetted-viromics/index.html#background",
    "href": "materials/3-workshop3/6-untargetted-viromics/index.html#background",
    "title": "Workshop III – Bioinformatics and Viral Genomics",
    "section": "Background",
    "text": "Background\nThe tables used for this exercise are the main outputs of Hecatomb (https://github.com/shandley/hecatomb), a software aimed to increase virus discovery on complex samples (Metagenomic WGS or VLP prep WGS).",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 6</b>:Untargetted Viromics"
    ]
  },
  {
    "objectID": "materials/3-workshop3/6-untargetted-viromics/index.html#dataset",
    "href": "materials/3-workshop3/6-untargetted-viromics/index.html#dataset",
    "title": "Workshop III – Bioinformatics and Viral Genomics",
    "section": "Dataset",
    "text": "Dataset\nPrimarily, we will use a file called bigtable.tsv. This file has the taxonomic classification of each read that was used as input to Hecatomb. It also has the alignment statistics and the number of equal reads found for each sample.\nWe will also use the file called metadata.tsv. The samples in the test dataset are samples taken from deceased Macaques from the study “SIV Infection-Mediated Changes in Gastrointestinal Bacterial Microbiome and Virome Are Associated with Immunodeficiency and Prevented by Vaccination” (https://www.sciencedirect.com/science/article/pii/S1931312816300518). The metadata contains the individuals’ gender and the vaccine that was administered.",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 6</b>:Untargetted Viromics"
    ]
  },
  {
    "objectID": "materials/3-workshop3/6-untargetted-viromics/index.html#setting",
    "href": "materials/3-workshop3/6-untargetted-viromics/index.html#setting",
    "title": "Workshop III – Bioinformatics and Viral Genomics",
    "section": "Setting",
    "text": "Setting\nWe are going to connect to the Rstudio server that is running on our machines ::: {.cell}\nhttp://44.202.27.9:8787 #IMPORTANT: Change 44.202.27.9 by your IP\n#Paste it in your browser\nuser: genomics\npassword:evomics2025\n:::\nAlso, we are going to connect to using ssh within the terminal. Then, we can go to workshop_materials and create a new directory called untargetedViromis. Then, go to that directory. Finally, we are going to download our dataset.\nssh genomics@serverIP #connect to the server\ncd ~/workshop_materials/\nmkdir untargetedViromics #create a new working directory\ncd untargetedViromics\ngit clone https://github.com/luisalbertoc95/UV_data-Workshop-III-Bioinformatics-SA-2025.git  \nmv UV_data-Workshop-III-Bioinformatics-SA-2025 uv_data #Change the name to work with a shorter one.",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 6</b>:Untargetted Viromics"
    ]
  },
  {
    "objectID": "materials/3-workshop3/6-untargetted-viromics/index.html#analysis",
    "href": "materials/3-workshop3/6-untargetted-viromics/index.html#analysis",
    "title": "Workshop III – Bioinformatics and Viral Genomics",
    "section": "Analysis",
    "text": "Analysis\n\nStep 1: Set up a new RMarkdwon/Quarto document\n\n\nStep 2: Initiate your environment\nFirst, We are going to set our working directory for all the chunks\n\nknitr::opts_knit$set(root.dir = \"/Users/luischica/Desktop/uv_data\") #IMPORTANT: change /Users/luischica/Desktop/Workshop_SA/data by ~/workshop_materials/untargetedViromics/uv_data\n\nOptionally, if you are using a normal R script, you can use: setwd(pathToYourWorkingDirectory)\nFor this exercise we will use 4 packages: ggplot2, dplyr and tidyr from the tidyverse suite, but also we will need the rstatix package.\n\n# Check and install tidyverse if needed, then load it\nif (!requireNamespace(\"tidyverse\", quietly = TRUE)) {\n  install.packages(\"tidyverse\")\n}\nlibrary(tidyverse)\n\n# Check and install rstatix if needed, then load it\nif (!requireNamespace(\"rstatix\", quietly = TRUE)) {\n  install.packages(\"rstatix\")\n}\nlibrary(rstatix)\n\n# Check and install DECIPHER if needed, then load it\nif (!requireNamespace(\"DECIPHER\", quietly = TRUE)) {\n  if (!requireNamespace(\"BiocManager\", quietly = TRUE)) {\n    install.packages(\"BiocManager\")\n  }\n  BiocManager::install(\"DECIPHER\")\n}\nlibrary(DECIPHER)\n\n\n\nStep 3: Set the location and load our input files\n\ndata &lt;- read.delim('bigtable.tsv',header=T,sep='\\t')\nmeta &lt;- read.csv('metadata.tsv',header=T,sep='\\t')\n\nInspect the dataframes\n\nhead(data)\n\n                         seqID              sampleID count normCount alnType\n1 A13-258-124-06_CGTACG:1:6006 A13-258-124-06_CGTACG     1  1.012666      aa\n2 A13-258-124-06_CGTACG:1:6007 A13-258-124-06_CGTACG     1  1.012666      aa\n3 A13-258-124-06_CGTACG:1:6019 A13-258-124-06_CGTACG     1  1.012666      aa\n4 A13-258-124-06_CGTACG:1:6020 A13-258-124-06_CGTACG     1  1.012666      aa\n5 A13-258-124-06_CGTACG:1:6030 A13-258-124-06_CGTACG     1  1.012666      aa\n6 A13-258-124-06_CGTACG:1:6031 A13-258-124-06_CGTACG     1  1.012666      aa\n    targetID    evalue pident fident nident mismatches  qcov  tcov qstart qend\n1 A0A1W5PTE0 2.240e-46   97.3  0.973     73          2 0.962 0.166    234   10\n2     E0NZW5 1.941e-23   74.1  0.741     46         16 0.795 0.785     18  203\n3 A0A349YS28 4.343e-17   51.9  0.519     40         37 0.991 0.193      3  233\n4 A0A2N5ZDN1 3.061e-35   81.8  0.818     63         14 0.987 0.279      4  234\n5 A0A1Q6JQ74 4.923e-17   56.5  0.565     39         30 0.885 0.107     15  221\n6 A0A345MUW0 7.251e-24   85.0  0.850     51          9 0.933 0.163     12  191\n  qlen tstart tend tlen alnlen bits\n1  234    372  446  452    225  167\n2  234     16   77   79    186  101\n3  233    155  231  399    231   83\n4  234    161  237  276    231  135\n5  234    422  490  643    207   83\n6  193    294  353  369    180  102\n                                            targetName taxMethod  kingdom\n1                                       VP1 (Fragment)    TopHit  Viruses\n2 Toxin-antitoxin system, toxin component, HicA family    TopHit Bacteria\n3                              Uncharacterized protein       LCA Bacteria\n4                       Lysine--tRNA ligase (Fragment)    TopHit Bacteria\n5                      Phage tail tape measure protein    TopHit Bacteria\n6             A0A345MUW0_9VIRU Uncharacterized protein       LCA  Viruses\n                        phylum                       class\n1                Cossaviricota             Quintoviricetes\n2                   Firmicutes               Negativicutes\n3 unclassified Bacteria phylum unclassified Bacteria class\n4                Bacteroidetes                 Bacteroidia\n5                   Firmicutes                  Clostridia\n6               Hofneiviricota              Faserviricetes\n                        order                               family\n1                Piccovirales                         Parvoviridae\n2             Selenomonadales                     Selenomonadaceae\n3 unclassified Bacteria order         unclassified Bacteria family\n4            Marinilabiliales unclassified Marinilabiliales family\n5               Clostridiales    unclassified Clostridiales family\n6               Tubulavirales                           Inoviridae\n                                genus                        species\n1                     Protoparvovirus               Simian bufavirus\n2                         Selenomonas Selenomonas sp. oral taxon 149\n3         unclassified Bacteria genus  unclassified Bacteria species\n4 unclassified Marinilabiliales genus     Marinilabiliales bacterium\n5    unclassified Clostridiales genus  Clostridiales bacterium 59_14\n6       unclassified Inoviridae genus                 Inoviridae sp.\n  baltimoreType baltimoreGroup\n1         ssDNA             II\n2          &lt;NA&gt;           &lt;NA&gt;\n3          &lt;NA&gt;           &lt;NA&gt;\n4          &lt;NA&gt;           &lt;NA&gt;\n5          &lt;NA&gt;           &lt;NA&gt;\n6         ssDNA             II\n\n\n\nhead(meta)\n\n               sampleID  vaccine sex MacGuffinGroup\n1  A13-04-182-06_TAGCTT     sham   F              B\n2  A13-12-250-06_GGCTAC     sham   F              B\n3 A13-135-177-06_AGTTCC Ad_alone   F              A\n4 A13-151-169-06_ATGTCA Ad_alone   F              A\n5 A13-252-114-06_CCGTCC     sham   M              A\n6 A13-253-140-06_GTCCGC     sham   M              B\n\n\n\n\nStep 4: Merging our metadata with our data table\nThe merge function with perform an inner join by default, or you can specify outer, and left- and right-outer. This shouldn’t matter if you have metadata for all of your samples.\n\ndataMeta &lt;-  merge(data, meta, by='sampleID')\n\n\n\nStep 5: Preliminary bigtable plots\nFirst, we will plot the alignment length against identity, and facet by viral family. We show the different alignment types by color, and we can scale the point size by the cluster number. The alpha=0.1 will help us to set it to 10% opacity and the points will overlap a lot at this scale.\nBefore making our plot, we will filter our dataMeta in order to remove all non viral taxonomic annotation. The function filter() will take all the hits to the word Viruses in our Kingdom column ::: {.cell}\nviruses &lt;- dataMeta %&gt;% \n    filter(kingdom==\"Viruses\")\n:::\nNow we can make our first plot\n\nggplot(viruses) + \n    geom_point(\n        aes(x=alnlen,y=pident,color=alnType,size=count),\n        alpha=0.1) + \n    facet_wrap(~family)\n\n\n\n\n\n\n\n\nWe can immediately see that a handful of viral families make up a majority of the viral hits. You can use these plots to help guide filtering strategies. We can divide the alignments into ‘quadrants’ by adding alignment length and percent identity thresholds, for instance alignment length of 150 and percent identity of 75.\nWe can also add some threshold lines to divide our plot into quadrants. This quadrants will be useful to see how much can we trust in each alignment.\n\nggplot(viruses) +\n    geom_point(\n        aes(x=alnlen,y=pident,color=alnType,size=count),\n        alpha=0.1) +\n    facet_wrap(~family) +\n    geom_vline(xintercept=150,colour='red',linetype='longdash') +\n    geom_hline(yintercept=75,colour='red',linetype='longdash')\n\n\n\n\n\n\n\n\nWe can see that for Adenoviridae and Parvoviridae the majority of hits occupy the top two quadrants, and we can be reasonably confident about these alignments. For Podoviridae and Circoviridae, the majority of hits occupy the bottom two quadrants. This could indicate that the viruses are only distantly related to the reference genomes in these families.\nTask: plot the Bacterial hits faceted by phylum\n\n\nStep 6: Filtering Strategies\nHecatomb is not intended to be a black box of predetermined filtering cutoffs that returns an immutable table of hits. Instead, it delivers as much information as possible to empower the user to decide which hits they want to keep and which hits to purge. Let’s take our raw viral hits data frame viruses and filter them to only keep the ones we are confident about.\nThe e-value is one of the most common metrics to use for filtering alignments. Let’s see what hits would be removed if we used a fairly stringent cutoff of 1e-20. We will create a new data set that contains only the hits remaining after applying the p-value threshold.\nWe are going to see two strategies for filtering our table. In the first strategy, we will add an additional column to our data set. This column will have 2 possible values: “filter” and “pass”. whether a read is tagged as filter or pass will depend of the the threshold added in the function ifelse()\n\nvirusesFiltered &lt;- viruses %&gt;% \n    mutate(filter=ifelse(evalue&lt;1e-20,'pass','filter'))\n\nWe can plot our new table\n\nggplot(virusesFiltered) +\n    geom_point(\n        aes(x=alnlen,y=pident,color=filter),\n        alpha=0.2) +\n    facet_wrap(~family)\n\n\n\n\n\n\n\n\nThe red sequences are destined to be removed, while the blue sequences will be kept. Some viral families will be removed altogether, which is probably a good thing if they only have low quality hits.\nThe second strategy, is a more straightforward method. We only need to use the function filter() and the desired e-value threshold. No additional column will be added.\n\nvirusesFiltered &lt;- viruses %&gt;% \n    filter(evalue&lt;1e-20)\n\nGoing back to the quadrant concept, you might only want to keep sequences above a certain length and percent identity:\n\nvirusesFiltered = virusesFiltered %&gt;% \n    filter(alnlen&gt;150 & pident&gt;75)\n\nNow, our filtered table has an alignment filter, an identity % filter and our previous p-value filter.\nThere are many alignment metrics included in the bigtable for you to choose from.\nTask: Filter your raw viral hits to only keep protein hits with an evalue &lt; 1e-10\n\n\nStep 7: Analyse taxon counts\n\nMake Per family plots First, we will sum the normalized count of each read to a family level using the columns sampleID and family\n\n\nviralFamCounts &lt;- virusesFiltered %&gt;% \n  group_by(family) %&gt;% \n  summarise(normCount=sum(normCount)) %&gt;% \n  arrange(desc(normCount))\n\nviralFamCounts$family &lt;- factor(viralFamCounts$family,levels=viralFamCounts$family)\n\nhead(viralFamCounts)\n\n# A tibble: 6 × 2\n  family                      normCount\n  &lt;fct&gt;                           &lt;dbl&gt;\n1 unclassified Viruses family     2188.\n2 Adenoviridae                    1906.\n3 Microviridae                    1831.\n4 Parvoviridae                    1274.\n5 Picornaviridae                   741.\n6 Siphoviridae                     720.\n\n\nAfter having our new dataframe with the counts per Family, we can create our abundance plot.\n\nggplot(viralFamCounts) +\n  geom_bar(aes(x=family,y=normCount),stat='identity') +\n  coord_flip()  \n\n\n\n\n\n\n\n\n\nDiscriminate our family plots by sample ID\n\nThe previous plot are a good way to understand the overall abundance of each family in all our samples. However, with that plot is impossible to differentiate between samples and therefore, between different treatments. We can sum our normalized counts in a similar way we did before. Now we are going to add the variable “sampleID” to the function group_by()\n\nviralFamCounts &lt;- virusesFiltered %&gt;% \n  group_by(sampleID,family) %&gt;% \n  summarise(normCount=sum(normCount)) %&gt;% \n  arrange(desc(normCount))\n\nhead(viralFamCounts)\n\n# A tibble: 6 × 3\n# Groups:   sampleID [4]\n  sampleID              family                      normCount\n  &lt;chr&gt;                 &lt;chr&gt;                           &lt;dbl&gt;\n1 A13-151-169-06_ATGTCA Adenoviridae                    1777.\n2 A13-151-169-06_ATGTCA Parvoviridae                     993.\n3 A13-135-177-06_AGTTCC unclassified Viruses family      956.\n4 A13-256-115-06_GTTTCG Microviridae                     620.\n5 A13-253-140-06_GTCCGC Myoviridae                       490.\n6 A13-253-140-06_GTCCGC Picornaviridae                   486.\n\n\nand then, make the new plot, filling by family\n\nggplot(viralFamCounts) +\n  geom_bar(aes(x=sampleID,y=normCount, fill =family ),stat='identity') +\n  coord_flip()  \n\n\n\n\n\n\n\n\nTask: Make a stacked bar chart of the viral families for the Male and Female monkeys\n\nVisualizing groups We have a few viral families that are very prominent in our samples. For the purposes of the tutorial we have a completely made up sample group category called MacGuffinGroup. Let’s see if there is a difference in viral loads according to our MacGuffinGroup groups. Collect sample counts for Microviridae. Include the metadata group in group_by() so you can use it in the plot.\n\nFor our first plot we are going to focus exclusively in Microviridae family\n\npodoCounts &lt;- virusesFiltered %&gt;% \n    group_by(family,sampleID,MacGuffinGroup) %&gt;% \n    filter(family=='Podoviridae') %&gt;% \n    summarise(n = sum(normCount))\n\nThen, we can plot using jitter plots, box plots or violin plots\n\nggplot(podoCounts) +\n    geom_jitter(aes(x=MacGuffinGroup,y=n),width = 0.1) +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\nStep 8: Statistical tests\nIn Step 6, we compared the viral counts between the two sample groups for Podoviridae, and it appeared as though group B had more viral sequence hits on average than group A. We can compare the normalized counts for these two groups to see if they’re significantly different.\n\nStudent’s T-test\n\nLet’s check out the data frame we made earlier that we’ll be using for the test\n\nhead(podoCounts)\n\n# A tibble: 6 × 4\n# Groups:   family, sampleID [6]\n  family      sampleID              MacGuffinGroup      n\n  &lt;chr&gt;       &lt;chr&gt;                 &lt;chr&gt;           &lt;dbl&gt;\n1 Podoviridae A13-04-182-06_TAGCTT  B                4.56\n2 Podoviridae A13-12-250-06_GGCTAC  B              203.  \n3 Podoviridae A13-135-177-06_AGTTCC A                1.91\n4 Podoviridae A13-151-169-06_ATGTCA A               12.7 \n5 Podoviridae A13-252-114-06_CCGTCC A                8.22\n6 Podoviridae A13-253-140-06_GTCCGC B               67.9 \n\n\nWe will use the base-r function t.test(), which takes two vectors. One vector has the group A counts and the other has the group B counts. We can use the filter() and pull() functions within the t.test() function. The function pull will take the column n, which has our abundance values.\n\nt.test(\n    podoCounts %&gt;% \n        filter(MacGuffinGroup=='A') %&gt;% \n        pull(n),\n    podoCounts %&gt;% \n        filter(MacGuffinGroup=='B') %&gt;% \n        pull(n),\n    alternative='two.sided',\n    paired=F,\n    var.equal=T)  \n\n\n    Two Sample t-test\n\ndata:  podoCounts %&gt;% filter(MacGuffinGroup == \"A\") %&gt;% pull(n) and podoCounts %&gt;% filter(MacGuffinGroup == \"B\") %&gt;% pull(n)\nt = -2.0096, df = 8, p-value = 0.07933\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -145.561036    9.996944\nsample estimates:\nmean of x mean of y \n 7.701613 75.483659 \n\n\n\nWilcoxon test\n\nThe Wilcoxon test is analogue to the t-test, however is used when our values do not follow a normal distribution. The syntax for this test is very similar to the t-test\n\nwilcox.test(\n    podoCounts %&gt;% \n        filter(MacGuffinGroup=='A') %&gt;% \n        pull(n),\n    podoCounts %&gt;% \n        filter(MacGuffinGroup=='B') %&gt;% \n        pull(n),\n    alternative='t',\n    paired=F)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  podoCounts %&gt;% filter(MacGuffinGroup == \"A\") %&gt;% pull(n) and podoCounts %&gt;% filter(MacGuffinGroup == \"B\") %&gt;% pull(n)\nW = 4, p-value = 0.09524\nalternative hypothesis: true location shift is not equal to 0\n\n\nIn some cases, the viral loads have minor importance for answering a research question, instead we might be just interested in comparing the presence or absence of viruses. For this you could use a Fisher’s exact test. To perform this test you need to assign a presence ‘1’ or absence ‘0’ for each viral family/genus/etc for each sample.\nFirst we are going to apply and even more stringent filter to be sure about the alignments ::: {.cell}\nvirusesStringent &lt;- viruses %&gt;% \n    filter(evalue&lt;1e-30,alnlen&gt;150,pident&gt;75,alnType=='aa') \n:::\nThen we will assign anything with any hits as ‘present’ for that an specific viral family (It can be done for all of them). For this example we are going to use Myoviridae.\nOur chunk has two lines. The first one will extract the counts for Myoviridae and the second will merge our new table with our metadata file\n\nmyovirPresAbs &lt;- virusesStringent %&gt;% \n    filter(family=='Myoviridae') %&gt;%\n    group_by(sampleID) %&gt;% \n    summarise(n=sum(normCount)) %&gt;%\n    mutate(present=ifelse(n&gt;0,1,0))\n\nmyovirPresAbs &lt;- merge(myovirPresAbs,meta,by='sampleID',all=T)\n\nIf we visualise our “myovirPresAbs” table, we will see some values missing or NA. Those are the samples in which no presence of Myoviridae was found. We can convert the NAs to zeros.\n\nmyovirPresAbs[is.na(myovirPresAbs)] = 0\n\nTo do the Fisher’s exact test we need to specify a 2x2 grid; The first column will be the number with Myoviridae for each group. The second column will be the numbers without for each group.\n\n# matrix rows\nmtxGroupA = c(\n    myovirPresAbs %&gt;% \n        filter(MacGuffinGroup=='A',present==1) %&gt;% \n        summarise(n=n()) %&gt;% \n        pull(n),\n    myovirPresAbs %&gt;% \n        filter(MacGuffinGroup=='A',present==0) %&gt;% \n        summarise(n=n()) %&gt;% \n        pull(n))\nmtxGroupB = c(\n    myovirPresAbs %&gt;% \n        filter(MacGuffinGroup=='B',present==1) %&gt;% \n        summarise(n=n()) %&gt;% \n        pull(n),\n    myovirPresAbs %&gt;% \n        filter(MacGuffinGroup=='B',present==0) %&gt;% \n        summarise(n=n()) %&gt;% \n        pull(n))\n\n# create the 2x2 matrix\nmyovirFishMtx = matrix(c(mtxGroupA,mtxGroupB),nrow = 2)\n\n# this bit is not necessary, but lets add row and col names to illustrate the matrix layout\ncolnames(myovirFishMtx) = c('GroupA','GroupB')\nrow.names(myovirFishMtx) = c('present','absent')\n\n# view\nmyovirFishMtx\n\n        GroupA GroupB\npresent      1      5\nabsent       4      0\n\n\nWe can run our t-test using our new matrix as input\n\nfisher.test(myovirFishMtx)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  myovirFishMtx\np-value = 0.04762\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.000000 0.975779\nsample estimates:\nodds ratio \n         0 \n\n\n\n\nStep 9: Contig based analysis\nContig analysis is a very important approach when identifying viruses from a metagenomic or VLP prep sample. Because of the length of the contigs, new genomic features can be identified and used for generating a taxonomic classification, functional analysis and more.\n\nFist, we will load our taxonomy table generated from hecatomb. This table relates every contig with their best hit in the data base.\n\nFor the purpose of this exercise we will filter our table for contigs classified at family level as Caudovirales. and with a sequence length between 2000 and 3000 base pairs.\n\ncontig_data &lt;- read.delim('MMseqsTax.txt',header=T,sep='\\t')\n\ncaudovirales_data &lt;- contig_data %&gt;%\n  filter(Order == \"Caudovirales\" & qlen &gt; 2000 & qlen &lt; 3000) %&gt;%\n  select(contigID) \n\n\nWe will load our fasta file generated after the assembly using the package Biostrings, included in the package DECIPHER. We can also use the package [] for selecting our target contigs from the fasta file\n\n\ncontigs &lt;- readDNAStringSet(\"assembly.fasta\", format=\"fasta\")\n\ncaudo_contigs &lt;- contigs[caudovirales_data$contigID] \n\n\nWe can use our set of contigs to make a multiple sequence alignment (MSA) with the final purpose of generating a phylogeny tree.\n\nThe steps to make our MSA are going to be based on the decision diagram from the DEIPHER package\n Steps:\n\nCreate the multiple sequences alignement\nRemove ambiguous regions from the alignment\n\n\nCreate the phylogenetic tree\n\n\nPlot the dendrogram\n\n\n#A\naln_contig &lt;- AlignSeqs(caudo_contigs)\n\nDetermining distance matrix based on shared 10-mers:\n================================================================================\n\nTime difference of 0.04 secs\n\nClustering into groups by similarity:\n================================================================================\n\nTime difference of 0 secs\n\nAligning Sequences:\n================================================================================\n\nTime difference of 2.76 secs\n\nIteration 1 of 2:\n\nDetermining distance matrix based on alignment:\n================================================================================\n\nTime difference of 0 secs\n\nReclustering into groups by similarity:\n================================================================================\n\nTime difference of 0 secs\n\nRealigning Sequences:\n================================================================================\n\nTime difference of 1.98 secs\n\nIteration 2 of 2:\n\nDetermining distance matrix based on alignment:\n================================================================================\n\nTime difference of 0 secs\n\nReclustering into groups by similarity:\n================================================================================\n\nTime difference of 0 secs\n\nRealigning Sequences:\n================================================================================\n\nTime difference of 1.07 secs\n\nRefining the alignment:\n================================================================================\n\nTime difference of 3.2 secs\n\n#B\nstagger_aln_contig&lt;- StaggerAlignment(aln_contig)\n\nCalculating distance matrix:\n================================================================================\n\nTime difference of 0 secs\n\nConstructing neighbor-joining tree:\n================================================================================\n\nTime difference of 0 secs\n\nStaggering insertions and deletions:\n================================================================================\n\nTime difference of 0.67 secs\n\nBrowseSeqs(stagger_aln_contig, highlight=1)\n#C\ntree &lt;- TreeLine(stagger_aln_contig, reconstruct=TRUE, maxTime=0.05)\n\nOptimizing model parameters:\nJC69     -ln(L) = 62850, AICc = 125775, BIC = 125989\nJC69+G4  -ln(L) = 62814, AICc = 125705, BIC = 125924\nK80      -ln(L) = 62843, AICc = 125763, BIC = 125982\nK80+G4   -ln(L) = 62806, AICc = 125692, BIC = 125916\nF81      -ln(L) = 62406, AICc = 124893, BIC = 125123\nF81+G4   -ln(L) = 62374, AICc = 124832, BIC = 125068\nHKY85    -ln(L) = 62402, AICc = 124887, BIC = 125123\nHKY85+G4 -ln(L) = 62372, AICc = 124830, BIC = 125072\nT92      -ln(L) = 62416, AICc = 124911, BIC = 125135\nT92+G4   -ln(L) = 62384, AICc = 124850, BIC = 125080\nTN93     -ln(L) = 62401, AICc = 124888, BIC = 125130\nTN93+G4  -ln(L) = 62369, AICc = 124826, BIC = 125074\nSYM      -ln(L) = 62755, AICc = 125596, BIC = 125838\nSYM+G4   -ln(L) = 62706, AICc = 125499, BIC = 125747\nGTR      -ln(L) = 62386, AICc = 124863, BIC = 125122\nGTR+G4   -ln(L) = 62352, AICc = 124798, BIC = 125063\n\nThe selected model was:  GTR+G4\n\nPHASE 1 OF 3: INITIAL TREES\n\n1/3. Optimizing initial tree #1 of 10 to 100:\n-ln(L) = 62350.7 (-0.002%), 2 Climbs  \n1/3. Optimizing initial tree #2 of 10 to 100:\n-ln(L) = 62346.9 (-0.006%), 3 Climbs  \n1/3. Optimizing initial tree #3 of 10 to 100:\n-ln(L) = 62349.4 (+0.004%), 6 Climbs  \n\nPHASE 2 OF 3: REGROW GENERATION 1 OF 10 TO 20\n\n2/3. Optimizing regrown tree #1 of 10 to 100:\n-ln(L) = 62345.2 (-0.003%), 1 Climb  \n2/3. Optimizing regrown tree #2 of 11 to 100:\n-ln(L) = 62346.1 (+0.001%), 2 Climbs  \n2/3. Optimizing regrown tree #3 of 11 to 100:\n-ln(L) = 62353.4 (+0.013%), 0 Climbs  \n2/3. Optimizing regrown tree #4 of 11 to 100:\n-ln(L) = 62345.5 (~0.000%), 3 Climbs  \n2/3. Optimizing regrown tree #5 of 11 to 100:\n-ln(L) = 62345.4 (~0.000%), 2 Climbs  \n2/3. Optimizing regrown tree #6 of 11 to 100:\n-ln(L) = 62346.4 (+0.002%), 3 Climbs  \n2/3. Optimizing regrown tree #7 of 11 to 100:\n-ln(L) = 62345.2 (~0.000%), 2 Climbs  \n2/3. Optimizing regrown tree #8 of 11 to 100:\n-ln(L) = 62347.9 (+0.004%), 1 Climb  \n2/3. Optimizing regrown tree #9 of 11 to 100:\n-ln(L) = 62347.6 (+0.004%), 4 Climbs  \n\nPHASE 3 OF 3: SHAKEN TREES\n\nGrafting 1 tree to the best tree:\n-ln(L) = 62344.1 (-0.002%), 1 Graft of 3  \n\n3/3. Optimizing shaken tree #1 of 3 to 1000:\n-ln(L) = 62349.9 (+0.009%), 5 Climbs  \n3/3. Optimizing shaken tree #2 of 3 to 1000:\n-ln(L) = 62361.3 (+0.028%), 4 Climbs  \n3/3. Optimizing shaken tree #3 of 3 to 1000:\n-ln(L) = 62371.9 (+0.045%), 3 Climbs  \n\nGrafting 3 trees to the best tree:\n-ln(L) = 62344.1 (0.000%), 0 Grafts of 10  \n\nModel parameters:\nFrequency(A) = 0.284\nFrequency(C) = 0.204\nFrequency(G) = 0.199\nFrequency(T) = 0.313\nRate A &lt;-&gt; C = 1.125\nRate A &lt;-&gt; G = 1.336\nRate A &lt;-&gt; T = 1.040\nRate C &lt;-&gt; G = 1.437\nRate C &lt;-&gt; T = 1.124\nRate G &lt;-&gt; T = 1.000\nAlpha = 10.863\n\nTime difference of 223.28 secs\n\n#D\nplot(dendrapply(tree,\n         function(x) {\n                 s &lt;- attr(x, \"probability\") \n                 if (!is.null(s) && !is.na(s)) {\n                         s &lt;- formatC(as.numeric(s), digits=2, format=\"f\")\n                         attr(x, \"edgetext\") &lt;- paste(s, \"\\n\")\n                 }\n                 attr(x, \"edgePar\") &lt;- list(p.col=NA, p.lwd=1e-5, t.col=\"#CC55AA\")\n                 if (is.leaf(x))\n                   \nattr(x, \"nodePar\") &lt;- list(lab.font=3, pch=NA)\nx \n}),\nhoriz=TRUE,yaxt='n')",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 6</b>:Untargetted Viromics"
    ]
  },
  {
    "objectID": "materials/3-workshop3/6-untargetted-viromics/index.html#additional-documentation-command-line-code-for-running-a-untargeted-viromics-analysis.",
    "href": "materials/3-workshop3/6-untargetted-viromics/index.html#additional-documentation-command-line-code-for-running-a-untargeted-viromics-analysis.",
    "title": "Workshop III – Bioinformatics and Viral Genomics",
    "section": "Additional documentation: command-line code for running a untargeted viromics analysis.",
    "text": "Additional documentation: command-line code for running a untargeted viromics analysis.\nThis document outlines a step-by-step workflow for predicting viruses in metagenomic samples using a combination of binning, phage prediction, and quality assessment tools. Conda/Mamba installation instructions are provided for each tool.\n\nSetting variables and creating directories\nASSEMBLY=\"~/workshop_materials/untargetedViromics/uv_data/assembly.fasta\"\nmkdir ~/workshop_materials/untargetedViromics/runningTools\nPHAGEPRED_WD=\"~/workshop_materials/untargetedViromics/runningTools/phagePrediction\"\nQUALPRED_WD=\"~/workshop_materials/untargetedViromics/runningTools/Qual\"\nCPUS_PER_TASK=4\ncd ~/workshop_materials/untargetedViromics/runningTools\n\n\nBinning using Reneo\nDo not run\n\nDescription\nReneo is a binning tool that groups contigs into bins (putative genomes) based on coverage, sequence composition, and a flow decomposition algorithm that allows us to get complete genomes from our contigs. This step is important for reducing complexity and isolating viral contigs.\n\n\nConda Installation\nconda create -n reneo -c bioconda reneo\nconda activate reneo\n\n\nCommand\nreneo run --input \"${ASSEMBLY}\" \\\n          --reads \"${READS}\" \\\n          --minlength 1000 \\\n          --output \"${RENEO_OUT}\" \\\n          --threads ${CPUS_PER_TASK}\necho \"reneo done\"\n\n\n\nPhage Prediction using Jaeger, geNomad, and DeepVirFinder\n\nDescription\n\n\nJaeger\nMethodology: Jaeger uses a machine learning model based on convolutional neural networks (CNNs) to predict phage sequences. It analyzes genomic features such as k-mer frequencies, GC content, and sequence length. Strengths: High sensitivity for detecting novel phages due to its ability to learn complex patterns in viral sequences. Can process both reads and contigs, making it versatile for different types of metagenomic data. Performs well on low-abundance sequences and fragmented genomes.\n\n\ngeNomad\nMethodology: geNomad combines homology-based searches (e.g., against protein domain databases like Pfam) with neural network models to classify sequences as viral, plasmid, or chromosomal. The homology component identifies conserved viral and plasmid proteins. The machine learning component uses k-mer frequencies and genomic features to refine predictions. Strengths: High accuracy in distinguishing between viruses, plasmids, and host sequences. Effective for novel sequences due to its hybrid approach. Provides detailed annotations, including viral taxonomy and functional genes.\n\n\nDeepVirFinder\nMethodology: DeepVirFinder employs a deep learning model based on convolutional neural networks (CNNs) to predict viral sequences. It uses k-mer frequencies and sequence composition as input features. Strengths: High sensitivity for detecting novel viruses, even in the absence of close homologs in reference databases. Works well on short reads and fragmented contigs. Can be applied to both DNA and RNA viruses.\n\n\nVirSorter2\nMethodology: VirSorter2 combines homology-based searches (using curated viral protein databases) with machine learning models to identify phage sequences. It uses genomic features such as gene density, strand shifts, and viral hallmark genes. Strengths: High accuracy in detecting phages and viral contigs in metagenomic assemblies. Can classify sequences into lytic, lysogenic, or eukaryotic viruses. Includes a curated database of viral proteins for improved homology-based detection.\n\n\nConda Installation\n## Jaeger\nmamba create -n jaeger -c bioconda jaeger\nconda activate jaeger\n\n## geNomad **Do not run**\nmamba create -n genomad -c bioconda genomad\nconda activate genomad\n\n## virsorter **Do not run**\nmamba create -n vs2 -c conda-forge -c bioconda virsorter=2\nvirsorter setup -d db -j 4\n\n## DeepVirFinder\nmamba create --name dvf python=3.6 numpy theano=1.0.3 keras=2.2.4 scikit-learn Biopython h5py=2.10.0\ngit clone https://github.com/jessieren/DeepVirFinder\ncd DeepVirFinder\n\n\nCommands\nmkdir -p \"${PHAGEPRED_WD}\"\n## DeepVirFinder\nconda activate dvf\npython dvf.py -i \"${ASSEMBLY}\" \\\n              -o \"${PHAGEPRED_WD}/deepvirfinder\" \\\n              -l 1000 \\\n              -c ${CPUS_PER_TASK}\n\necho \"deepvirfinder done\"\n\n## Jaeger\nconda activate jaeger\nJaeger -i \"${ASSEMBLY}\"  \\\n       -o \"${PHAGEPRED_WD}/jaeger\" \\\n       -s 2.5 \\\n       --fsize 1000 \\\n       --stride 1000\n\necho \"jaeger done\"\n\n## geNomad **Do not run**\nconda activate genomad\ngenomad end-to-end --min-score 0.6 \\\n       --cleanup \\\n       --threads ${CPUS_PER_TASK} \\\n       \"${ASSEMBLY}\" \\\n       \"${PHAGEPRED_WD}/geNomad\" \\\n       \"${GENOMAD_DB}\"\n\necho \"genomad done\"\n\n## virsorter **Do not run**\nvirsorter run -w \"${PHAGEPRED_WD}/virsorter\" \\\n-i \"${ASSEMBLY}\" \\\n--include-groups \"dsDNAphage,ssDNA, RNA\" \\\n-j 4 \\\n--min-length 1000 \\\n--min-score 0.8 \\\n--provirus-off  \\\nall\n\n\n\nIntegrative workflows for phage prediction, taxonomic classification, lifestyle prediction, and host prediction\n\nConda Installation\nmamba create -n phabox2 phabox=2.1.10 -c conda-forge -c bioconda -y\n\n## Downloading the database using wget\ncd ~/workshop_materials/untargetedViromics/runningTools\nwget https://github.com/KennthShang/PhaBOX/releases/download/v2/phabox_db_v2.zip\nunzip phabox_db_v2.zip &gt; /dev/null\n\n\nCommand\nconda activate phabox2\nphabox2 --task end_to_end --dbdir phabox_db_v2 \\\n        --outpth  ${PHAGEPRED_WD}/Phabox_OUT \\\n        --contigs ${ASSEMBLY} \\\n        --len 1000 \\\n        --threads ${CPUS_PER_TASK}\necho \"phabox2 done\"\n\n\n\nStep 4: Quality Assessment using CheckV\n\nDescription\nCheckV is a tool designed to assess the quality and completeness of viral genomes recovered from metagenomes. It employs a multi-step approach: Completeness Estimation: Uses a database of high-quality reference viral genomes to identify conserved, single-copy genes (e.g., capsid proteins, terminases, integrases) that serve as markers for viral completeness. Estimates completeness by detecting the presence/absence of these hallmark genes and their collinearity (gene order conservation). Contamination Detection: Identifies host contamination (e.g., bacterial or eukaryotic genes) using a database of non-viral sequences. Flags sequences with atypical GC content, codon usage, or gene content for further scrutiny.\n\n\nConda Installation\nmamba create -n checkv -c conda-forge -c bioconda checkv\nconda activate checkv\ncheckv download_database ./\n\n\nCommand\nmkdir -p \"${QUALPRED_WD}\"\n\ncheckv end_to_end \"${ASSEMBLY}\" \\\n       \"${QUALPRED_WD}\" \\\n       -d checkv-db-v1.5 \\\n       -t ${CPUS_PER_TASK}\n\necho \"checkV done\"\n\n\n\nPhold for Phage Annotation\nDo not Run\n\nDescription\nPhold is a protein structure prediction and annotation tool designed specifically for bacteriophages. It combines homology-based searches with protein language models to annotate phage genomes. Annotation is performed by aligning the predicted structures to a database of structures usinf foldseek.\n\n\nConda Installation\nmamba create -n pholdENV -c bioconda phold\nconda activate pholdENV\nphold install #Installs the databases\n\n\nCommand\n`{.bash} phold run -i \"${ASSEMBLY}\" \\        -o \"${PHAGEPRED_WD}/phold\" \\        -d \"${PHOLD_DB}\" \\        -t ${CPUS_PER_TASK} --cpu echo \"phold done\"",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 6</b>:Untargetted Viromics"
    ]
  },
  {
    "objectID": "materials/3-workshop3/5-targetted-viromics/index.html",
    "href": "materials/3-workshop3/5-targetted-viromics/index.html",
    "title": "Targetted Viromics",
    "section": "",
    "text": "link_to_slides",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 5</b>:Targetted Viromics and Phylogenetics"
    ]
  },
  {
    "objectID": "materials/3-workshop3/5-targetted-viromics/index.html#slides",
    "href": "materials/3-workshop3/5-targetted-viromics/index.html#slides",
    "title": "Targetted Viromics",
    "section": "",
    "text": "link_to_slides",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 5</b>:Targetted Viromics and Phylogenetics"
    ]
  },
  {
    "objectID": "materials/3-workshop3/5-targetted-viromics/index.html#exercises",
    "href": "materials/3-workshop3/5-targetted-viromics/index.html#exercises",
    "title": "Targetted Viromics",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1\nlink to exercise 1\n\n\n\nExercise 2\nlink to exercise 2\n\n\n\nExercise 3\nlink to exercise 3",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 5</b>:Targetted Viromics and Phylogenetics"
    ]
  },
  {
    "objectID": "materials/3-workshop3/4-mapping-and-assembly/index.html",
    "href": "materials/3-workshop3/4-mapping-and-assembly/index.html",
    "title": "Alignment, mapping, and variant calling",
    "section": "",
    "text": "Make slides full screen",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 4</b>: Read Mapping and Variant Calling"
    ]
  },
  {
    "objectID": "materials/3-workshop3/4-mapping-and-assembly/index.html#wifi-details",
    "href": "materials/3-workshop3/4-mapping-and-assembly/index.html#wifi-details",
    "title": "Alignment, mapping, and variant calling",
    "section": "",
    "text": "Make slides full screen",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 4</b>: Read Mapping and Variant Calling"
    ]
  },
  {
    "objectID": "materials/3-workshop3/4-mapping-and-assembly/index.html#slides",
    "href": "materials/3-workshop3/4-mapping-and-assembly/index.html#slides",
    "title": "Alignment, mapping, and variant calling",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 4</b>: Read Mapping and Variant Calling"
    ]
  },
  {
    "objectID": "materials/3-workshop3/0-welcome/slides.html#section",
    "href": "materials/3-workshop3/0-welcome/slides.html#section",
    "title": "Welcome to the workshop",
    "section": "",
    "text": "Goals for this session\n\n\nGet to know your instructors and neighbors\nSet expectations for the week\nGet excited!"
  },
  {
    "objectID": "materials/3-workshop3/0-welcome/slides.html#section-1",
    "href": "materials/3-workshop3/0-welcome/slides.html#section-1",
    "title": "Welcome to the workshop",
    "section": "",
    "text": "Workshop materials are at:\nhttps://elsherbini.github.io/durban-data-science-for-biology/"
  },
  {
    "objectID": "materials/3-workshop3/0-welcome/slides.html#discussions-discord",
    "href": "materials/3-workshop3/0-welcome/slides.html#discussions-discord",
    "title": "Welcome to the workshop",
    "section": "Discussions: discord",
    "text": "Discussions: discord\nAsk questions at #workshop-questions on https://discord.gg/UDAsYTzZE."
  },
  {
    "objectID": "materials/3-workshop3/0-welcome/slides.html#stickies",
    "href": "materials/3-workshop3/0-welcome/slides.html#stickies",
    "title": "Welcome to the workshop",
    "section": "Stickies",
    "text": "Stickies\n\n\n\n\n\n\nDuring an activity, place a yellow sticky on your laptop if you’re good to go and a pink sticky if you want help.\n\n\n\n\nImage by Megan Duffy"
  },
  {
    "objectID": "materials/3-workshop3/0-welcome/slides.html#practicalities",
    "href": "materials/3-workshop3/0-welcome/slides.html#practicalities",
    "title": "Welcome to the workshop",
    "section": "Practicalities",
    "text": "Practicalities\n\nWiFi:\nNetwork: KTB Free Wifi (no password needed)\nNetwork AHRI\nPassword: @hR1W1F1!17\nNetwork AHRI Internet only\nPassword: AHRI twenty three!\nNetwork CAPRISA-Corp Password: corp@caprisa17\nBathrooms are out the lobby to your left"
  },
  {
    "objectID": "materials/3-workshop3/0-welcome/slides.html#introductions",
    "href": "materials/3-workshop3/0-welcome/slides.html#introductions",
    "title": "Welcome to the workshop",
    "section": "Introductions",
    "text": "Introductions\n\n\n\n−+\n03:00\n\n\n\nTake ~3 minutes to introduce yourself to your neighbors.\nPlease share …\n\nYour name\nWhere you’re from and where you work\nYour current go-to method for analyzing data"
  },
  {
    "objectID": "materials/3-workshop3/0-welcome/slides.html#your-instructors",
    "href": "materials/3-workshop3/0-welcome/slides.html#your-instructors",
    "title": "Welcome to the workshop",
    "section": "Your Instructors",
    "text": "Your Instructors\nWho are we?"
  },
  {
    "objectID": "materials/3-workshop3/0-welcome/slides.html#lets-make-this-workshop-work-for-all",
    "href": "materials/3-workshop3/0-welcome/slides.html#lets-make-this-workshop-work-for-all",
    "title": "Welcome to the workshop",
    "section": "Let’s make this workshop work for all",
    "text": "Let’s make this workshop work for all\n\n\nYou belong here. This workshop is intended for a wide-audience with a focus on beginners. If you feel out of place - it’s our problem, not yours!\nStay committed. This week-long workshop is intended to build each day and leave you with skills you can really use. Commit to stay engaged for best results, for you and your group!\nThis is a challenging but friendly environment. We are here to learn and grow. In order to make the right environment please follow “the 4 social rules” and code-of-conduct."
  },
  {
    "objectID": "materials/3-workshop3/0-welcome/slides.html#lets-take-a-poll",
    "href": "materials/3-workshop3/0-welcome/slides.html#lets-take-a-poll",
    "title": "Welcome to the workshop",
    "section": "Let’s take a poll",
    "text": "Let’s take a poll\nGo to the event on wooclap"
  },
  {
    "objectID": "materials/3-workshop3/0-welcome/slides.html#pre-workshop-survey",
    "href": "materials/3-workshop3/0-welcome/slides.html#pre-workshop-survey",
    "title": "Welcome to the workshop",
    "section": "Pre-workshop survey",
    "text": "Pre-workshop survey\n\n\n\n−+\n10:00\n\n\n\n\nGo to https://forms.gle/GKtsjR8SW9NkhVkh6 to fill out the survey."
  },
  {
    "objectID": "materials/3-workshop3/0-welcome/slides.html#the-content-of-this-workshop",
    "href": "materials/3-workshop3/0-welcome/slides.html#the-content-of-this-workshop",
    "title": "Welcome to the workshop",
    "section": "The content of this workshop",
    "text": "The content of this workshop\n\nWe’ve created 6 modules as well as an interactive activity for this workshop.\n\nThere might be too much material to get through in this week!\n\nAs instructors we’re going to be trying to teach at the right pace to keep everyone learning all week.\n\nThe materials will stay on the website forever for you to work through at your own pace."
  },
  {
    "objectID": "materials/3-workshop3/0-welcome/slides.html#when-might-you-not-need-sequencing",
    "href": "materials/3-workshop3/0-welcome/slides.html#when-might-you-not-need-sequencing",
    "title": "Welcome to the workshop",
    "section": "When Might You Not Need Sequencing?",
    "text": "When Might You Not Need Sequencing?\n\n\nKnown Targets, Limited Scope\n\nIf your research question only involves confirming a single, well-characterized virus or a small set of variants, qPCR or targeted assays might suffice.\n\n\nResource Constraints\n\nSequencing can be expensive and time-consuming. If budgets are tight and your question is narrow, simpler assays may be more practical.\n\n\nLarge Cohort Screening\n\nFor massive surveillance efforts where you only care about the presence/absence of a known pathogen (e.g., routine screening for a particular strain), targeted testing like qPCR is faster and cheaper.\n\n\nQuick Turnaround Needed\n\nIf you need immediate results (hours, not days), rapid antigen or antibody tests can be more appropriate."
  },
  {
    "objectID": "materials/3-workshop3/0-welcome/slides.html#question-1-detecting-emerging-variants-of-endemic-viruses",
    "href": "materials/3-workshop3/0-welcome/slides.html#question-1-detecting-emerging-variants-of-endemic-viruses",
    "title": "Welcome to the workshop",
    "section": "Question 1 — Detecting Emerging Variants of Endemic Viruses",
    "text": "Question 1 — Detecting Emerging Variants of Endemic Viruses\nWithout Sequencing\n- Limited Scope: Diagnostic tests (e.g., qPCR) only detect known variants and specific targets.\n- Potentially Missed Variants: Novel or unexpected changes may escape traditional assays.\n- Inadequate Surveillance: Harder to link genetic variation with clinical or epidemiological outcomes.\n\nWith Sequencing\n- Identify New Mutations: Discover novel mutations that might impact transmissibility or virulence.\n- Track Spread: Pinpoint where and when new variants emerge, supporting epidemiological modeling.\n- Predict Drug Resistance: Monitor known resistance-associated mutations"
  },
  {
    "objectID": "materials/3-workshop3/0-welcome/slides.html#question-2-discovering-viruses-in-outbreaks",
    "href": "materials/3-workshop3/0-welcome/slides.html#question-2-discovering-viruses-in-outbreaks",
    "title": "Welcome to the workshop",
    "section": "Question 2 — Discovering Viruses in Outbreaks",
    "text": "Question 2 — Discovering Viruses in Outbreaks\nWithout Sequencing\n- Knowledge Gaps: You can only use qPCR and ELISA to test for known pathogens and antigens - Speed of Discovery: Cell culture based enrichment of viruses takes time and optimization\n\nWith Sequencing\n- Unbiased Detection: Metagenomic approaches allow for the identification of entirely new viruses.\n- Rapid Characterization: Genome assembly and annotation to quickly compare to known viruses.\n- Public Health Preparedness: Early detection can guide containment strategies before widespread transmission."
  },
  {
    "objectID": "materials/3-workshop3/0-welcome/slides.html#lets-take-a-poll-1",
    "href": "materials/3-workshop3/0-welcome/slides.html#lets-take-a-poll-1",
    "title": "Welcome to the workshop",
    "section": "Let’s take a poll",
    "text": "Let’s take a poll\nGo to the event on wooclap\n\n\n\n\nback to module"
  },
  {
    "objectID": "materials/3-workshop3/0-welcome/exercise_1.html#activity",
    "href": "materials/3-workshop3/0-welcome/exercise_1.html#activity",
    "title": "Exploring Data for Patterns",
    "section": "Activity",
    "text": "Activity\n\nPatterns are the essence of data exploration and our eyes’ ability to pick them out is integral to data understanding. Much of the data we work with, however, do not have a natural form and we need to make decisions about how they are to be represented. Try different ways to visualize the datasets so meaningful patterns may be found.\n\n\nGenetic profiles of cancer\nThese datasets contains 10 cancer samples. Table 1 describes the mutational status for a set of genes (A-E) and whether a mutation if absent (0) or present (1). Table 2 summarizes the expression levels of those genes, ranging from no expression (0) to high expression (3).\n\n\n\nTable 1: Mutational status for a set of genes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample -&gt;\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nGene A\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nGene B\n0\n0\n0\n0\n1\n1\n1\n0\n1\n1\n\n\nGene C\n0\n0\n1\n0\n0\n0\n1\n1\n1\n1\n\n\nGene D\n1\n1\n0\n0\n1\n1\n0\n0\n0\n0\n\n\nGene E\n0\n1\n1\n0\n1\n0\n0\n0\n1\n0\n\n\n\n\n\n\n\n\n\nTable 2: Expression levels for a set of genes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample -&gt;\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nGene A\n2\n1\n1\n2\n2\n0\n2\n1\n1\n2\n\n\nGene B\n1\n1\n2\n1\n0\n0\n0\n2\n0\n0\n\n\nGene C\n1\n1\n3\n1\n2\n2\n3\n0\n3\n0\n\n\nGene D\n0\n0\n2\n1\n3\n3\n2\n1\n1\n1\n\n\nGene E\n1\n3\n3\n1\n3\n1\n2\n1\n3\n2\n\n\n\n\n\n\n\n\n          1. Think about the problem on your own for 5 minutes.\n          2. In your groups, discuss and create different visualizations to highlight underlying patterns\n          3. Summarize the group’s approach\n          4. Elect/volunteer a spokesperson to present the solution\n\n\nConsider the following concepts when creating your visualizations\n\n\n\n\nPatterns\nPatterns are the essence of data exploration. What kinds of representation will produce the most meaningful insights?\n   \n\n\nEncodings\nSome visual estimations are easier to make than others. How might you use encodings that are less accurate but otherwise better at conveying overall trends?\n  \n\n\n\n\nColor\nColor is a powerful encoding that presents several challenges. Have you chosen a color scale that is optimal for that data type?\n   \n\n\nSalience and Relevance\nPop-out effects enable quick recognition. Are the most noticeable elements of your visualizations also the most relevant?"
  },
  {
    "objectID": "materials/2-workshop2/8-group-project/index.html",
    "href": "materials/2-workshop2/8-group-project/index.html",
    "title": "Group Activity",
    "section": "",
    "text": "Asavela’s Demo\nLiz’s Demo\nNote: The filepaths to the datasets may not match!\n\n\n\nFind your group on the instances tab!\n\n\nWednesday (4/24)\n\n\n\n\n\n\n\nTime\nTask\n\n\n\n\n1:30-2:00pm\n\nFind your groups!\nRead project description and aims for your project\nIdentify files and create a new quarto project\n\n\n\n2:00 - 3:00pm\n\nCreate phyloseq object\nPhyloseq cleaning\n\n\n\n3:00 - 3:30pm\nBreak\n\n\n3:30- 5:00pm\n\nExplore the data!\nVisualizing microbiome composition\nAlpha Diversity\n\n\n\n\nThursday (4/25)\n\n\n\n\n\n\n\n\nTime\nTask\n\n\n\n\n1:30 - 2:00pm\n\nVisualizing microbiome composition\nAlpha Diversity\n\n\n\n2:00 - 3:00pm\n\nOrdination and Correlations\n\n\n\n3:00 - 3:30pm\nBreak\n\n\n3:30 - 5:00pm\n\nIntegrating Shotgun Metagenomics\n\n\n\n\nFriday (4/26)\n\n\n\n\n\n\n\n\nTime\nTask\n\n\n\n\n\n\n\n\nAM\n\nReview and Presentation preparation\n\n\n\nPM\nGroup Presentations\n\n\n\n\nDataset Demo\n\n\n\n\n\n\n\n\n\n\n\nResearch Project A - Does yogurt consumption change the vaginal microbiome?\n\n\nAim\nTo investigate whether the consumption of yogurt influences microbiome composition after antibiotics treatment.\n\nStudy Description\nThis is a randomized controlled trial to study whether yogurt consumption has an effect on the microbiome post antibiotic treatment. 16S rRNA sequencing was done to characterize patient microbiome composition. Absolute abundance of bacteria (in gene copies / mL) was measured by 3 qPCR assays (for total, L. crispatus, L. iners). Cytokine levels (in ug/mL of vaginal fluid) were also measured by Luminex. Data was collected at two timepoints, pre and post antibiotic treatment.\n\n\n\n\n\n\nResearch Project B - Does birth control change inflammation during menstruation?\n\n\nAim\nTo investigate whether taking birth control is associated with vaginal inflammation throughout the menstrual cycle.\n\nStudy Description\nThis is an observational study to evaluate the relationship between birth control and vaginal inflammation in response to menstruation. 16S rRNA sequencing was done to characterize patient microbiome composition. Cytokine levels (in ug/mL of vaginal fluid) were also measured by Luminex. We also looked at the number and type of immune cells in the vagina using Flow Cytometry. Data was collected at four timepoints; before, at the start, the end, and after menstruation.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Group Activity:</b> Applying what you've learned to new data"
    ]
  },
  {
    "objectID": "materials/2-workshop2/8-group-project/index.html#example-workflows",
    "href": "materials/2-workshop2/8-group-project/index.html#example-workflows",
    "title": "Group Activity",
    "section": "",
    "text": "Asavela’s Demo\nLiz’s Demo\nNote: The filepaths to the datasets may not match!\n\n\n\nFind your group on the instances tab!\n\n\nWednesday (4/24)\n\n\n\n\n\n\n\nTime\nTask\n\n\n\n\n1:30-2:00pm\n\nFind your groups!\nRead project description and aims for your project\nIdentify files and create a new quarto project\n\n\n\n2:00 - 3:00pm\n\nCreate phyloseq object\nPhyloseq cleaning\n\n\n\n3:00 - 3:30pm\nBreak\n\n\n3:30- 5:00pm\n\nExplore the data!\nVisualizing microbiome composition\nAlpha Diversity\n\n\n\n\nThursday (4/25)\n\n\n\n\n\n\n\n\nTime\nTask\n\n\n\n\n1:30 - 2:00pm\n\nVisualizing microbiome composition\nAlpha Diversity\n\n\n\n2:00 - 3:00pm\n\nOrdination and Correlations\n\n\n\n3:00 - 3:30pm\nBreak\n\n\n3:30 - 5:00pm\n\nIntegrating Shotgun Metagenomics\n\n\n\n\nFriday (4/26)\n\n\n\n\n\n\n\n\nTime\nTask\n\n\n\n\n\n\n\n\nAM\n\nReview and Presentation preparation\n\n\n\nPM\nGroup Presentations\n\n\n\n\nDataset Demo\n\n\n\n\n\n\n\n\n\n\n\nResearch Project A - Does yogurt consumption change the vaginal microbiome?\n\n\nAim\nTo investigate whether the consumption of yogurt influences microbiome composition after antibiotics treatment.\n\nStudy Description\nThis is a randomized controlled trial to study whether yogurt consumption has an effect on the microbiome post antibiotic treatment. 16S rRNA sequencing was done to characterize patient microbiome composition. Absolute abundance of bacteria (in gene copies / mL) was measured by 3 qPCR assays (for total, L. crispatus, L. iners). Cytokine levels (in ug/mL of vaginal fluid) were also measured by Luminex. Data was collected at two timepoints, pre and post antibiotic treatment.\n\n\n\n\n\n\nResearch Project B - Does birth control change inflammation during menstruation?\n\n\nAim\nTo investigate whether taking birth control is associated with vaginal inflammation throughout the menstrual cycle.\n\nStudy Description\nThis is an observational study to evaluate the relationship between birth control and vaginal inflammation in response to menstruation. 16S rRNA sequencing was done to characterize patient microbiome composition. Cytokine levels (in ug/mL of vaginal fluid) were also measured by Luminex. We also looked at the number and type of immune cells in the vagina using Flow Cytometry. Data was collected at four timepoints; before, at the start, the end, and after menstruation.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Group Activity:</b> Applying what you've learned to new data"
    ]
  },
  {
    "objectID": "materials/2-workshop2/8-group-project/group_activity_1_birthcontrol_starter.html#goals",
    "href": "materials/2-workshop2/8-group-project/group_activity_1_birthcontrol_starter.html#goals",
    "title": "Birth Control Group Activity I starter code",
    "section": "Goals",
    "text": "Goals\n\nCreate a phyloseq object\nCreate relative abundance bar plots\nExplore within-sample diversity (Alpha Diversity)\n\n\n\n\nLoading in libraries and data 📚\nLoading in libraries. You may need to add more!\n\n\nlibrary(phyloseq)\nlibrary(microViz)\nlibrary(tidyverse)\n\nIdentify your project file paths, and read them in! Depending on where you created your Quarto project, this may vary. Make sure you change the following file paths to match your directory structure.\n\namplicon_ids &lt;- read_csv(\"data/Group B Dataset Menstruation/04_period_amplicon_sample_ids.csv\")\nsamp_id &lt;- read_csv(\"data/Group B Dataset Menstruation/00_sample_ids_period.csv\")\nparticipant_metadata &lt;- read_csv(\"data/Group B Dataset Menstruation/01_participant_metadata_period.csv\")\nflow &lt;- read_csv(\"data/Group B Dataset Menstruation/03_flow_cytometry_period.csv\")\nluminex &lt;- read_csv(\"data/Group B Dataset Menstruation/02_luminex_period.csv\")\n\nThis loads in the count and tax table for the whole sequencing run, so there may be samples you don’t need!\n\nall_samples_count_table &lt;- readRDS(\"data/gv_seqtab_nobim.rds\")\nall_samples_tax_table &lt;- readRDS(\"data/gv_spetab_nobim_silva.rds\")\n\n\n\nCreating the phyloseq 🅿️\nTo speed up some data manipulation steps to get you to your analysis, I’ve provided some code to help you merge all the csvs together and create your phyloseq.\nIn a phyloseq object, we can put qPCR, cytokine data, and other sample metadata into the sample_data. We can create a bc_sample_data dataframe that will become the sample_data.\n\n#creating sample data for the phyloseq\nbc_sample_data &lt;- amplicon_ids %&gt;%\n    left_join(samp_id %&gt;% select(-arm), by = c(\"pid\",\"time_point\")) %&gt;% # merges sample ids\n    left_join(participant_metadata %&gt;% select(-arm), by = \"pid\") %&gt;% # merges participant metadata\n    mutate(arm_timepoint = str_c(arm, time_point, sep = \"_\")) %&gt;%#creating a arm and timepoint column for later plotting\n    left_join(flow, by = \"sample_id\") %&gt;% # merges flow\n    left_join(luminex %&gt;% # merges luminex\n        pivot_wider(names_from = cytokine, values_from = c(conc,limits)), by = \"sample_id\") %&gt;% \n    mutate(arm_timepoint = str_c(arm, time_point, sep = \"_\")) %&gt;%\n    column_to_rownames(\"amplicon_sample_id\")\n\n\n\n\n\n\n\nWhy did I pivot_wider?\n\n\n\n\n\n💭 Because the luminex csv was in long format, pivoting it to wider helps set each analyte as a column and makes each row an unique sample.\n\n\n\n\n\n\n\n\n\nWhich columns do I merge by?\n\n\n\n\n\n💭 How did I know which columns to merge by? This depends on the structure of your data and the task you’re performing. Usually you can choose columns that uniquely identify each row, and you can merge based on these columns. Some times, you need more than one!\n\n\n\nEarlier, I mentioned that there are samples from other projects in the count and tax table! Here we need to update the count and tax table to to reflect the correct ASVs and samples.\nExtracting a vector of birth control dataset sample ids\n\nbc_ids &lt;- bc_sample_data %&gt;% \n              as.data.frame() %&gt;% \n              rownames_to_column(\"amplicon_sample_id\") %&gt;%\n              pull(amplicon_sample_id)\n\nRemoving unused ASVs from the count table\n\ncount_table &lt;- all_samples_count_table %&gt;%\n                as.data.frame() %&gt;%\n                rownames_to_column(\"amplicon_sample_id\") %&gt;%\n                filter(amplicon_sample_id %in% bc_ids) %&gt;% #filtering the count data by the birth control ids\n                mutate_at(vars(-amplicon_sample_id), as.numeric) %&gt;%\n                column_to_rownames(\"amplicon_sample_id\") %&gt;%\n                select(where(~sum(.) != 0)) # removing unused ASVs - when the sum of a column is 0\n\nFiltering the taxa table so there are only ASVs for the birth control dataset sample\n\n#getting all asvs for the birth control data\nasvs &lt;- colnames(count_table)\n#filtering the taxa table so there is only asvs for the birth control samples\ntax_table &lt;- all_samples_tax_table %&gt;%\n              as.data.frame() %&gt;%\n              rownames_to_column(\"tax_table_asv\") %&gt;%\n              filter(tax_table_asv %in% asvs) %&gt;% # filters the tax table by the asvs we want\n              column_to_rownames(\"tax_table_asv\") %&gt;%\n              as.matrix()\n\nNow, we can create our phyloseq!\n\n#creating phyloseq object\nbc_ps &lt;- phyloseq(otu_table(count_table, taxa_are_rows=FALSE), \n               sample_data(bc_sample_data), \n               tax_table(tax_table))\n\nWarning: From here on out, there will be less helpful code!\n\n\nThese are questions that can be used as a guide. Looking at the data, think about what questions you can address.\n\n\nPhyloseq cleaning and diagnosis 🔍\n\nDescribe the structure of the experimental design\nRemove low-yield samples\nHow sparse is your count table?\nExplore the distribution of ASV lengths. Are there any you’d like to remove?\nAre there NAs in the tax table?\n\n\n\nPlot the relative abundance bar plots 📊\n\nGenerate plots to compare relative abudances across arms and time points\nWhat are some noticable differences throughout menstruation?\nDoes birth control change inflammation during menstruation?\n\nOptional questions (Examine your sample_data to see what other questions you can ask!):\n\nWhat cytokines correlated with Lactobaccilus?\nWhat cytokines correlate with certain taxa?\nHow do the absolute abundance of bacteria and Lactobacillus species change throughout menstruation?\n\n\n\n\n\n\n\nIs your plot hard to see?\n\n\n\n\n\n💭 Consider adjusting the code chunk options!\n#| fig-width: 10\n#| fig-height: 10\nOr a number depending on your window size may help.\n\n\n\n\n\n\n\n\n\nChecking your plots or if you’re stuck!\n\n\n\n\n\n💭 Remember, ord_explore() is a very useful function within the microViz package that can build ordinations and relative abundance plots!\n\n\n\n\n\nExploring within-sample diversity (Alpha Diversity) 🎏\n\nDoes your data contain singletons?\nCalculate alpha diversity using three different diversity measures\nIs there a difference in alpha diversity between the birth control and no birth control arms? How about across menstruation?\nIs the difference between arms statistically significant?\n\n\n\nOrdination and Correlation\n\nCreate Microbiome heatmaps\nAnnotate the heatmap with arms and/or timepoints\nUsing ord_explore() or with code, generate ordination plots\nCreate a correlation heatmap of taxa and cytokines (optional)\n\n\n\n\n\n\n\nReferences\n\n\n\n\n\n💭 The microViz documentation can be helpful.\n\n\n\n\n\nShotgun Metagenomics\n\nCreate a phyloseq object using the VIRGO dataset\nTry applying Michael’s code!"
  },
  {
    "objectID": "materials/2-workshop2/6-virgo/VIRGO2_analysis.html",
    "href": "materials/2-workshop2/6-virgo/VIRGO2_analysis.html",
    "title": "VIRGO2 analysis of shotgun metagenomic data",
    "section": "",
    "text": "The output of VIRGO2 is a data frame of dimensions Genes by Samples and the values are the number of reads that were mapped to that gene. This data frame is generally LARGE and WIDE with up to 1.7 million genes. For the example code I’ve selected a random subset of 100,000 VIRGO2 genes. So the dataframe contains 100,000 rows (features). To expedite things, I’ve already merged a column for the taxonomic and functional annotations for each gene from the VIRGO2 annotation files.\nThis code will go through some basic exploration of the data to a simple analysis of differential taxonomic composition, differential functional composition, and then investigating which taxa are driving the differential abundance."
  },
  {
    "objectID": "materials/2-workshop2/6-virgo/VIRGO2_analysis.html#working-with-virgo2-output",
    "href": "materials/2-workshop2/6-virgo/VIRGO2_analysis.html#working-with-virgo2-output",
    "title": "VIRGO2 analysis of shotgun metagenomic data",
    "section": "",
    "text": "The output of VIRGO2 is a data frame of dimensions Genes by Samples and the values are the number of reads that were mapped to that gene. This data frame is generally LARGE and WIDE with up to 1.7 million genes. For the example code I’ve selected a random subset of 100,000 VIRGO2 genes. So the dataframe contains 100,000 rows (features). To expedite things, I’ve already merged a column for the taxonomic and functional annotations for each gene from the VIRGO2 annotation files.\nThis code will go through some basic exploration of the data to a simple analysis of differential taxonomic composition, differential functional composition, and then investigating which taxa are driving the differential abundance."
  },
  {
    "objectID": "materials/2-workshop2/6-virgo/VIRGO2_analysis.html#installing-a-required-package",
    "href": "materials/2-workshop2/6-virgo/VIRGO2_analysis.html#installing-a-required-package",
    "title": "VIRGO2 analysis of shotgun metagenomic data",
    "section": "Installing a required package",
    "text": "Installing a required package\n\nif (!require(\"BiocManager\", quietly = TRUE))\n  install.packages(\"BiocManager\")\nBiocManager::install(\"Heatplus\", ask=FALSE)"
  },
  {
    "objectID": "materials/2-workshop2/6-virgo/VIRGO2_analysis.html#loading-required-packages",
    "href": "materials/2-workshop2/6-virgo/VIRGO2_analysis.html#loading-required-packages",
    "title": "VIRGO2 analysis of shotgun metagenomic data",
    "section": "Loading required packages",
    "text": "Loading required packages\nThis block of code loads the packages we will use in this section.\n\nlibrary(dplyr)\nlibrary(gplots)\nlibrary(vegan)\nlibrary(RColorBrewer)\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(Heatplus)\nlibrary(GUniFrac)"
  },
  {
    "objectID": "materials/2-workshop2/6-virgo/VIRGO2_analysis.html#reading-in-and-examining-the-metadata-file",
    "href": "materials/2-workshop2/6-virgo/VIRGO2_analysis.html#reading-in-and-examining-the-metadata-file",
    "title": "VIRGO2 analysis of shotgun metagenomic data",
    "section": "Reading in and examining the metadata file",
    "text": "Reading in and examining the metadata file\nThis block of code sets the working directory and reads in the metadata file.\n\n#reading in and examining the metaData file\nmetaData &lt;- read.csv(\"data/instructional/InstructionalDataset_samplesMatch.csv\")\n\nNow lets examine some of the metadata fields and look at how many entries there are.\n\n#counting entries in metaDataFile\n#this function pipes the metaData object to the count function to generation a table of the number of times each variable in the column appears\nmetaData %&gt;% \n  count(UID, name=\"Count\")\n\nmetaData %&gt;% \n  count(CST, name=\"Count\")\n\nmetaData %&gt;% \n  count(pid, name=\"Count\")\n\nmetaData %&gt;% \n  count(arm, name=\"Count\")\n\nmetaData %&gt;% \n  count(timepoint,name=\"Count\")\n\nNow we will read in the VIRGO2 read counts table, this will be followed by a bit of exploration of the dataset to identify and investigate potential problems.\n\n#reading in and examining the read counts file\nreadCounts &lt;- read.csv(\"./data/instructional/InstructionalDataset_VIRGO2.csv\")\n\n#printing the column names\nprint(colnames(readCounts))\n#printing the first couple lines of the dataframe\nprint(head(readCounts))\n\n#printing the number of unique KEGG and Taxa values in the dataframe\nprint(n_distinct(readCounts$KEGG))\nprint(n_distinct(readCounts$Taxa))\n\nAs we can see, the dataframe contains 4 columns of information about the genes (Gene ID, the KEGG category, the taxonomy of the gene, and the length of the gene). This is then followed by one column for each sample where the values are the number of reads from the sample that mapped to each of the genes.\nFirst we will look for differences in the number of reads mapped per metagenome to identify any samples for which there were insufficient reads mapped.\n\n#investigating the total number of reads per MG for outliers\n#this line sums the columns to calculate the number of reads per MG, the first 4 columns are skipped because they contain annotations and not sample data\ntotalReads = colSums(readCounts[,5:ncol(readCounts)])\n#this line returns the log10 transformed median value for the distribution of read counts\nlog10(median(totalReads))\n\n#this line plots a simple histogram of the distribution of read counts per MG\nhist(log10(totalReads),col=\"#000000\")\n\n#identifying the sample with the minimum number of reads\nwhich.min(totalReads)\n####dropping metagenome with minimum reads, from data table and metadata table\ndrops &lt;- c(\"VAG0042\")\nreadCounts &lt;- readCounts[ , !(names(readCounts) %in% drops)]\nmetaData &lt;- metaData[!(metaData$UID %in% drops),]\n\nNext we will just check that the sample was actually dropped and re-examine the distribution and minimum number of reads mapped.\n\n#checking it was dropped\n#again recalculating the number of reads mapped per MG\ntotalReads = colSums(readCounts[,5:ncol(readCounts)])\n\nhist(log10(totalReads),col=\"#000000\")\n\nAnother feature we might want to examine is the number of genes mapped per metagenome. This could help identify differences in the total gene content per metagenome but could also highlight metagenomes where the bulk of the reads mapped to very few genes.\n\n#examining the gene counts per MG\n#converting the abundance data to presence absence data\ngeneCounts &lt;- colSums(readCounts[,5:ncol(readCounts)] != 0)\n\n#breaking this down, this part return true if the value in the cell is not equal to 0 and a False if the value is equal to zero, the column sums then returns the number of TRUE values, whic is the number of genes observed\nreadCounts[,5:ncol(readCounts)] != 0\n\n#generating a histogram of the number of genes per metagenome\n\nhist(geneCounts,breaks=seq(0,25000,by=1000))\n\nWhat do we think? Is this distribution problematic or expected?\n\nmetaData$geneCounts &lt;- geneCounts\nboxplot(geneCounts~CST,data=metaData,notch=FALSE,col=c('#fe0308','#f6d3da','#ff7200','#f8a40e','#221886'),ylab = 'No. of Genes',xlab=\"CST\")\n\nAs you can see, communities which contain more species (CST IV-B) yield more genes per metagenome than those that contain relatively few species (CST I, CST III).\nIn this next section we will begin to calculate the taxonomic composition of each metagenome, using the read counts per gene.\nEssentially, we will divide the read counts for each gene by the length of the gene. This approximates the coverage of the genes.\n\n#calculating taxonomic composition (in relative abundance) of each MG\n#this line divides the read counts by the gene length (NOTE THIS IS AN APPROXIMATION, WHY?)\ngeneCoverages &lt;- readCounts[,5:ncol(readCounts)]/readCounts$Length\n\nprint(head(geneCoverages))\n\nThen to calculate the taxonomic composition, we will sum the coverages of genes per taxa to get a total coverage of the taxa.\n\n#calculating the taxa relative abundances by first summing coverages of genes across taxa\n\n#first we add back the gene metadata columns\ngeneCoverages &lt;- cbind(readCounts[,1:4],geneCoverages)\n#then we drop the metadata values we don't need\ndrops &lt;- c('Gene','KEGG','Length')\ngeneCoverages &lt;- geneCoverages[ , !(names(geneCoverages) %in% drops)]\n\n#then we group the coverage by taxa and apply the sum function\ntaxaCoverages &lt;- data.frame(geneCoverages %&gt;% \n  group_by(Taxa) %&gt;% \n  summarise(across(everything(), sum)))\n\nprint(head(taxaCoverages))\n\nIn the next code block, we are going to drop the coverages of genes which did not have taxonomic annotations, and the genes annotated as originating from multiple genera. Opinions differ on whether or not to consider these proportions in your analyses and plots but for this analysis we will just remove them.\n\n#dropping the coverage contributed by genes without taxonomy and by genes which were annotated as \"MultiGenera\"\ntaxaCoverages = taxaCoverages[!taxaCoverages$Taxa == \"\", ]\ntaxaCoverages = taxaCoverages[!taxaCoverages$Taxa == \"MultiGenera\", ]\n\nWe will then change the row names to the taxa designations and then remove the column to taxa. Following that we will determine relative abundance as the coverage of each taxa divided by the sum of coverage values for the sample. This is akin to dividing the number of reads per taxa (in 16S amplicon data) by the total number of reads with the key difference being that we are considering variation in the gene length.\n\nrownames(taxaCoverages) &lt;- taxaCoverages$Taxa\ndrops &lt;- c('Taxa')\ntaxaCoverages &lt;- taxaCoverages[ , !(names(taxaCoverages) %in% drops)]\n\ntaxaRel &lt;- data.frame(t(t(taxaCoverages)/colSums(taxaCoverages)))\n\n#sorting the taxa by their study wide relative abundances\ntaxaRel &lt;- taxaRel[order(rowSums(taxaRel),decreasing=T),]\n\nprint(head(taxaRel))\n\nIn the next section we will apply some filters on the taxa to drop those that are not very prevalent or abundant in any given community. When we do this, we will keep track of how much “relative abundance” was lost when removing these taxa, to ensure that it is not having a major effect on our estimation of community composition.\n\n#applying a filter to remove any taxa less than 10-5 in average relative abundance (You should try additional filtering thresholds)\n#first we'll look at the sum of each column to demonstrate that, prior to filtering the total relative abundance for each sample was 1!\ncolSums(taxaRel)\n\n#then we apply a rowMeans filter (average relative abundance of the taxa) as 10^-5 or 0.00001\ntaxaRel_filt &lt;-  taxaRel %&gt;% filter(rowMeans(select(., where(is.numeric))) &gt; 0.00001)\n\n#checking what effect this filter had on our relative abundance scores\nmean(colSums(taxaRel_filt))\n#on average, this filter removed &lt;0.001% of the community composition\n1-min(colSums(taxaRel_filt))\n#the sample most impacted by this filter had 0.005% of it's community composition removed\nprint(dim(taxaRel))\nprint(dim(taxaRel_filt))\n\n\n#recomputing relative abundance after dropping those taxa, so samples are back to summing to 1\ntaxaRel &lt;- data.frame(t(t(taxaRel_filt)/colSums(taxaRel_filt)))\ncolSums(taxaRel)\n\nIn the next code block we will generate a heatmap displaying the taxonomic composition of each sample. As part of this process we’ll generate a dendrogram which will arrange samples with similar taxonomic composition together.\n\n#setting the ravel lab heatmap color palette\nheat_cmap &lt;- colorRampPalette(c('#FFFF00','#00EE76','#BBFFFF','#104E8B','#FF1493','#B22222'), space = \"rgb\")(12)\n\n# calculate the Bray-Curtis dissimilarity matrix on the full dataset:\ndata.dist &lt;- vegdist(t(taxaRel), method = \"bray\")\ndata.dist.g &lt;- vegdist(taxaRel, method = \"bray\")\n\n#Do ward linkage to generate dendrograms for hierarchical clustering.\nrow.clus &lt;- hclust(data.dist, \"average\")\ncol.clus &lt;- hclust(data.dist.g, \"average\")\n\n#creating a heatmap, sorting samples by the dendrogram generated in the above step\nheatmap(as.matrix(t(top_n(taxaRel,n=25))),Rowv = as.dendrogram(row.clus),Colv=NA, col = heat_cmap)\n\nIn the next section we will create a stacked bar plot (because I am addicted to them) which also displays the taxonomic composition. Importantly, we will sort the dataframe by the dendrogram so that, like the heatmap, samples which have similar taxonomic composition appear near each other in the plot.\n\n#subsetting out the top 25 taxa\ntaxaRel_trim &lt;- taxaRel[1:25,]\n\n#sorting the dataframe by the dendrogram orders\nsampSort &lt;- row.clus$labels[match(row.clus$order, sort(row.clus$order))]\ntaxaRelBar &lt;- data.frame(t(taxaRel_trim[,sampSort]))\n\n#calculating the \"other\" variable which tracks the proprotion of coverage not contributed by the top 25\ntaxaRelBar$other &lt;- 1-rowSums(taxaRelBar)\ntaxaRelBar$Sample &lt;- row.names(taxaRelBar)\n\n#melting the dataframe from \"wide\" format to \"long\" format\ntaxaRelBarLong = melt(taxaRelBar, id = c(\"Sample\"))\n#ensuring that the samples in the bar graph will maintain their order\ntaxaRelBarLong$Sample &lt;- factor(taxaRelBarLong$Sample,levels=unique(taxaRelBarLong$Sample))\n\n#setting the taxa color scheme\ncolours = c('#ff8c00','#ff0000','#58e0d9','#20b2aa','#409490','#0000cd','#ffeeee','#bbeff2','#99b0af','#1acad6','#333333','#008B45','#cf36b5','#05ebdf','#684a6b','#6accc7','#a16060','#3bb16f','#86bf4d','#146c73','#ff00d9','#ac8fc7','#00494f','#085e5a','#c997cc','#e1e1e1')\n\n#make the stackedplot!\nstackedBarPlot = ggplot(taxaRelBarLong, aes(x = Sample, fill = variable, y = value)) + \n  geom_bar(stat = \"identity\", colour = \"black\") + \n  theme(axis.text.x = element_text(angle = 90, size = 14, colour = \"black\", vjust = 0.5, hjust = 1), \n        axis.title.y = element_text(size = 16, face = \"bold\"), legend.title = element_text(size = 16, face = \"bold\"), \n        legend.text = element_text(size = 12, face = \"bold\", colour = \"black\"),\n        legend.key.size = unit(10, \"pt\"),\n        legend.background = element_rect(fill=\"lightgray\"),\n        axis.text.y = element_text(colour = \"black\", size = 12, face = \"bold\")) + \n  guides(fill=guide_legend(ncol =1)) + \n  scale_y_continuous(expand = c(0,0)) + \n  labs(x = \"\", y = \"Relative Abundance (%)\", fill = \"Taxa\") + \n  scale_fill_manual(values = colours)\n\nstackedBarPlot\n\nIn the final section on taxonomic composition, we’ll perform a differential abundance test to determine if their are taxa which are more abundant in the treatment versus the placebo arm. There are lots of different tools to do this analysis, most of them try to deal with the central problems with microbiome datasets (1: the data are compositional; 2: the data are sparse). For this analysis I’m using ZicoSeq which uses the read counts as inputs and not the relative abundance scores we calculated. Under the hood it’s running a permutation test but if you want to know more I would suggest your read the paper. It seems like every week there is a new tool do this test, generally I recommend trying a couple options and look for consistency between them.\nIn this first part we’ll generate the input expected by ZicoSeq, which is a dataframe of read counts, this will look similar to what we did for the taxonomic composition except we are just summing read counts per taxa.\n\n#generating taxaReadCounts for analysis by ZicoSeq\n\n#dropping gene annotations we don't need\ndrops &lt;- c('Gene','KEGG','Length')\ntaxaCounts &lt;- readCounts[ , !(names(readCounts) %in% drops)]\n\n#summing the read counts per Taxa\ntaxaCounts &lt;- data.frame(taxaCounts %&gt;% \n                           group_by(Taxa) %&gt;% \n                           summarise(across(everything(), sum)))\n\n#filtering out reads mapped to genes without taxonomic annotations or with MultiGenera annotation\ntaxaCounts = taxaCounts[!taxaCounts$Taxa == \"\", ]\ntaxaCounts = taxaCounts[!taxaCounts$Taxa == \"MultiGenera\", ]\n\n#making the row names the taxa\nrownames(taxaCounts) &lt;- taxaCounts$Taxa\ndrops &lt;- c('Taxa')\ntaxaCounts &lt;- taxaCounts[ , !(names(taxaCounts) %in% drops)]\n\n#sorting the dataframe so that the most abundant taxa are at the top of the dataframe\ntaxaCounts &lt;- taxaCounts[order(rowSums(taxaCounts),decreasing=T),]\n\n#dropping taxa which have absolutely no reads, ZicoSeq doesn't like this\ntaxaCounts &lt;- taxaCounts[rowSums(taxaCounts != 0) &gt; 0,]\n\nprint(head(taxaCounts))\nprint(head(metaData))\n\nDifferences in baseline\n\nbaseline_metaData &lt;- metaData[metaData$timepoint == 'baseline',]\nbaseline_taxaCounts &lt;- taxaCounts[ , (names(taxaCounts) %in% baseline_metaData$UID)]\n\nprint(head(baseline_metaData))\nprint(head(baseline_taxaCounts))\ndim(baseline_taxaCounts)\nbaseline_taxaCounts &lt;- as.matrix(baseline_taxaCounts)\nbaseline_taxaCounts &lt;- baseline_taxaCounts[rowSums(baseline_taxaCounts != 0) &gt; 0,]\n\n\nbaseline_taxaZicoOut &lt;- ZicoSeq(meta.dat = baseline_metaData, feature.dat = baseline_taxaCounts                                      ,grp.name =c('arm'),feature.dat.type = \"count\",\n                                     # Filter to remove rare taxa\n                                     prev.filter = 0.15, mean.abund.filter = 0.00001,  \n                                     max.abund.filter = 0.0001, min.prop = 0, \n                                     # Winsorization to replace outliers\n                                     is.winsor = TRUE, outlier.pct = 0.03, winsor.end = 'both',\n                                     # Posterior sampling \n                                     is.post.sample = TRUE, post.sample.no = 100, \n                                     # Use the square-root transformation\n                                     link.func = list(function (x) x^0.5), stats.combine.func = max,\n                                     # Permutation-based multiple testing correction\n                                     perm.no = 99,  strata = NULL, \n                                     # Reference-based multiple stage normalization\n                                     ref.pct = 0.5, stage.no = 6, excl.pct = 0.2,\n                                     is.fwer = TRUE, verbose = TRUE, return.feature.dat = T)\n\n\nZicoSeq.plot(baseline_taxaZicoOut, metaData, pvalue.type = 'p.adj.fdr', cutoff = 0.01, text.size = 10,out.dir = NULL, width = 10, height = 6)\n\nx &lt;- data.frame(baseline_taxaZicoOut$p.adj.fdr)\ny &lt;- data.frame(baseline_taxaZicoOut$R2)\nz &lt;- data.frame(baseline_taxaZicoOut$coef.list)\nz &lt;- t(z)\n\nbaselineTaxaOut &lt;- cbind(x,y,z)\nbaselineTaxaOut &lt;- baselineTaxaOut[order(baselineTaxaOut$baseline_taxaZicoOut.p.adj.fdr,decreasing=FALSE),]\n#print(baselineTaxaOut)\n\nWeek 1\n\nweek1_metaData &lt;- metaData[metaData$timepoint == 'week_1',]\nweek1_taxaCounts &lt;- taxaCounts[ , (names(taxaCounts) %in% week1_metaData$UID)]\n\nprint(head(week1_metaData))\nprint(head(week1_taxaCounts))\ndim(week1_taxaCounts)\nweek1_taxaCounts &lt;- as.matrix(week1_taxaCounts)\nweek1_taxaCounts &lt;- week1_taxaCounts[rowSums(week1_taxaCounts != 0) &gt; 0,]\n\n\nweek1_taxaZicoOut &lt;- ZicoSeq(meta.dat = week1_metaData, feature.dat = week1_taxaCounts, \n                                     grp.name =c('arm'),feature.dat.type = \"count\",\n                                     # Filter to remove rare taxa\n                                     prev.filter = 0.15, mean.abund.filter = 0.00001,  \n                                     max.abund.filter = 0.0001, min.prop = 0, \n                                     # Winsorization to replace outliers\n                                     is.winsor = TRUE, outlier.pct = 0.03, winsor.end = 'both',\n                                     # Posterior sampling \n                                     is.post.sample = TRUE, post.sample.no = 100, \n                                     # Use the square-root transformation\n                                     link.func = list(function (x) x^0.5), stats.combine.func = max,\n                                     # Permutation-based multiple testing correction\n                                     perm.no = 999,  strata = NULL, \n                                     # Reference-based multiple stage normalization\n                                     ref.pct = 0.5, stage.no = 6, excl.pct = 0.2,\n                                     is.fwer = TRUE, verbose = TRUE, return.feature.dat = T)\n\n\nZicoSeq.plot(week1_taxaZicoOut, metaData, pvalue.type = 'p.adj.fdr', cutoff = 0.01, text.size = 10,out.dir = NULL, width = 10, height = 6)\n\nx &lt;- data.frame(week1_taxaZicoOut$p.adj.fdr)\ny &lt;- data.frame(week1_taxaZicoOut$R2)\nz &lt;- data.frame(week1_taxaZicoOut$coef.list)\nz &lt;- t(z)\n\nweek1TaxaOut &lt;- cbind(x,y,z)\nweek1TaxaOut &lt;- week1TaxaOut[order(week1TaxaOut$week1_taxaZicoOut.p.adj.fdr,decreasing=FALSE),]\n#print(week1TaxaOut)\n\nWeek 7\n\nweek7_metaData &lt;- metaData[metaData$timepoint == 'week_7',]\nweek7_taxaCounts &lt;- taxaCounts[ , (names(taxaCounts) %in% week7_metaData$UID)]\n\nprint(head(week7_metaData))\nprint(head(week7_taxaCounts))\ndim(week7_taxaCounts)\nweek7_taxaCounts &lt;- as.matrix(week7_taxaCounts)\nweek7_taxaCounts &lt;- week7_taxaCounts[rowSums(week7_taxaCounts != 0) &gt; 0,]\n\n\nweek7_taxaZicoOut &lt;- ZicoSeq(meta.dat = week7_metaData, feature.dat = week7_taxaCounts, \n                                     grp.name =c('arm'),feature.dat.type = \"count\",\n                                     # Filter to remove rare taxa\n                                     prev.filter = 0.15, mean.abund.filter = 0.00001,  \n                                     max.abund.filter = 0.0001, min.prop = 0, \n                                     # Winsorization to replace outliers\n                                     is.winsor = TRUE, outlier.pct = 0.03, winsor.end = 'both',\n                                     # Posterior sampling \n                                     is.post.sample = TRUE, post.sample.no = 100, \n                                     # Use the square-root transformation\n                                     link.func = list(function (x) x^0.5), stats.combine.func = max,\n                                     # Permutation-based multiple testing correction\n                                     perm.no = 99,  strata = NULL, \n                                     # Reference-based multiple stage normalization\n                                     ref.pct = 0.5, stage.no = 6, excl.pct = 0.2,\n                                     is.fwer = TRUE, verbose = TRUE, return.feature.dat = T)\n\n\nZicoSeq.plot(week7_taxaZicoOut, metaData, pvalue.type = 'p.adj.fdr', cutoff = 0.01, text.size = 10,\n             out.dir = NULL, width = 10, height = 6)\nx &lt;- data.frame(week7_taxaZicoOut$p.adj.fdr)\ny &lt;- data.frame(week7_taxaZicoOut$R2)\nz &lt;- data.frame(week7_taxaZicoOut$coef.list)\nz &lt;- t(z)\n\nweek7TaxaOut &lt;- cbind(x,y,z)\nweek7TaxaOut &lt;- week7TaxaOut[order(week7TaxaOut$week7_taxaZicoOut.p.adj.fdr,decreasing=FALSE),]\n#print(week7TaxaOut)"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/index.html#have-this-microviz-documentation-open",
    "href": "materials/2-workshop2/5-16s-pt2/index.html#have-this-microviz-documentation-open",
    "title": "Correlation and Ordination",
    "section": "Have this microviz documentation open",
    "text": "Have this microviz documentation open\nFirst let’s load the libraries:\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(phyloseq)\nlibrary(microViz)\n\n\nsample_df &lt;- readRDS(\"data/instructional/sample_data_instructional.rds\")\ncount_tab &lt;- readRDS(\"data/instructional/count_table_instructional.rds\")\ntax_tab &lt;- readRDS(\"data/instructional/tax_table_instructional.rds\")\n\n\n# make a data frame of only the vaginal samples\nvaginal_samples_to_keep &lt;- sample_df %&gt;% \n  filter(sample_type == \"vaginal\")\n\nasv_df &lt;- tax_tab %&gt;%\n  data.frame() %&gt;% \n  rownames_to_column(var = \"asv\") %&gt;% \n  mutate(asv_len = nchar(asv))\n\nkeepers &lt;- asv_df %&gt;% \n  filter(asv_len &gt; 385 & asv_len &lt; 460) %&gt;% \n  # Careful below! Don't drop NA unless you intend to\n  filter(Order != \"Chloroplast\" | is.na(Order)) %&gt;% \n  filter(Family != \"Mitochondria\" | is.na(Family)) %&gt;%\n  pull(asv) %&gt;% # pull() goes from column to vector\n  unique()\n\n# filter the count table to only include vaginal samples\n# and to only include ASVs who are non-zero in at least one sample\n\nfiltered_counts &lt;- count_tab %&gt;%\n  # convert to data frame\n  as.data.frame() %&gt;%\n  # change rownames to a column (https://tibble.tidyverse.org/reference/rownames.html)\n  rownames_to_column(\"amplicon_sample_id\") %&gt;%  \n  # make the table \"longer\" to be in tidy format\n  # (https://r4ds.hadley.nz/data-tidy.html#sec-tidy-data)\n  pivot_longer(-amplicon_sample_id,names_to=\"asv\", values_to = \"count\") %&gt;%\n  # semi_join() returns all rows from x with a match in y.\n  semi_join(vaginal_samples_to_keep) %&gt;%\n  # get rid of asvs that are zero in all samples\n  group_by(asv) %&gt;%\n  filter(sum(count &gt; 0) &gt; 0) %&gt;%\n  ungroup() %&gt;%\n  #get rid of ASVs that aren't in the keepers vector\n  filter(asv %in% keepers)\n\n# we need to convert the count table back into a matrix\n# to make a phyloseq object\nfiltered_counts_matrix &lt;- filtered_counts %&gt;%\n  pivot_wider(names_from = asv, values_from = count) %&gt;%\n  column_to_rownames(\"amplicon_sample_id\") %&gt;%\n  as.matrix()\n\n\n# lastly, remove asv tax assignments if they are\n# not present in any of the vaginal samples\n# also remove Label because microviz doesn't like it\nfiltered_taxonomy &lt;- tax_tab %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"asv\") %&gt;%\n  as_tibble() %&gt;%\n  semi_join(filtered_counts) %&gt;%\n  select(-Label) %&gt;%\n  column_to_rownames(\"asv\") %&gt;%\n  as.matrix()\n\nps &lt;- phyloseq(sample_data(sample_df %&gt;% \n                             filter(sample_type == \"vaginal\") %&gt;%\n                             column_to_rownames(var = \"amplicon_sample_id\")),\n               otu_table(filtered_counts_matrix, taxa_are_rows = FALSE),\n               tax_table(filtered_taxonomy)) %&gt;%\n               prune_samples(sample_sums(.) &gt; 100, .) %&gt;%\n               tax_fix() %&gt;% # this propogates higher taxonomy when lower is NA\n               tax_mutate(genus_species = str_c(Genus, Species, sep=\" \")) %&gt;%\n               tax_rename(rank = \"genus_species\")\n\n\nps_manual_taxonomy &lt;- ps %&gt;%\n  tax_fix() %&gt;%\n  tax_mutate(Species = case_when(\n    Species ==  \"acidophilus/casei/crispatus/gallinarum\" ~ \"crispatus\",\n    Species == \"crispatus/gasseri/helveticus/johnsonii/kefiranofaciens\" ~ \"crispatus\",\n    Species == \"animalis/apodemi/crispatus/murinus\" ~ \"crispatus\",\n    .default = Species)) %&gt;%\n    #also remake genus_species to fix those taxa\n    tax_mutate(genus_species = str_c(Genus, Species, sep = \" \")) %&gt;%\n    tax_rename(rank = \"genus_species\")\n\nWhy manual taxonomy? Well, if we look at the original dataframe we can see these taxa where our taxonomic database SILVA didn’t know which species it was exactly, but instead gave a list of possible species. However, we know these are vaginal samples, and that 99% of the time, these ASVs correspond to L. crispatus because the other possible species are very rare in vaginal samples.\n\nps %&gt;%\n  tax_fix() %&gt;%\n  tax_agg(\"genus_species\") %&gt;%\n  ps_seriate() %&gt;% # this changes the order of the samples to be sorted by similarity\n  comp_barplot(tax_level = \"genus_species\", sample_order = \"asis\", n_taxa = 10) +\n  facet_wrap(vars(arm, time_point), scales=\"free_x\")\n\nOk, now we’ll switch to live-coding for the rest of the session.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 5</b>: Correlations and Ordinations"
    ]
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#practicalities",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#practicalities",
    "title": "16S Data Analysis Part I",
    "section": "Practicalities",
    "text": "Practicalities\n\nWiFi:\nNetwork: KTB Free Wifi (no password needed)\nNetwork AHRI Password: @hR1W1F1!17\nNetwork CAPRISA-Corp Password: corp@caprisa17\nBathrooms are out the lobby to your left"
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#hello",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#hello",
    "title": "16S Data Analysis Part I",
    "section": "Hello 👋👋",
    "text": "Hello 👋👋\nMy name is Elizabeth Costello (Liz)\n\nI’m a Research Scientist in David Relman’s lab at Stanford University’s School of Medicine.\nI enjoy microbial ecology, bioinformatics AND spending time outdoors.\n\n\nLiz kayaking"
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#workflow-check-in",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#workflow-check-in",
    "title": "16S Data Analysis Part I",
    "section": "Workflow check-in",
    "text": "Workflow check-in\nLet’s review what we’ve done and what’s next\n\n\nDone: Starting with the raw sequence data, we filtered, trimmed, inferred ASVs, removed chimeras, and assigned taxonomy.\nNext up: Take the results of these steps (count table; taxonomy table) and do some initial and exploratory data analysis."
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#goals-for-this-module",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#goals-for-this-module",
    "title": "16S Data Analysis Part I",
    "section": "🎯 Goals for this module",
    "text": "🎯 Goals for this module\n\n\nUnderstand what exploratory data analysis is\nApply exploratory data analysis to 16S data\n\nGet to know the data\nCreate a phyloseq object\nExplore (stacked) bar plots\nExplore alpha (within-sample) diversity\nExplore beta (between-sample) diversity using ordination\n\nBecome familiar with the phyloseq package"
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#exploratory-data-analysis",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#exploratory-data-analysis",
    "title": "16S Data Analysis Part I",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nA popular quote:\n\n“Exploratory data analysis is an attitude, a flexibility, and a reliance on display, NOT a bundle of techniques” -John Tukey\n\n\n🫣 What do the data look like? 🔬"
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#key-considerations-for-molecular-survey-data",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#key-considerations-for-molecular-survey-data",
    "title": "16S Data Analysis Part I",
    "section": "Key considerations for molecular survey data",
    "text": "Key considerations for molecular survey data\n\n\nUneven/incomplete sampling depth\nWideness/sparseness\nCompositionality\nCollinearities\nConfounders\n“Garbage in, garbage out”"
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#the-phyloseq-package",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#the-phyloseq-package",
    "title": "16S Data Analysis Part I",
    "section": "The phyloseq package",
    "text": "The phyloseq package\n\nMcMurdie & Holmes (2013)"
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#alpha-diversity-𝜶",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#alpha-diversity-𝜶",
    "title": "16S Data Analysis Part I",
    "section": "Alpha diversity (𝜶)",
    "text": "Alpha diversity (𝜶)\n\n\n\n\n\nFierer et al. (2012)\n\n\n\n\nWithin-sample diversity\nRichness: The number of taxa or lineages within a given sample\nEvenness: The relative abundance of taxa or lineages within a given sample\nA metric that considers both richness & evenness (e.g., the Shannon diversity index)"
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#beta-diversity-β",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#beta-diversity-β",
    "title": "16S Data Analysis Part I",
    "section": "Beta diversity (β)",
    "text": "Beta diversity (β)\n\n\n\n\n\nFierer et al. (2012)\n\n\n\n\nBetween-sample diversity\nTaxonomic or phylogenetic difference in community composition between samples\nVarious measures of pairwise resemblance\nDimensionality reduction (e.g., ordination)\nIdentify primary sources of variation in data"
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#lets-analyze-some-data",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_slides.html#lets-analyze-some-data",
    "title": "16S Data Analysis Part I",
    "section": "Let’s analyze some data!",
    "text": "Let’s analyze some data!\nGo to RStudio, navigate to directory 16S_part1, and open Quarto document 16S_part1_code.qmd\n\n\n\nback to module"
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#specify-path-to-the-files",
    "href": "materials/2-workshop2/3-16s_processing/index.html#specify-path-to-the-files",
    "title": "16S Data Processing",
    "section": "Specify path to the files",
    "text": "Specify path to the files\n\npath &lt;- \"../subsampled_reads/Gut\" ## CHANGE to the directory containing the fastq files.\n#list.files(path) # this lists the names of each file inside the directory",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#separate-foward-and-reverse-reads-using-file-extensions",
    "href": "materials/2-workshop2/3-16s_processing/index.html#separate-foward-and-reverse-reads-using-file-extensions",
    "title": "16S Data Processing",
    "section": "Separate foward and reverse reads using file extensions",
    "text": "Separate foward and reverse reads using file extensions\nThis allows you to assess the quality of the foward and reverse reads appropriately\nlist.files function is used with specific patterns (“R1.fastq.gz” for forward reads and “R2.fastq.gz” for reverse reads) to separate the forward and reverse reads.\nThe sort function ensures that the forward and reverse reads are in the same order, which is necessary for the subsequent paired-end read assembly and analysis.\nfull.names = TRUE: This argument specifies that you want the full path names of the files rather than just their names.\n\n# Sort ensures forward/reverse reads are in same order\nfwdR&lt;- sort(list.files(path, pattern = \"R1.fastq.gz\", full.names = TRUE))\nrevR &lt;- sort(list.files(path, pattern = \"R2.fastq.gz\", full.names = TRUE))",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#inspect-the-quality-of-the-reads",
    "href": "materials/2-workshop2/3-16s_processing/index.html#inspect-the-quality-of-the-reads",
    "title": "16S Data Processing",
    "section": "Inspect the quality of the reads",
    "text": "Inspect the quality of the reads\n\nplotQualityProfile(fwdR[1:2])\nplotQualityProfile(revR[1:4])\n\nRed line: Represents the scaled proportion of reads that extend to at least a certain position in the sequencing read.\nIn the context of next-generation sequencing (NGS), reads are the sequences generated by the sequencing machine. The length of these reads can vary depending on the technology used and the specific settings of the sequencing run. The “position” in this context refers to the position along the length of the reads. For example, position 1 would be the first base of the reads, position 2 would be the second base, and so on. The “scaled proportion of reads that extend to at least that position” means the proportion of reads that are at least as long as that position, adjusted (or scaled) for the total number of reads. This kind of plot can be used to assess the quality of the sequencing run. For example, if a large proportion of reads do not extend to the expected full length, it may indicate an issue with the sequencing process, such as poor quality of the DNA input or a technical problem with the sequencing machine.\nThe number after the red line in a sequencing analysis plot typically represents the count or proportion of reads that extend to at least the position indicated by the red line. This can be used to assess the quality and depth of the sequencing data. If the number is high, it means that a large proportion of the reads extend to the full length, which is generally a good sign. It indicates that the sequencing process was successful and that the data is of high quality. Conversely, if the number is low, it suggests that many reads did not extend to the full length. This could be due to various factors such as poor quality of the DNA input, technical issues with the sequencing machine, or errors in the sequencing process. It’s important to note that the specifics can vary depending on the exact context and the software or pipeline being used. Therefore, it’s always a good idea to refer to the documentation or guidelines associated with the specific sequencing analysis pipeline you’re using.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#extract-sample-names",
    "href": "materials/2-workshop2/3-16s_processing/index.html#extract-sample-names",
    "title": "16S Data Processing",
    "section": "Extract sample names",
    "text": "Extract sample names\n\nsample.names &lt;- sapply(strsplit(basename(fwdR), \"_R\"), `[`, 1)\n\nsapply is a function that applies a given function to each element of a list or vector and returns the results in a simplified format.\nstrsplit separates each element of the character vector fwdR at the “_R” character.\nbasename(fwdR) extracts the last part of the file path specified by fwdR. It removes the directory part of the file path and returns only the file name.\nThe sapply function is used to apply the subsetting function to each element of the list returned by strsplit. The 1 inside indicates that we want to keep only the first element of each split character vector.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#name-the-filtered-reads",
    "href": "materials/2-workshop2/3-16s_processing/index.html#name-the-filtered-reads",
    "title": "16S Data Processing",
    "section": "Name the filtered reads",
    "text": "Name the filtered reads\nTo identify the filtered reads, we add a string “_F_filt” to the sample names paste0 allows you to put the file extension “_F_filt.fastq.gz” to each element of the sample.names vector.\n\nfiltFs &lt;- file.path(path,\"filtered\", paste0(sample.names, \"_F_filt.fastq.gz\"))\nfiltRs &lt;- file.path(path,\"filtered\", paste0(sample.names, \"_R_filt.fastq.gz\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\nfile.path(path, \"filtered\", paste0(sample.names, \"\\_F_filt.fastq.gz\")): This part constructs the file paths for the filtered reads. file.path() concatenates directory paths and file names. path = base directory where the raw files are located. \"filtered\" is a subdirectory within path. paste0(sample.names, \"\\_F_filt.fastq.gz\") creates file names by appending each sample name from the vector sample.names with \"\\_F_filt.fastq.gz\".\nnames(filtFs) &lt;- sample.names and names(filtRs) &lt;- sample.names: These lines assign the sample names to the constructed file paths. This way, each file path is associated with its corresponding sample name.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#filter-and-trim",
    "href": "materials/2-workshop2/3-16s_processing/index.html#filter-and-trim",
    "title": "16S Data Processing",
    "section": "Filter and trim",
    "text": "Filter and trim\n\nout &lt;- filterAndTrim(fwdR,filtFs,revR,filtRs,compress=TRUE,\n                     truncLen=c(300,270), #truncation length (fwdR,revR) \n                     maxN=0,#sequences that contain no unknown bases\n                     maxEE=c(2,2),\n                     truncQ=2 , multithread = TRUE  \n                     )\n\ncompress = TRUE: Indicates whether or not to compress the output. truncLen: Specifies the truncation length for the forward and reverse reads, respectively. maxN = 0: Maximum number of unknown bases allowed in the sequences. maxEE = c(2, 2): Specifies the maximum expected errors for the forward and reverse reads, respectively. truncQ = 2: Quality score threshold for truncation.\nMultithreading is a programming technique used to achieve multitasking within a single process. It allows a program to perform multiple tasks concurrently by breaking them into smaller threads of execution that can run independently.\nThe multithread parameter is controlling whether the filterAndTrim function will utilize multiple threads (multithreading) to process the data in parallel.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#inspect-filtered-reads",
    "href": "materials/2-workshop2/3-16s_processing/index.html#inspect-filtered-reads",
    "title": "16S Data Processing",
    "section": "Inspect filtered reads",
    "text": "Inspect filtered reads\n\nhead(out)\nsave(out, file=\"filtering_summary.RData\")\n\n\nSpecify the minimum number of reads expected post filtering and trimming\n\nkeep &lt;- out[,\"reads.out\"] &gt; 200",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#determine-number-of-samples-that-are-to-be-kept",
    "href": "materials/2-workshop2/3-16s_processing/index.html#determine-number-of-samples-that-are-to-be-kept",
    "title": "16S Data Processing",
    "section": "Determine number of samples that are to be kept",
    "text": "Determine number of samples that are to be kept\n\nlength(out[keep])/2 # number samples w/ over 200 reads\n\n#404 samples out of 418\n\n\nDetermine the percentage of remaining samples\n\n(length(out[keep]))/2/(length(out)/2) # percent of samples kept\n #0.9665072 good number",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#determine-samples-to-keep-for-downstream-analysis",
    "href": "materials/2-workshop2/3-16s_processing/index.html#determine-samples-to-keep-for-downstream-analysis",
    "title": "16S Data Processing",
    "section": "Determine samples to keep for downstream analysis",
    "text": "Determine samples to keep for downstream analysis\n\nfiltFs &lt;- file.path(filtFs)[keep]\nfiltRs &lt;- file.path(filtRs)[keep]",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#error-rates",
    "href": "materials/2-workshop2/3-16s_processing/index.html#error-rates",
    "title": "16S Data Processing",
    "section": "Error rates",
    "text": "Error rates\nThe DADA2 algorithm depends on a parametric error model (err) and every amplicon dataset has a different set of error rates. The learnErrors method learns the error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many optimization problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).\n\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#these-contain-information-about-sequencing-errors-base-quality-scores-or-other-quality-related-metrics.",
    "href": "materials/2-workshop2/3-16s_processing/index.html#these-contain-information-about-sequencing-errors-base-quality-scores-or-other-quality-related-metrics.",
    "title": "16S Data Processing",
    "section": "These contain information about sequencing errors, base quality scores, or other quality-related metrics.",
    "text": "These contain information about sequencing errors, base quality scores, or other quality-related metrics.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#visualize-the-estimated-error-rates-as-a-sanity-check.",
    "href": "materials/2-workshop2/3-16s_processing/index.html#visualize-the-estimated-error-rates-as-a-sanity-check.",
    "title": "16S Data Processing",
    "section": "Visualize the estimated error rates as a sanity check.",
    "text": "Visualize the estimated error rates as a sanity check.\n\nplotErrors(errF, nominalQ = TRUE)\nplotErrors(errR, nominalQ=TRUE)\n\nplotErrors: Creates plots of errors or quality metrics associated with sequencing data. errF and errR: These are variables representing the errors or quality metrics for the forward and reverse reads, respectively. nominalQ = TRUE: This argument specifies whether to use nominal quality scores. If TRUE, it indicates that the quality scores are nominal (e.g., Phred scores), and the plot should be adjusted accordingly. If FALSE or not specified, the function may interpret the quality scores differently.\nThe error rates for each possible transition (eg. A-&gt;C, A-&gt;G, …) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence. The red line shows the error rates expected under the nominal definition of the Q-value. Here the black line (the estimated rates) fits the observed rates well, and the error rates drop with increased quality as expected.\n\nderepFs &lt;- derepFastq(filtFs, verbose=TRUE)\nderepRs &lt;- derepFastq(filtRs, verbose=TRUE)",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#name-dereplicated-files",
    "href": "materials/2-workshop2/3-16s_processing/index.html#name-dereplicated-files",
    "title": "16S Data Processing",
    "section": "Name dereplicated files",
    "text": "Name dereplicated files\n\nnames(derepFs) &lt;- sample.names[keep]\nnames(derepRs) &lt;- sample.names[keep]",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#sample-inference",
    "href": "materials/2-workshop2/3-16s_processing/index.html#sample-inference",
    "title": "16S Data Processing",
    "section": "Sample Inference",
    "text": "Sample Inference\nThe core sample inference algorithm is applied to the dereplicated data.\n\ndadaFs &lt;- dada(derepFs, err=errF, pool = FALSE, multithread=TRUE)\ndadaRs &lt;- dada(derepRs, err=errR, pool = FALSE, multithread=TRUE)",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#merge-paired-reads",
    "href": "materials/2-workshop2/3-16s_processing/index.html#merge-paired-reads",
    "title": "16S Data Processing",
    "section": "Merge paired reads",
    "text": "Merge paired reads\n\nmergers &lt;- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#construct-sequence-table",
    "href": "materials/2-workshop2/3-16s_processing/index.html#construct-sequence-table",
    "title": "16S Data Processing",
    "section": "Construct Sequence Table",
    "text": "Construct Sequence Table\nAmplicon sequence variant table (ASV) table: a higher-resolution version of the OTU table produced by traditional methods.\n\nseqtab &lt;- makeSequenceTable(mergers)\ndim(seqtab) #tells you the (rows, columns) in a matrix\n#rows represent samples and columns have unique sequence variants",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#determine-how-many-bases-are-in-each-amplicon-using-nchar-table-summarizes-the-number-of-amplicons-with-a-specific-length",
    "href": "materials/2-workshop2/3-16s_processing/index.html#determine-how-many-bases-are-in-each-amplicon-using-nchar-table-summarizes-the-number-of-amplicons-with-a-specific-length",
    "title": "16S Data Processing",
    "section": "Determine how many bases are in each amplicon using nchar #Table summarizes the number of amplicons with a specific length",
    "text": "Determine how many bases are in each amplicon using nchar #Table summarizes the number of amplicons with a specific length\n\ntable(nchar(getSequences(seqtab)))",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#view-length-distribution",
    "href": "materials/2-workshop2/3-16s_processing/index.html#view-length-distribution",
    "title": "16S Data Processing",
    "section": "View length distribution",
    "text": "View length distribution\n\nhist(nchar(getSequences(seqtab)),\n     main=\"Distribution of sequence lengths\")",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#in-the-event-that-a-lot-of-reads-have-much-longer-or-shorter-lengths-than-expected-filter-seqtab-to-keep-only-the-reads-that-fall-within-the-expected-range-may-be-the-result-of-non-specific-priming",
    "href": "materials/2-workshop2/3-16s_processing/index.html#in-the-event-that-a-lot-of-reads-have-much-longer-or-shorter-lengths-than-expected-filter-seqtab-to-keep-only-the-reads-that-fall-within-the-expected-range-may-be-the-result-of-non-specific-priming",
    "title": "16S Data Processing",
    "section": "In the event that a lot of reads have much longer or shorter lengths than expected , filter seqtab to keep only the reads that fall within the expected range May be the result of non-specific priming",
    "text": "In the event that a lot of reads have much longer or shorter lengths than expected , filter seqtab to keep only the reads that fall within the expected range May be the result of non-specific priming\n\nseqtab.f &lt;- seqtab[,nchar(colnames(seqtab)) %in% seq(430, 480)]\n# % of reads with desired length\nsum(seqtab.f)/sum(seqtab)",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#remove-chimeras",
    "href": "materials/2-workshop2/3-16s_processing/index.html#remove-chimeras",
    "title": "16S Data Processing",
    "section": "Remove chimeras",
    "text": "Remove chimeras\n\nseqtab.nochim &lt;- removeBimeraDenovo(seqtab.f, method=\"consensus\", multithread=TRUE, verbose=TRUE)\n#nonchimeric % of reads\nsum(seqtab.nochim)/sum(seqtab.f)\ndim(seqtab.nochim)\n\nsave(seqtab.nochim, file=\"seqtab.nochim120424.RData\")",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#inspect-distribution-of-sequence-lengths",
    "href": "materials/2-workshop2/3-16s_processing/index.html#inspect-distribution-of-sequence-lengths",
    "title": "16S Data Processing",
    "section": "Inspect distribution of sequence lengths:",
    "text": "Inspect distribution of sequence lengths:\n\ntable(nchar(getSequences(seqtab.nochim)))",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#view-length-distribution-1",
    "href": "materials/2-workshop2/3-16s_processing/index.html#view-length-distribution-1",
    "title": "16S Data Processing",
    "section": "View length distribution",
    "text": "View length distribution\n\nhist(nchar(getSequences(seqtab.nochim)),\n     main=\"Distribution of sequence lengths\")",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#track-reads-through-the-pipeline",
    "href": "materials/2-workshop2/3-16s_processing/index.html#track-reads-through-the-pipeline",
    "title": "16S Data Processing",
    "section": "Track reads through the pipeline",
    "text": "Track reads through the pipeline\nInspect the the number of reads that made it through each step in the pipeline to verify everything worked as expected.\n\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;- cbind(out, \n               sapply(dadaFs, getN), \n               sapply(dadaRs, getN),\n               sapply(mergers, getN), \n               rowSums(seqtab.f), \n               rowSums(seqtab.nochim))\n\ncolnames(track) &lt;- c(\"input\",\n                     \"filtered\",\n                     \"denoisedF\",\n                     \"denoisedR\",\n                     \"merged\",\n                     \"tabled\", \n                     \"nonchim\")\n\n# A .txt file to map the loss of reads across the different DADA2 steps\nwrite.table(track,\n            \"track_2304424.txt\", sep = \"\\t\", col.names = T, row.names = F)\nsave(track, file=\"track_2304424.txt\")",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#assign-taxonomy",
    "href": "materials/2-workshop2/3-16s_processing/index.html#assign-taxonomy",
    "title": "16S Data Processing",
    "section": "Assign taxonomy",
    "text": "Assign taxonomy\n\ntaxa &lt;- assignTaxonomy(\n  seqtab.nochim,\n  \"~/workshop_2/taxonomic_database/silva_nr99_v138.1_wSpecies_train_set.fa\",\n  minBoot = 60,\n  multithread=TRUE)\nunname(head(taxa))",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/3-16s_processing/index.html#check-assignmnent",
    "href": "materials/2-workshop2/3-16s_processing/index.html#check-assignmnent",
    "title": "16S Data Processing",
    "section": "Check assignmnent",
    "text": "Check assignmnent\n\ntaxa.print &lt;- taxa \nrownames(taxa.print) &lt;- NULL # Removing sequence rownames for display only\nhead(taxa.print)\n\nsaveRDS(taxa,\n        \"taxa_230424.rds\")",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 3</b>: 16S Data Processing"
    ]
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/index.html",
    "href": "materials/2-workshop2/2-r-refresher/index.html",
    "title": "R refresher",
    "section": "",
    "text": "Let’s do a refresher on R, RStudio and Quarto for data analysis.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 2</b>: R Refresher"
    ]
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/index.html#slides-intro-to-rstudio-quarto-and-r",
    "href": "materials/2-workshop2/2-r-refresher/index.html#slides-intro-to-rstudio-quarto-and-r",
    "title": "R refresher",
    "section": "Slides: Intro to RStudio, Quarto, and R",
    "text": "Slides: Intro to RStudio, Quarto, and R\nMake slides full screen",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 2</b>: R Refresher"
    ]
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/index.html#exercise-2.1-data-wrangling-and-simple-manipulations",
    "href": "materials/2-workshop2/2-r-refresher/index.html#exercise-2.1-data-wrangling-and-simple-manipulations",
    "title": "R refresher",
    "section": "Exercise 2.1 : Data wrangling and simple manipulations",
    "text": "Exercise 2.1 : Data wrangling and simple manipulations\nIn this worksheet, we will discuss how to perform basic data manipulations, such as filtering data rows that meet certain conditions, choosing data columns, and arranging data in ascending or descending order.\nWe will be using the R package, tidyverse for the data manipulation functions %&gt;%, filter(), select(), arrange(), count(), and mutate().\n\nThe pipe (%&gt;%, read: “and then”)\nWhen writing complex data analysis pipelines, we frequently use the pipe operator %&gt;% to move data from one analysis step to the next. The pipe is pronounced “and then”, and it takes the data on its left and uses it as the first argument for the function on its right.\nFor example, to see the first few lines of a dataset, we often write head(data). Instead, we can write data %&gt;% head().\nTry this yourself. Write code that displays the first few lines of table_01, using %&gt;% and head():\n\n```{r}\n#| eval: false\n#| warning: false\n\nlibrary(tidyverse)\n\ntable_01 &lt;- read_csv(\"../instructional_dataset/data/01_participant_metadata_UKZN_workshop_2023.csv\")\n\ntable_02 &lt;- read_csv(\"../instructional_dataset/data/02_visit_clinical_measurements_UKZN_workshop_2023.csv\")\n```\n\n\ntable_01 ___ head()\n\n\n\nChoosing data rows\nThe function filter() allows you to find rows in a dataset that meet one or more specific conditions. The syntax is data %&gt;% filter(condition), where condition is a logical condition. For example, filter(x &gt; 5) would pick all rows for which the value in column x is greater than 5.\nBelow, write code to pick all measurements from the placebo arm in table_02 :\n\ntable_02 %&gt;%\n  filter(___)\n\n\n\nFiltering for multiple conditions\nYou can also state multiple conditions, separated by a comma. For example, filter(x &gt; 5, y &lt; 2) would pick all rows for which the value in the column x is greater than 5 and the value in the column y is less than 2. Note that the conditions are combined via logical and, both need to be satisfied for the row to be picked.\nTo try this out, pick all measurements at baseline for the placebo arm in table_02 :\n\ntable_02 %&gt;%\n  filter(time_point == ___, arm == ___)\n\n\n\nChoosing data columns\nThe function select() allows you to pick specific data columns by name. This is frequently useful when a dataset has many more columns than we are interested in at the time. For example, if we are only interested in the participants’ education, and sex, we could select these three columns:\n\ntable_01 %&gt;%\n  select(education, sex)\n\n# A tibble: 44 × 2\n   education                     sex  \n   &lt;chr&gt;                         &lt;lgl&gt;\n 1 grade 10-12, matriculated     FALSE\n 2 grade 10-12, matriculated     FALSE\n 3 post-secondary                FALSE\n 4 grade 10-12, not matriculated FALSE\n 5 grade 10-12, matriculated     FALSE\n 6 post-secondary                FALSE\n 7 grade 10-12, not matriculated FALSE\n 8 grade 10-12, not matriculated FALSE\n 9 grade 10-12, not matriculated FALSE\n10 less than grade 9             FALSE\n# ℹ 34 more rows\n\n\nTry this yourself, picking the columns representing the participant age (age), and then study arm (arm).\n\ntable_01 %&gt;%\n  select(___)\n\n\n\nChoosing columns for removal\nAnother situation that arises frequently is one where we want to remove specific columns. We can also do this with select(), but now write select(-column) to remove one or more columns.\nTry this. Remove the column smoker.\n\ntable_01 %&gt;%\n  select(___)\n\nAnd now remove both smoker and education.\n\ntable_01 %&gt;%\n  select(-___, -___)\n\n\n\nSorting data\nThe function arrange() allows you to sort data by one or more columns. For example, data %&gt;% arrange(x) would sort the data by increasing values of x, and data %&gt;% arrange(x, y) would sort the data first by x and then, for ties in x, by y.\nSort the participants by age:\n\ntable_01 %&gt;%\n  arrange(___)\n\nTo arrange data in descending order, enclose the data column in desc(). For example, data %&gt;% arrange(desc(x)) would sort the data by decreasing values of x. (desc stands for “descending”.)\n\n\nCounting\nWe frequently want to count how many times a particular value or combination of values occurs in a dataset. We do this using the count() function. For example, the following code counts how many participants there are in each study arm in table_01.\n\ntable_01 %&gt;%\n  count(arm)\n\n# A tibble: 2 × 2\n  arm           n\n  &lt;chr&gt;     &lt;int&gt;\n1 placebo      23\n2 treatment    21\n\n\nNow try this yourself. Count how many smokers and non smokers there are.\n\ntable_01 %&gt;%\n  count(___)\n\nWe can also perform more fine-grained counts, by providing the count() function with more than one column name. See if you can count how many smokers and non smokers the dataset contains for each treatment arm\n\ntable_01 %&gt;%\n  count(___, smoker)\n\nNow count how many participants are in each study arm for each education level in the dataset\n\ntable_01 %&gt;%\n  count(___, arm)\n\n\n\nChaining analysis steps into pipelines\nWe can chain multiple analysis steps into a pipeline by continuing to add “and then” statements. For example, data %&gt;% count(...) %&gt;% arrange(...) would first count and then sort the data.\nTry this out by counting the number of participants in each study arm and then sorting by the number of participants\n\ntable_01 %&gt;%\n  count(___) %&gt;%\n  arrange(___)",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 2</b>: R Refresher"
    ]
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/index.html#exercise-2.2-aesthetics-using-ggplot",
    "href": "materials/2-workshop2/2-r-refresher/index.html#exercise-2.2-aesthetics-using-ggplot",
    "title": "R refresher",
    "section": "Exercise 2.2 : Aesthetics using ggplot",
    "text": "Exercise 2.2 : Aesthetics using ggplot\nFor this exercise, we will be using the instructional dataset (path to dataset)\nIn this worksheet, we will work on ggplot and the mapping of data values onto aesthetics.\n\nlibrary(tidyverse) # load the tidyverse library\n\n# we want to use the data in the visit_clinical_measurements file\nclinical_measurements &lt;- read_csv(path to dataset) # read in your data \n\n#then show the first few rows\nhead(clinical_measurements)\n\n\nBasic geom examples\nRemember: we call the ggplot() function with a dataset and an aesthetic mapping (created with aes()), and then we add a geom, such as geom_line() to draw lines or geom_point() to draw points.\nTry this for yourself. Map the column ph onto the x axis and the column crp_blood onto the y axis, and use geom_line() to display the data.\nWhenever you see ___ in the code below, that means you should swap it in with your own code.\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\nTry again. Now use geom_point() instead of geom_line().\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\nAnd now swap which column you map to x and which to y.\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\n\n\nMore complex geoms\nYou can use other geoms to make different types of plots. For example, geom_boxplot() will make boxplots. For boxplots, we frequently want categorical data on the x or y axis. For example, we might want a separate boxplot for each month. Try this out. Put nugent_score on the x axis, ph on the y axis, and use geom_boxplot().\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\nNow try swapping the x and y axis geom_jitter()\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\nNow try swapping the x and y axis\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\n\n\nAdding color\nTry again with geom_jitter(), this time using ph as the location along the y axis and arm for the color. Remember to check the ggplot cheat sheet, or type ?geom_jitter() in the console to and look at the “Aesthetics” section if you get stuck.\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\n(Hint: Try adding size = 3 as a parameter to the geom_jitter() to create larger points.)",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 2</b>: R Refresher"
    ]
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/index.html#exercise-2.3-table-manipulations",
    "href": "materials/2-workshop2/2-r-refresher/index.html#exercise-2.3-table-manipulations",
    "title": "R refresher",
    "section": "Exercise 2.3: Table manipulations",
    "text": "Exercise 2.3: Table manipulations\nIn this exercise, we will work with basic data manipulations, grouping and summarizing, making data tables wider or longer, and joining data tables.\nWe will be using the R package, tidyverse for the data manipulation functions %&gt;%, group_by(), summarize(), pivot_wider(), pivot_longer(), and join functions such as left_join()\nPaste the following into the top code chunk of your qmd file.\nlibrary(tidyverse)\n\ntable_01 &lt;- read_csv(\"01_participant_metadata_UKZN_workshop_2023.csv\")\n\ntable_02 &lt;- read_csv(\"02_visit_clinical_measurements_UKZN_workshop_2023.csv\")\n\nAnalyzing subsets\nGroup the table_01 dataset by education_level and smoker.\n\ntable_01 %&gt;%\n  ___ %&gt;%\n  print()\n\nAnswer for yourself How many distinct groups did this produce?\nNow undo the previous grouping.\n\n```{r}\n# build all the code for this exercise\n```\n\nAlso verify what the output looks like when you omit the print() function at the end.\n\n\nPerforming summaries\nOnce we have set up a grouping for a data table, we can then calculate summary data with the summarize() function. This function works similarly to mutate(), in that we provide it with statements of the form &lt;new column name&gt; = &lt;computation&gt;, where &lt;new column name&gt; stands for the name of the new column that is being created and &lt;computation&gt; stands for the computation that is used to generate the values in the new column.\nAs an example, using table_02 we want to calculate the median ph of participants, by arm, we could write summarise(median_ph = median(ph)), and this would create a new column called median_ph\nTry this out. First group by arm and then make the new column:\n\ntable_02 %&gt;%\n  group_by(____) %&gt;%\n  summarise(___)\n\nNow see what it looks like if you instead group by timepoint\n\ntable_02 %&gt;%\n  group_by(____) %&gt;%\n  summarise(___)\n\nNow try grouping by both timepoint and arm\n\ntable_02 %&gt;%\n  group_by(__, __) %&gt;%\n  summarise(___)\n\nWe can perform multiple summaries at once by adding more statements inside the summarise() function, separated by a ,. To try this out, calculate the median nugent in addition to the median ph\n\ntable_02 %&gt;%\n  group_by(____) %&gt;%\n  summarise(___, ___)\n\nWhen performing summaries, we often want to know how many observations there are in each group (i.e., we want to count). We can do this with the function n(), which inside summarise() gives us the group size. So, we can count by adding a statement such as count = n() inside `summarise(). Try this out.\n\ntable_02 %&gt;%\n  group_by(____) %&gt;%\n  summarise(___, ___)\n\n\n\nMaking tables wider or longer\nFor efficient data processing, we usually want tables in long form, where each columns is one variable and each row is one observation. However, in some applications, for example when making a table easier to read for humans, a wide format can be preferred. In a wide format, some variables are displayed as column names, and other variables are distributed over multiple columns.\nFirst, make a summary table that shows median ph by arm and time_point, just like you did above, and save it to a variable ph_summary_long\n\nph_summary_long &lt;- table_02 %&gt;%\n  group_by(___) %&gt;%\n  summarise(___)\n\nNow, try using pivot_wider() to make a column for each arm. Remember, use ?pivot_wider if you want help, and try asking google or chatGPT if you get stuck.\n\nph_summary_long %&gt;%\n  pivot_wider(____)\n\nWhat if you wanted to instead make a column for each time point, and have the arms be different rows?\n\nph_summary_long %&gt;%\n  pivot_wider(____)\n\n\n\nCombining datasets with joins\nFinally, we sometimes encounter the situation where we have two data sets that contain different pieces of information about the same subjects or objects, and we need to merge these tables for further analysis. In this situation, we need to perform a join, and there are multiple different types of joins available: left_join(), right_join(), inner_join(), full_join(). These joins all differ in how they handle cases where an observation is present in only one of the two tables but missing in the other.\nOur instructional dataset has no missing values, so all types of joins are actually equivalent. Try to join table_01 and table_02 using left_join()\n\n```{r}\n# join table_01 and table_02\n```",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 2</b>: R Refresher"
    ]
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/index.html#exercise-2.4-color-scales",
    "href": "materials/2-workshop2/2-r-refresher/index.html#exercise-2.4-color-scales",
    "title": "R refresher",
    "section": "Exercise 2.4 : Color scales",
    "text": "Exercise 2.4 : Color scales\nIn this worksheet, we will discuss how to change and customize color scales.\nWe will be using the R package tidyverse, which includes ggplot() and related functions. We will also be using the R package colorspace for the scale functions it provides.\n\n# load required library\nlibrary(tidyverse)\nlibrary(colorspace)\n\ntemperatures &lt;- read_csv(\"https://wilkelab.org/SDS375/datasets/tempnormals.csv\") %&gt;%\n  mutate(\n    location = factor(\n      location, levels = c(\"Death Valley\", \"Houston\", \"San Diego\", \"Chicago\")\n    )\n  ) %&gt;%\n  select(location, day_of_year, month, temperature)\n\ntemps_months &lt;- read_csv(\"https://wilkelab.org/SDS375/datasets/tempnormals.csv\") %&gt;%\n  group_by(location, month_name) %&gt;%\n  summarize(mean = mean(temperature)) %&gt;%\n  mutate(\n    month = factor(\n      month_name,\n      levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n    ),\n    location = factor(\n      location, levels = c(\"Death Valley\", \"Houston\", \"San Diego\", \"Chicago\")\n    )\n  ) %&gt;%\n  select(-month_name)\n\nWe will be working with the dataset temperatures that we have used in previous worksheets. This dataset contains the average temperature for each day of the year for four different locations.\n\ntemperatures\n\n# A tibble: 1,464 × 4\n   location     day_of_year month temperature\n   &lt;fct&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 Death Valley           1 01           51  \n 2 Death Valley           2 01           51.2\n 3 Death Valley           3 01           51.3\n 4 Death Valley           4 01           51.4\n 5 Death Valley           5 01           51.6\n 6 Death Valley           6 01           51.7\n 7 Death Valley           7 01           51.9\n 8 Death Valley           8 01           52  \n 9 Death Valley           9 01           52.2\n10 Death Valley          10 01           52.3\n# ℹ 1,454 more rows\n\n\nWe will also be working with an aggregated version of this dataset called temps_months, which contains the mean temperature for each month for the same locations.\n\ntemps_months\n\n# A tibble: 48 × 3\n# Groups:   location [4]\n   location  mean month\n   &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;\n 1 Chicago   50.4 Apr  \n 2 Chicago   74.1 Aug  \n 3 Chicago   29   Dec  \n 4 Chicago   28.9 Feb  \n 5 Chicago   24.8 Jan  \n 6 Chicago   75.8 Jul  \n 7 Chicago   71.0 Jun  \n 8 Chicago   38.8 Mar  \n 9 Chicago   60.9 May  \n10 Chicago   41.6 Nov  \n# ℹ 38 more rows\n\n\nAs a challenge, try to create this above table yourself using group_by() and summarize(), and then make a month column which is a factor with levels froing from “Jan” to “Dec”, and make the location column a factor with levels “Death Valley”, “Houston”, “San Diego”, “Chicago”. If you are having trouble, the solution is at the end of this page, make sure you copy it into your code so the rest of the exercise works.\n\n# check solution at the end before moving on!\ntemps_months &lt;- read_csv(\"https://wilkelab.org/SDS375/datasets/tempnormals.csv\") %&gt;%\n  group_by(___) %&gt;%\n  summarize(___) %&gt;%\n  mutate(\n    month = factor(\n      month_name,\n      ___\n    ),\n    location = factor(\n      location, ___\n    )\n  ) %&gt;%\n  select(-month_name)\n\n\nBuilt in ggplot2 color scales\nWe will start with built-in ggplot2 color scales, which require no additional packages. The scale functions are always named scale_color_*() or scale_fill_*(), depending on whether they apply to the color or fill aesthetic. The * indicates some other words specifying the type of the scale, for example scale_color_brewer() or scale_color_distiller() for discrete or continuous scales from the ColorBrewer project, respectively. You can find all available built-in scales here: https://ggplot2.tidyverse.org/reference/index.html#section-scales\nNow consider the following plot.\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE)\n\n\n\n\n\n\n\n\nIf you wanted to change the color scale to one from the ColorBrewer project, which scale function would you have to add? scale_color_brewer(), scale_color_distiller(), scale_fill_brewer(), scale_fill_distiller()?\n\n # answer the question above to yourself\n\nNow try this out.\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE) +\n  ___\n\nMost color scale functions have additional customizations. How to use them depends on the specific scale function. For the ColorBrewer scales you can set direction = 1 or direction = -1 to set the direction of the scale (light to dark or dark to light). You can also set the palette via a numeric argument, e.g. palette = 1, palette = 2, palette = 3 etc.\nTry this out by setting the direction of the scale from light to dark and using palette #4.\n\n # build all the code for this exercise\n\n\n\nManual scales\nFor discrete data with a small number of categories, it’s usually best to set colors manually. This can be done with the scale functions scale_*_manual(). These functions take an argument values that specifies the color values to use.\nTo see how this works, let’s go back to this plot of temperatures over time for four locations.\n\nggplot(temperatures, aes(day_of_year, temperature, color = location)) +\n  geom_line(size = 1.5)\n\n\n\n\n\n\n\n\nLet’s use the following four colors: \"gold2\", \"firebrick\", \"blue3\", \"springgreen4\". We can visualize this palette using the function swatchplot() from the colorspace package.\n\ncolorspace::swatchplot(c(\"gold2\", \"firebrick\", \"blue3\", \"springgreen4\"))\n\n\n\n\n\n\n\n\nNow apply this color palette to the temperatures plot, by using the manual color scale. Hint: use the values argument to provide the colors to the manual scale.\n\nggplot(temperatures, aes(day_of_year, temperature, color = location)) +\n  geom_line(size = 1.5) +\n  ___\n\nOne problem with this approach is that we can’t easily control which data value gets assigned to which color. What if we wanted San Diego to be shown in green and Chicago in blue? The simplest way to resolve this issue is to use a named vector. A named vector in R is a vector where each value has a name. Named vectors are created by writing c(name1 = value1, name2 = value2, ...). See the following example.\n\n# regular vector\nc(\"cat\", \"mouse\", \"house\")\n\n[1] \"cat\"   \"mouse\" \"house\"\n\n# named vector\nc(A = \"cat\", B = \"mouse\", C = \"house\")\n\n      A       B       C \n  \"cat\" \"mouse\" \"house\" \n\n\nThe names in the second example are A, B, and C. Notice that the names are not in quotes. However, if you need a name containing a space (such as Death Valley), you need to enclose the name in backticks. Thus, our named vector of colors could be written like so:\n\nc(`Death Valley` = \"gold2\", Houston = \"firebrick\", Chicago = \"blue3\", `San Diego` = \"springgreen4\")\n\n  Death Valley        Houston        Chicago      San Diego \n       \"gold2\"    \"firebrick\"        \"blue3\" \"springgreen4\" \n\n\nNow try to use this color vector in the figure showing temperatures over time.\n\n # build all the code for this exercise\n\nTry some other colors also. You can find a list of all named colors here. You can also run the command colors() in your R console to get a list of all available color names.\nHint: It’s a good idea to never use the colors \"red\", \"green\", \"blue\", \"cyan\", \"magenta\", \"yellow\". They are extreme points in the RGB color space and tend to look unnatural and too crazy. Try this by making a swatch plot of these colors, and compare for example to the color scale containing the colors \"firebrick\", \"springgreen4\", \"blue3\", \"turquoise3\", \"darkorchid2\", \"gold2\". Do you see the difference?\n\n # build all the code for this exercise\n\nSolution to the challenge to make the summary table of mean temperature by month:\n\n# paste this below the \"temperatures\" code-chunk\ntemps_months &lt;- read_csv(\"https://wilkelab.org/SDS375/datasets/tempnormals.csv\") %&gt;%\n  group_by(location, month_name) %&gt;%\n  summarize(mean = mean(temperature)) %&gt;%\n  mutate(\n    month = factor(\n      month_name,\n      levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n    ),\n    location = factor(\n      location, levels = c(\"Death Valley\", \"Houston\", \"San Diego\", \"Chicago\")\n    )\n  ) %&gt;%\n  select(-month_name)",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 2</b>: R Refresher"
    ]
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/index.html#exercise-2.5-compund-figures",
    "href": "materials/2-workshop2/2-r-refresher/index.html#exercise-2.5-compund-figures",
    "title": "R refresher",
    "section": "Exercise 2.5 : Compund figures",
    "text": "Exercise 2.5 : Compund figures\nIn this worksheet, we will discuss how to combine several ggplot2 plots into one compound figure.\nWe will be using the R package tidyverse, which includes ggplot() and related functions. We will also be using the package patchwork for plot composition.\n\n# load required library\nlibrary(tidyverse)\nlibrary(patchwork)\n\nWe will be working with the dataset mtcars, which contains fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).\n\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\n\nCombining plots\nFirst we set up four different plots that we will subsequently combine. The plots are stored in variables p1, p2, p3, p4.\n\np1 &lt;- ggplot(mtcars) + \n  geom_point(aes(mpg, disp))\np1  \n\n\n\n\n\n\n\np2 &lt;- ggplot(mtcars) + \n  geom_boxplot(aes(gear, disp, group = gear))\np2\n\n\n\n\n\n\n\np3 &lt;- ggplot(mtcars) + \n  geom_smooth(aes(disp, qsec))\np3\n\n\n\n\n\n\n\np4 &lt;- ggplot(mtcars) + \n  geom_bar(aes(carb))\np4\n\n\n\n\n\n\n\n\nTo show plots side-by-side, we use the operator |, as in p1 | p2. Try this by making a compound plot of plots p1, p2, p3 side-by-side.\n\n # build all the code for this exercise\n\nTo show plots on top of one-another, we use the operator /, as in p1 / p2. Try this by making a compound plot of plots p1, p2, p3 on top of each other.\n\n # build all the code for this exercise\n\nWe can also use parentheses to group plots with respect to the operators | and /. For example, we can place several plots side-by-side and then place this entire row of plots on top of another plot. Try putting p1, p2, p3, on the top row, and p4 on the bottom row.\n\n(___) / ___",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 2</b>: R Refresher"
    ]
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/exerciseRrefresher.html",
    "href": "materials/2-workshop2/2-r-refresher/exerciseRrefresher.html",
    "title": "Data Science for Biology Workshop Series",
    "section": "",
    "text": "In this worksheet, we will discuss how to perform basic data manipulations, such as filtering data rows that meet certain conditions, choosing data columns, and arranging data in ascending or descending order.\nWe will be using the R package, tidyverse for the data manipulation functions %&gt;%, filter(), select(), arrange(), count(), and mutate().\n\n\nWhen writing complex data analysis pipelines, we frequently use the pipe operator %&gt;% to move data from one analysis step to the next. The pipe is pronounced “and then”, and it takes the data on its left and uses it as the first argument for the function on its right.\nFor example, to see the first few lines of a dataset, we often write head(data). Instead, we can write data %&gt;% head().\nTry this yourself. Write code that displays the first few lines of table_01, using %&gt;% and head():\n\n```{r}\n#| eval: false\n#| warning: false\n\nlibrary(tidyverse)\n\ntable_01 &lt;- read_csv(\"../instructional_dataset/data/01_participant_metadata_UKZN_workshop_2023.csv\")\n\ntable_02 &lt;- read_csv(\"../instructional_dataset/data/02_visit_clinical_measurements_UKZN_workshop_2023.csv\")\n```\n\n\ntable_01 ___ head()\n\n\n\n\nThe function filter() allows you to find rows in a dataset that meet one or more specific conditions. The syntax is data %&gt;% filter(condition), where condition is a logical condition. For example, filter(x &gt; 5) would pick all rows for which the value in column x is greater than 5.\nBelow, write code to pick all measurements from the placebo arm in table_02 :\n\ntable_02 %&gt;%\n  filter(___)\n\n\n\n\nYou can also state multiple conditions, separated by a comma. For example, filter(x &gt; 5, y &lt; 2) would pick all rows for which the value in the column x is greater than 5 and the value in the column y is less than 2. Note that the conditions are combined via logical and, both need to be satisfied for the row to be picked.\nTo try this out, pick all measurements at baseline for the placebo arm in table_02 :\n\ntable_02 %&gt;%\n  filter(time_point == ___, arm == ___)\n\n\n\n\nThe function select() allows you to pick specific data columns by name. This is frequently useful when a dataset has many more columns than we are interested in at the time. For example, if we are only interested in the participants’ education, and sex, we could select these three columns:\n\ntable_01 %&gt;%\n  select(education, sex)\n\n# A tibble: 44 × 2\n   education                     sex  \n   &lt;chr&gt;                         &lt;lgl&gt;\n 1 grade 10-12, matriculated     FALSE\n 2 grade 10-12, matriculated     FALSE\n 3 post-secondary                FALSE\n 4 grade 10-12, not matriculated FALSE\n 5 grade 10-12, matriculated     FALSE\n 6 post-secondary                FALSE\n 7 grade 10-12, not matriculated FALSE\n 8 grade 10-12, not matriculated FALSE\n 9 grade 10-12, not matriculated FALSE\n10 less than grade 9             FALSE\n# ℹ 34 more rows\n\n\nTry this yourself, picking the columns representing the participant age (age), and then study arm (arm).\n\ntable_01 %&gt;%\n  select(___)\n\n\n\n\nAnother situation that arises frequently is one where we want to remove specific columns. We can also do this with select(), but now write select(-column) to remove one or more columns.\nTry this. Remove the column smoker.\n\ntable_01 %&gt;%\n  select(___)\n\nAnd now remove both smoker and education.\n\ntable_01 %&gt;%\n  select(-___, -___)\n\n\n\n\nThe function arrange() allows you to sort data by one or more columns. For example, data %&gt;% arrange(x) would sort the data by increasing values of x, and data %&gt;% arrange(x, y) would sort the data first by x and then, for ties in x, by y.\nSort the participants by age:\n\ntable_01 %&gt;%\n  arrange(___)\n\nTo arrange data in descending order, enclose the data column in desc(). For example, data %&gt;% arrange(desc(x)) would sort the data by decreasing values of x. (desc stands for “descending”.)\n\n\n\nWe frequently want to count how many times a particular value or combination of values occurs in a dataset. We do this using the count() function. For example, the following code counts how many participants there are in each study arm in table_01.\n\ntable_01 %&gt;%\n  count(arm)\n\n# A tibble: 2 × 2\n  arm           n\n  &lt;chr&gt;     &lt;int&gt;\n1 placebo      23\n2 treatment    21\n\n\nNow try this yourself. Count how many smokers and non smokers there are.\n\ntable_01 %&gt;%\n  count(___)\n\nWe can also perform more fine-grained counts, by providing the count() function with more than one column name. See if you can count how many smokers and non smokers the dataset contains for each treatment arm\n\ntable_01 %&gt;%\n  count(___, smoker)\n\nNow count how many participants are in each study arm for each education level in the dataset\n\ntable_01 %&gt;%\n  count(___, arm)\n\n\n\n\nWe can chain multiple analysis steps into a pipeline by continuing to add “and then” statements. For example, data %&gt;% count(...) %&gt;% arrange(...) would first count and then sort the data.\nTry this out by counting the number of participants in each study arm and then sorting by the number of participants\n\ntable_01 %&gt;%\n  count(___) %&gt;%\n  arrange(___)"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/exerciseRrefresher.html#exercise-2.1-data-wrangling-and-simple-manipulations",
    "href": "materials/2-workshop2/2-r-refresher/exerciseRrefresher.html#exercise-2.1-data-wrangling-and-simple-manipulations",
    "title": "Data Science for Biology Workshop Series",
    "section": "",
    "text": "In this worksheet, we will discuss how to perform basic data manipulations, such as filtering data rows that meet certain conditions, choosing data columns, and arranging data in ascending or descending order.\nWe will be using the R package, tidyverse for the data manipulation functions %&gt;%, filter(), select(), arrange(), count(), and mutate().\n\n\nWhen writing complex data analysis pipelines, we frequently use the pipe operator %&gt;% to move data from one analysis step to the next. The pipe is pronounced “and then”, and it takes the data on its left and uses it as the first argument for the function on its right.\nFor example, to see the first few lines of a dataset, we often write head(data). Instead, we can write data %&gt;% head().\nTry this yourself. Write code that displays the first few lines of table_01, using %&gt;% and head():\n\n```{r}\n#| eval: false\n#| warning: false\n\nlibrary(tidyverse)\n\ntable_01 &lt;- read_csv(\"../instructional_dataset/data/01_participant_metadata_UKZN_workshop_2023.csv\")\n\ntable_02 &lt;- read_csv(\"../instructional_dataset/data/02_visit_clinical_measurements_UKZN_workshop_2023.csv\")\n```\n\n\ntable_01 ___ head()\n\n\n\n\nThe function filter() allows you to find rows in a dataset that meet one or more specific conditions. The syntax is data %&gt;% filter(condition), where condition is a logical condition. For example, filter(x &gt; 5) would pick all rows for which the value in column x is greater than 5.\nBelow, write code to pick all measurements from the placebo arm in table_02 :\n\ntable_02 %&gt;%\n  filter(___)\n\n\n\n\nYou can also state multiple conditions, separated by a comma. For example, filter(x &gt; 5, y &lt; 2) would pick all rows for which the value in the column x is greater than 5 and the value in the column y is less than 2. Note that the conditions are combined via logical and, both need to be satisfied for the row to be picked.\nTo try this out, pick all measurements at baseline for the placebo arm in table_02 :\n\ntable_02 %&gt;%\n  filter(time_point == ___, arm == ___)\n\n\n\n\nThe function select() allows you to pick specific data columns by name. This is frequently useful when a dataset has many more columns than we are interested in at the time. For example, if we are only interested in the participants’ education, and sex, we could select these three columns:\n\ntable_01 %&gt;%\n  select(education, sex)\n\n# A tibble: 44 × 2\n   education                     sex  \n   &lt;chr&gt;                         &lt;lgl&gt;\n 1 grade 10-12, matriculated     FALSE\n 2 grade 10-12, matriculated     FALSE\n 3 post-secondary                FALSE\n 4 grade 10-12, not matriculated FALSE\n 5 grade 10-12, matriculated     FALSE\n 6 post-secondary                FALSE\n 7 grade 10-12, not matriculated FALSE\n 8 grade 10-12, not matriculated FALSE\n 9 grade 10-12, not matriculated FALSE\n10 less than grade 9             FALSE\n# ℹ 34 more rows\n\n\nTry this yourself, picking the columns representing the participant age (age), and then study arm (arm).\n\ntable_01 %&gt;%\n  select(___)\n\n\n\n\nAnother situation that arises frequently is one where we want to remove specific columns. We can also do this with select(), but now write select(-column) to remove one or more columns.\nTry this. Remove the column smoker.\n\ntable_01 %&gt;%\n  select(___)\n\nAnd now remove both smoker and education.\n\ntable_01 %&gt;%\n  select(-___, -___)\n\n\n\n\nThe function arrange() allows you to sort data by one or more columns. For example, data %&gt;% arrange(x) would sort the data by increasing values of x, and data %&gt;% arrange(x, y) would sort the data first by x and then, for ties in x, by y.\nSort the participants by age:\n\ntable_01 %&gt;%\n  arrange(___)\n\nTo arrange data in descending order, enclose the data column in desc(). For example, data %&gt;% arrange(desc(x)) would sort the data by decreasing values of x. (desc stands for “descending”.)\n\n\n\nWe frequently want to count how many times a particular value or combination of values occurs in a dataset. We do this using the count() function. For example, the following code counts how many participants there are in each study arm in table_01.\n\ntable_01 %&gt;%\n  count(arm)\n\n# A tibble: 2 × 2\n  arm           n\n  &lt;chr&gt;     &lt;int&gt;\n1 placebo      23\n2 treatment    21\n\n\nNow try this yourself. Count how many smokers and non smokers there are.\n\ntable_01 %&gt;%\n  count(___)\n\nWe can also perform more fine-grained counts, by providing the count() function with more than one column name. See if you can count how many smokers and non smokers the dataset contains for each treatment arm\n\ntable_01 %&gt;%\n  count(___, smoker)\n\nNow count how many participants are in each study arm for each education level in the dataset\n\ntable_01 %&gt;%\n  count(___, arm)\n\n\n\n\nWe can chain multiple analysis steps into a pipeline by continuing to add “and then” statements. For example, data %&gt;% count(...) %&gt;% arrange(...) would first count and then sort the data.\nTry this out by counting the number of participants in each study arm and then sorting by the number of participants\n\ntable_01 %&gt;%\n  count(___) %&gt;%\n  arrange(___)"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/exerciseRrefresher.html#exercise-2.2-aesthetics-using-ggplot",
    "href": "materials/2-workshop2/2-r-refresher/exerciseRrefresher.html#exercise-2.2-aesthetics-using-ggplot",
    "title": "Data Science for Biology Workshop Series",
    "section": "Exercise 2.2 : Aesthetics using ggplot",
    "text": "Exercise 2.2 : Aesthetics using ggplot\nFor this exercise, we will be using the instructional dataset (path to dataset)\nIn this worksheet, we will work on ggplot and the mapping of data values onto aesthetics.\n\nlibrary(tidyverse) # load the tidyverse library\n\n# we want to use the data in the visit_clinical_measurements file\nclinical_measurements &lt;- read_csv(path to dataset) # read in your data \n\n#then show the first few rows\nhead(clinical_measurements)\n\n\nBasic geom examples\nRemember: we call the ggplot() function with a dataset and an aesthetic mapping (created with aes()), and then we add a geom, such as geom_line() to draw lines or geom_point() to draw points.\nTry this for yourself. Map the column ph onto the x axis and the column crp_blood onto the y axis, and use geom_line() to display the data.\nWhenever you see ___ in the code below, that means you should swap it in with your own code.\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\nTry again. Now use geom_point() instead of geom_line().\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\nAnd now swap which column you map to x and which to y.\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\n\n\nMore complex geoms\nYou can use other geoms to make different types of plots. For example, geom_boxplot() will make boxplots. For boxplots, we frequently want categorical data on the x or y axis. For example, we might want a separate boxplot for each month. Try this out. Put nugent_score on the x axis, ph on the y axis, and use geom_boxplot().\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\nNow try swapping the x and y axis geom_jitter()\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\nNow try swapping the x and y axis\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\n\n\nAdding color\nTry again with geom_jitter(), this time using ph as the location along the y axis and arm for the color. Remember to check the ggplot cheat sheet, or type ?geom_jitter() in the console to and look at the “Aesthetics” section if you get stuck.\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\n(Hint: Try adding size = 3 as a parameter to the geom_jitter() to create larger points.)"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/exerciseRrefresher.html#exercise-2.3-table-manipulations",
    "href": "materials/2-workshop2/2-r-refresher/exerciseRrefresher.html#exercise-2.3-table-manipulations",
    "title": "Data Science for Biology Workshop Series",
    "section": "Exercise 2.3: Table manipulations",
    "text": "Exercise 2.3: Table manipulations\nIn this exercise, we will work with basic data manipulations, grouping and summarizing, making data tables wider or longer, and joining data tables.\nWe will be using the R package, tidyverse for the data manipulation functions %&gt;%, group_by(), summarize(), pivot_wider(), pivot_longer(), and join functions such as left_join()\nPaste the following into the top code chunk of your qmd file.\nlibrary(tidyverse)\n\ntable_01 &lt;- read_csv(\"01_participant_metadata_UKZN_workshop_2023.csv\")\n\ntable_02 &lt;- read_csv(\"02_visit_clinical_measurements_UKZN_workshop_2023.csv\")\n\nAnalyzing subsets\nGroup the table_01 dataset by education_level and smoker.\n\ntable_01 %&gt;%\n  ___ %&gt;%\n  print()\n\nAnswer for yourself How many distinct groups did this produce?\nNow undo the previous grouping.\n\n```{r}\n# build all the code for this exercise\n```\n\nAlso verify what the output looks like when you omit the print() function at the end.\n\n\nPerforming summaries\nOnce we have set up a grouping for a data table, we can then calculate summary data with the summarize() function. This function works similarly to mutate(), in that we provide it with statements of the form &lt;new column name&gt; = &lt;computation&gt;, where &lt;new column name&gt; stands for the name of the new column that is being created and &lt;computation&gt; stands for the computation that is used to generate the values in the new column.\nAs an example, using table_02 we want to calculate the median ph of participants, by arm, we could write summarise(median_ph = median(ph)), and this would create a new column called median_ph\nTry this out. First group by arm and then make the new column:\n\ntable_02 %&gt;%\n  group_by(____) %&gt;%\n  summarise(___)\n\nNow see what it looks like if you instead group by timepoint\n\ntable_02 %&gt;%\n  group_by(____) %&gt;%\n  summarise(___)\n\nNow try grouping by both timepoint and arm\n\ntable_02 %&gt;%\n  group_by(__, __) %&gt;%\n  summarise(___)\n\nWe can perform multiple summaries at once by adding more statements inside the summarise() function, separated by a ,. To try this out, calculate the median nugent in addition to the median ph\n\ntable_02 %&gt;%\n  group_by(____) %&gt;%\n  summarise(___, ___)\n\nWhen performing summaries, we often want to know how many observations there are in each group (i.e., we want to count). We can do this with the function n(), which inside summarise() gives us the group size. So, we can count by adding a statement such as count = n() inside `summarise(). Try this out.\n\ntable_02 %&gt;%\n  group_by(____) %&gt;%\n  summarise(___, ___)\n\n\n\nMaking tables wider or longer\nFor efficient data processing, we usually want tables in long form, where each columns is one variable and each row is one observation. However, in some applications, for example when making a table easier to read for humans, a wide format can be preferred. In a wide format, some variables are displayed as column names, and other variables are distributed over multiple columns.\nFirst, make a summary table that shows median ph by arm and time_point, just like you did above, and save it to a variable ph_summary_long\n\nph_summary_long &lt;- table_02 %&gt;%\n  group_by(___) %&gt;%\n  summarise(___)\n\nNow, try using pivot_wider() to make a column for each arm. Remember, use ?pivot_wider if you want help, and try asking google or chatGPT if you get stuck.\n\nph_summary_long %&gt;%\n  pivot_wider(____)\n\nWhat if you wanted to instead make a column for each time point, and have the arms be different rows?\n\nph_summary_long %&gt;%\n  pivot_wider(____)\n\n\n\nCombining datasets with joins\nFinally, we sometimes encounter the situation where we have two data sets that contain different pieces of information about the same subjects or objects, and we need to merge these tables for further analysis. In this situation, we need to perform a join, and there are multiple different types of joins available: left_join(), right_join(), inner_join(), full_join(). These joins all differ in how they handle cases where an observation is present in only one of the two tables but missing in the other.\nOur instructional dataset has no missing values, so all types of joins are actually equivalent. Try to join table_01 and table_02 using left_join()\n\n```{r}\n# join table_01 and table_02\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/exerciseRrefresher.html#exercise-2.4-color-scales",
    "href": "materials/2-workshop2/2-r-refresher/exerciseRrefresher.html#exercise-2.4-color-scales",
    "title": "Data Science for Biology Workshop Series",
    "section": "Exercise 2.4 : Color scales",
    "text": "Exercise 2.4 : Color scales\nIn this worksheet, we will discuss how to change and customize color scales.\nWe will be using the R package tidyverse, which includes ggplot() and related functions. We will also be using the R package colorspace for the scale functions it provides.\n\n# load required library\nlibrary(tidyverse)\nlibrary(colorspace)\n\ntemperatures &lt;- read_csv(\"https://wilkelab.org/SDS375/datasets/tempnormals.csv\") %&gt;%\n  mutate(\n    location = factor(\n      location, levels = c(\"Death Valley\", \"Houston\", \"San Diego\", \"Chicago\")\n    )\n  ) %&gt;%\n  select(location, day_of_year, month, temperature)\n\ntemps_months &lt;- read_csv(\"https://wilkelab.org/SDS375/datasets/tempnormals.csv\") %&gt;%\n  group_by(location, month_name) %&gt;%\n  summarize(mean = mean(temperature)) %&gt;%\n  mutate(\n    month = factor(\n      month_name,\n      levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n    ),\n    location = factor(\n      location, levels = c(\"Death Valley\", \"Houston\", \"San Diego\", \"Chicago\")\n    )\n  ) %&gt;%\n  select(-month_name)\n\nWe will be working with the dataset temperatures that we have used in previous worksheets. This dataset contains the average temperature for each day of the year for four different locations.\n\ntemperatures\n\n# A tibble: 1,464 × 4\n   location     day_of_year month temperature\n   &lt;fct&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 Death Valley           1 01           51  \n 2 Death Valley           2 01           51.2\n 3 Death Valley           3 01           51.3\n 4 Death Valley           4 01           51.4\n 5 Death Valley           5 01           51.6\n 6 Death Valley           6 01           51.7\n 7 Death Valley           7 01           51.9\n 8 Death Valley           8 01           52  \n 9 Death Valley           9 01           52.2\n10 Death Valley          10 01           52.3\n# ℹ 1,454 more rows\n\n\nWe will also be working with an aggregated version of this dataset called temps_months, which contains the mean temperature for each month for the same locations.\n\ntemps_months\n\n# A tibble: 48 × 3\n# Groups:   location [4]\n   location  mean month\n   &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;\n 1 Chicago   50.4 Apr  \n 2 Chicago   74.1 Aug  \n 3 Chicago   29   Dec  \n 4 Chicago   28.9 Feb  \n 5 Chicago   24.8 Jan  \n 6 Chicago   75.8 Jul  \n 7 Chicago   71.0 Jun  \n 8 Chicago   38.8 Mar  \n 9 Chicago   60.9 May  \n10 Chicago   41.6 Nov  \n# ℹ 38 more rows\n\n\nAs a challenge, try to create this above table yourself using group_by() and summarize(), and then make a month column which is a factor with levels froing from “Jan” to “Dec”, and make the location column a factor with levels “Death Valley”, “Houston”, “San Diego”, “Chicago”. If you are having trouble, the solution is at the end of this page, make sure you copy it into your code so the rest of the exercise works.\n\n# check solution at the end before moving on!\ntemps_months &lt;- read_csv(\"https://wilkelab.org/SDS375/datasets/tempnormals.csv\") %&gt;%\n  group_by(___) %&gt;%\n  summarize(___) %&gt;%\n  mutate(\n    month = factor(\n      month_name,\n      ___\n    ),\n    location = factor(\n      location, ___\n    )\n  ) %&gt;%\n  select(-month_name)\n\n\nBuilt in ggplot2 color scales\nWe will start with built-in ggplot2 color scales, which require no additional packages. The scale functions are always named scale_color_*() or scale_fill_*(), depending on whether they apply to the color or fill aesthetic. The * indicates some other words specifying the type of the scale, for example scale_color_brewer() or scale_color_distiller() for discrete or continuous scales from the ColorBrewer project, respectively. You can find all available built-in scales here: https://ggplot2.tidyverse.org/reference/index.html#section-scales\nNow consider the following plot.\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE)\n\n\n\n\n\n\n\n\nIf you wanted to change the color scale to one from the ColorBrewer project, which scale function would you have to add? scale_color_brewer(), scale_color_distiller(), scale_fill_brewer(), scale_fill_distiller()?\n\n # answer the question above to yourself\n\nNow try this out.\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE) +\n  ___\n\nMost color scale functions have additional customizations. How to use them depends on the specific scale function. For the ColorBrewer scales you can set direction = 1 or direction = -1 to set the direction of the scale (light to dark or dark to light). You can also set the palette via a numeric argument, e.g. palette = 1, palette = 2, palette = 3 etc.\nTry this out by setting the direction of the scale from light to dark and using palette #4.\n\n # build all the code for this exercise\n\n\n\nManual scales\nFor discrete data with a small number of categories, it’s usually best to set colors manually. This can be done with the scale functions scale_*_manual(). These functions take an argument values that specifies the color values to use.\nTo see how this works, let’s go back to this plot of temperatures over time for four locations.\n\nggplot(temperatures, aes(day_of_year, temperature, color = location)) +\n  geom_line(size = 1.5)\n\n\n\n\n\n\n\n\nLet’s use the following four colors: \"gold2\", \"firebrick\", \"blue3\", \"springgreen4\". We can visualize this palette using the function swatchplot() from the colorspace package.\n\ncolorspace::swatchplot(c(\"gold2\", \"firebrick\", \"blue3\", \"springgreen4\"))\n\n\n\n\n\n\n\n\nNow apply this color palette to the temperatures plot, by using the manual color scale. Hint: use the values argument to provide the colors to the manual scale.\n\nggplot(temperatures, aes(day_of_year, temperature, color = location)) +\n  geom_line(size = 1.5) +\n  ___\n\nOne problem with this approach is that we can’t easily control which data value gets assigned to which color. What if we wanted San Diego to be shown in green and Chicago in blue? The simplest way to resolve this issue is to use a named vector. A named vector in R is a vector where each value has a name. Named vectors are created by writing c(name1 = value1, name2 = value2, ...). See the following example.\n\n# regular vector\nc(\"cat\", \"mouse\", \"house\")\n\n[1] \"cat\"   \"mouse\" \"house\"\n\n# named vector\nc(A = \"cat\", B = \"mouse\", C = \"house\")\n\n      A       B       C \n  \"cat\" \"mouse\" \"house\" \n\n\nThe names in the second example are A, B, and C. Notice that the names are not in quotes. However, if you need a name containing a space (such as Death Valley), you need to enclose the name in backticks. Thus, our named vector of colors could be written like so:\n\nc(`Death Valley` = \"gold2\", Houston = \"firebrick\", Chicago = \"blue3\", `San Diego` = \"springgreen4\")\n\n  Death Valley        Houston        Chicago      San Diego \n       \"gold2\"    \"firebrick\"        \"blue3\" \"springgreen4\" \n\n\nNow try to use this color vector in the figure showing temperatures over time.\n\n # build all the code for this exercise\n\nTry some other colors also. You can find a list of all named colors here. You can also run the command colors() in your R console to get a list of all available color names.\nHint: It’s a good idea to never use the colors \"red\", \"green\", \"blue\", \"cyan\", \"magenta\", \"yellow\". They are extreme points in the RGB color space and tend to look unnatural and too crazy. Try this by making a swatch plot of these colors, and compare for example to the color scale containing the colors \"firebrick\", \"springgreen4\", \"blue3\", \"turquoise3\", \"darkorchid2\", \"gold2\". Do you see the difference?\n\n # build all the code for this exercise\n\nSolution to the challenge to make the summary table of mean temperature by month:\n\n# paste this below the \"temperatures\" code-chunk\ntemps_months &lt;- read_csv(\"https://wilkelab.org/SDS375/datasets/tempnormals.csv\") %&gt;%\n  group_by(location, month_name) %&gt;%\n  summarize(mean = mean(temperature)) %&gt;%\n  mutate(\n    month = factor(\n      month_name,\n      levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n    ),\n    location = factor(\n      location, levels = c(\"Death Valley\", \"Houston\", \"San Diego\", \"Chicago\")\n    )\n  ) %&gt;%\n  select(-month_name)"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/exerciseRrefresher.html#exercise-2.5-compund-figures",
    "href": "materials/2-workshop2/2-r-refresher/exerciseRrefresher.html#exercise-2.5-compund-figures",
    "title": "Data Science for Biology Workshop Series",
    "section": "Exercise 2.5 : Compund figures",
    "text": "Exercise 2.5 : Compund figures\nIn this worksheet, we will discuss how to combine several ggplot2 plots into one compound figure.\nWe will be using the R package tidyverse, which includes ggplot() and related functions. We will also be using the package patchwork for plot composition.\n\n# load required library\nlibrary(tidyverse)\nlibrary(patchwork)\n\nWe will be working with the dataset mtcars, which contains fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).\n\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\n\nCombining plots\nFirst we set up four different plots that we will subsequently combine. The plots are stored in variables p1, p2, p3, p4.\n\np1 &lt;- ggplot(mtcars) + \n  geom_point(aes(mpg, disp))\np1  \n\n\n\n\n\n\n\np2 &lt;- ggplot(mtcars) + \n  geom_boxplot(aes(gear, disp, group = gear))\np2\n\n\n\n\n\n\n\np3 &lt;- ggplot(mtcars) + \n  geom_smooth(aes(disp, qsec))\np3\n\n\n\n\n\n\n\np4 &lt;- ggplot(mtcars) + \n  geom_bar(aes(carb))\np4\n\n\n\n\n\n\n\n\nTo show plots side-by-side, we use the operator |, as in p1 | p2. Try this by making a compound plot of plots p1, p2, p3 side-by-side.\n\n # build all the code for this exercise\n\nTo show plots on top of one-another, we use the operator /, as in p1 / p2. Try this by making a compound plot of plots p1, p2, p3 on top of each other.\n\n # build all the code for this exercise\n\nWe can also use parentheses to group plots with respect to the operators | and /. For example, we can place several plots side-by-side and then place this entire row of plots on top of another plot. Try putting p1, p2, p3, on the top row, and p4 on the bottom row.\n\n(___) / ___"
  },
  {
    "objectID": "materials/2-workshop2/0-welcome/slides.html#section",
    "href": "materials/2-workshop2/0-welcome/slides.html#section",
    "title": "Welcome to the workshop",
    "section": "",
    "text": "Goals for this session\n\n\nGet to know your instructors and neighbors\nSet expectations for the week\nGet excited!"
  },
  {
    "objectID": "materials/2-workshop2/0-welcome/slides.html#section-1",
    "href": "materials/2-workshop2/0-welcome/slides.html#section-1",
    "title": "Welcome to the workshop",
    "section": "",
    "text": "Workshop materials are at:\nhttps://elsherbini.github.io/durban-data-science-for-biology/"
  },
  {
    "objectID": "materials/2-workshop2/0-welcome/slides.html#discussions-discord",
    "href": "materials/2-workshop2/0-welcome/slides.html#discussions-discord",
    "title": "Welcome to the workshop",
    "section": "Discussions: discord",
    "text": "Discussions: discord\nAsk questions at #workshop-questions on https://discord.com/channels/1227264067899621427."
  },
  {
    "objectID": "materials/2-workshop2/0-welcome/slides.html#stickies",
    "href": "materials/2-workshop2/0-welcome/slides.html#stickies",
    "title": "Welcome to the workshop",
    "section": "Stickies",
    "text": "Stickies\n\n\n\n\n\n\nDuring an activity, place a yellow sticky on your laptop if you’re good to go and a pink sticky if you want help.\n\n\n\n\nImage by Megan Duffy"
  },
  {
    "objectID": "materials/2-workshop2/0-welcome/slides.html#practicalities",
    "href": "materials/2-workshop2/0-welcome/slides.html#practicalities",
    "title": "Welcome to the workshop",
    "section": "Practicalities",
    "text": "Practicalities\n\nWiFi:\nNetwork: Southernsunconference\nNetwork Password: S0uthernsun1"
  },
  {
    "objectID": "materials/2-workshop2/0-welcome/slides.html#introductions",
    "href": "materials/2-workshop2/0-welcome/slides.html#introductions",
    "title": "Welcome to the workshop",
    "section": "Introductions",
    "text": "Introductions\n\n\n\n−+\n03:00\n\n\n\nTake ~3 minutes to introduce yourself to your neighbors.\nPlease share …\n\nYour name\nWhere you’re from and where you work\nYour current go-to method for analyzing data"
  },
  {
    "objectID": "materials/2-workshop2/0-welcome/slides.html#your-instructors",
    "href": "materials/2-workshop2/0-welcome/slides.html#your-instructors",
    "title": "Welcome to the workshop",
    "section": "Your Instructors",
    "text": "Your Instructors\nWho are we?"
  },
  {
    "objectID": "materials/2-workshop2/0-welcome/slides.html#lets-make-this-workshop-work-for-all",
    "href": "materials/2-workshop2/0-welcome/slides.html#lets-make-this-workshop-work-for-all",
    "title": "Welcome to the workshop",
    "section": "Let’s make this workshop work for all",
    "text": "Let’s make this workshop work for all\n\n\nYou belong here. This workshop is intended for a wide-audience with a focus on beginners. If you feel out of place - it’s our problem, not yours!\nStay committed. This week-long workshop is intended to build each day and leave you with skills you can really use. Commit to stay engaged for best results, for you and your group!\nThis is a challenging but friendly environment. We are here to learn and grow. In order to make the right environment please follow “the 4 social rules” and code-of-conduct."
  },
  {
    "objectID": "materials/2-workshop2/0-welcome/slides.html#flow-of-the-workshop",
    "href": "materials/2-workshop2/0-welcome/slides.html#flow-of-the-workshop",
    "title": "Welcome to the workshop",
    "section": "Flow of the Workshop",
    "text": "Flow of the Workshop"
  },
  {
    "objectID": "materials/2-workshop2/0-welcome/slides.html#the-premise-of-the-workshop",
    "href": "materials/2-workshop2/0-welcome/slides.html#the-premise-of-the-workshop",
    "title": "Welcome to the workshop",
    "section": "The premise of the workshop",
    "text": "The premise of the workshop\nWe’ve extended the true-to-life dataset from workshop 1 to include high dimensional 16S data\n\nabout 40 participants split across two treatment arms\nthree time points (before treatment, after treatment, and longer follow-up)\nseveral measurements per time-point including cytokine concentrations and flow cytometry data\n16S sequencing for every time point, as well as microbial gene counts from shotgun metagenomics\n\nWe’ve also created group datasets so you can practice applying what you’ve learned on new data."
  },
  {
    "objectID": "materials/2-workshop2/0-welcome/slides.html#why-this-format",
    "href": "materials/2-workshop2/0-welcome/slides.html#why-this-format",
    "title": "Welcome to the workshop",
    "section": "Why this format",
    "text": "Why this format\nWe’ve taught workshops here before where we packed a lot (too much) into one week.\nBased on feedback, we focused on basic R skills at the first workshop, and will spend this week using R to analyze microbiome data as a case-study in higher dimensional data analysis.\nThis is the first time running this new workshop - please give lots of feedback on how to improve it for next time."
  },
  {
    "objectID": "materials/2-workshop2/0-welcome/slides.html#the-content-of-this-workshop",
    "href": "materials/2-workshop2/0-welcome/slides.html#the-content-of-this-workshop",
    "title": "Welcome to the workshop",
    "section": "The content of this workshop",
    "text": "The content of this workshop\n\nWe’ve created 6 modules as well as a group activity for this workshop.\n\nThere might be too much material to get through in this week!\n\nAs instructors we’re going to be trying to teach at the right pace to keep everyone learning all week.\n\nThe materials will stay on the website forever for you to work through at your own pace."
  },
  {
    "objectID": "materials/2-workshop2/0-welcome/slides.html#lets-take-a-poll",
    "href": "materials/2-workshop2/0-welcome/slides.html#lets-take-a-poll",
    "title": "Welcome to the workshop",
    "section": "Let’s take a poll",
    "text": "Let’s take a poll\nGo to the event on wooclap\n\n\n\n\nback to module"
  },
  {
    "objectID": "materials/2-workshop2/0-welcome/exercise_1.html#activity",
    "href": "materials/2-workshop2/0-welcome/exercise_1.html#activity",
    "title": "Exploring Data for Patterns",
    "section": "Activity",
    "text": "Activity\n\nPatterns are the essence of data exploration and our eyes’ ability to pick them out is integral to data understanding. Much of the data we work with, however, do not have a natural form and we need to make decisions about how they are to be represented. Try different ways to visualize the datasets so meaningful patterns may be found.\n\n\nGenetic profiles of cancer\nThese datasets contains 10 cancer samples. Table 1 describes the mutational status for a set of genes (A-E) and whether a mutation if absent (0) or present (1). Table 2 summarizes the expression levels of those genes, ranging from no expression (0) to high expression (3).\n\n\n\nTable 1: Mutational status for a set of genes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample -&gt;\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nGene A\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nGene B\n0\n0\n0\n0\n1\n1\n1\n0\n1\n1\n\n\nGene C\n0\n0\n1\n0\n0\n0\n1\n1\n1\n1\n\n\nGene D\n1\n1\n0\n0\n1\n1\n0\n0\n0\n0\n\n\nGene E\n0\n1\n1\n0\n1\n0\n0\n0\n1\n0\n\n\n\n\n\n\n\n\n\nTable 2: Expression levels for a set of genes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample -&gt;\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nGene A\n2\n1\n1\n2\n2\n0\n2\n1\n1\n2\n\n\nGene B\n1\n1\n2\n1\n0\n0\n0\n2\n0\n0\n\n\nGene C\n1\n1\n3\n1\n2\n2\n3\n0\n3\n0\n\n\nGene D\n0\n0\n2\n1\n3\n3\n2\n1\n1\n1\n\n\nGene E\n1\n3\n3\n1\n3\n1\n2\n1\n3\n2\n\n\n\n\n\n\n\n\n          1. Think about the problem on your own for 5 minutes.\n          2. In your groups, discuss and create different visualizations to highlight underlying patterns\n          3. Summarize the group’s approach\n          4. Elect/volunteer a spokesperson to present the solution\n\n\nConsider the following concepts when creating your visualizations\n\n\n\n\nPatterns\nPatterns are the essence of data exploration. What kinds of representation will produce the most meaningful insights?\n   \n\n\nEncodings\nSome visual estimations are easier to make than others. How might you use encodings that are less accurate but otherwise better at conveying overall trends?\n  \n\n\n\n\nColor\nColor is a powerful encoding that presents several challenges. Have you chosen a color scale that is optimal for that data type?\n   \n\n\nSalience and Relevance\nPop-out effects enable quick recognition. Are the most noticeable elements of your visualizations also the most relevant?"
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/project_b_birthcontrol_example_notebook.html",
    "href": "materials/1-workshop1/8-group-projects/project_b_birthcontrol_example_notebook.html",
    "title": "Birth Control vs Inflammation over menses",
    "section": "",
    "text": "This is an observational study to evaluate the relationship between birth control and vaginal inflammation In response to menstruation. Absolute abundance of bacteria was measured by 3 qPCR assays (for total Bacteria, L. crispatus, L. iners). Cytokine levels (in copies/ml of vaginal fluid) were also measured by Luminex. We also looked at the number and type of immune cells in the vagina using Flow Cytometry. Data was collected at four timepoints; before, at the start, the end, and after menstruation."
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/project_b_birthcontrol_example_notebook.html#merging-the-dataset",
    "href": "materials/1-workshop1/8-group-projects/project_b_birthcontrol_example_notebook.html#merging-the-dataset",
    "title": "Birth Control vs Inflammation over menses",
    "section": "Merging the dataset",
    "text": "Merging the dataset\n\n\nCode\nmerged_dataset &lt;- \n  sample_map %&gt;%\n  left_join(metadata) %&gt;%\n  left_join(luminex) %&gt;%\n  left_join(flow_cytometry) %&gt;%\n  mutate(cd4_ratio = cd4_t_cells / cd3_positive )  %&gt;%\n  mutate(time_point = fct_relevel(time_point,\"week_prior\",\"onset\",\"end_bleeding\", \"week_post\"))\n\n\nLet’s create table one\n\n\nCode\nprocessed_for_tableone &lt;- merged_dataset %&gt;%\n  select(-limits, -sample_id) %&gt;%\n  pivot_wider(names_from = cytokine, values_from = conc) %&gt;% # move cytokine to wide\n  filter(time_point == \"week_prior\") # we just want baseline characteristics for table 1\n\nCreateTableOne(\n  data = processed_for_tableone,\n  vars = c(\"age\", \"pcos_status\", \"period_product\", \"cd4_ratio\", \"IL-1a\", \"IL-1b\", \"IP-10\", \"MIG\", \"TNF-a\" ),\n  factorVars = c(\"pcos_status\",\"period_product\" ),\n  strata = \"arm\",\n  test = FALSE\n) %&gt;%\n  print(\n    nonnormal = c(\"cd4_ratio\", \"IL-1a\", \"IL-1b\", \"IP-10\", \"MIG\", \"TNF-a\"),\n    \n  )\n\n\n                          Stratified by arm\n                           birth_control          no_birth_control      \n  n                            15                     12                \n  age (mean (SD))           30.80 (2.54)           32.08 (3.20)         \n  pcos_status = pcos (%)        6 (40.0)               8 (66.7)         \n  period_product (%)                                                    \n     menstrual_cup              2 (13.3)               2 (16.7)         \n     pad                        3 (20.0)               1 ( 8.3)         \n     tampon                    10 (66.7)               9 (75.0)         \n  cd4_ratio (median [IQR])   0.51 [0.47, 0.52]      0.45 [0.38, 0.52]   \n  IL-1a (median [IQR])      25.65 [11.97, 44.53]   40.94 [11.66, 87.95] \n  IL-1b (median [IQR])       0.24 [0.24, 2.04]      0.24 [0.24, 4.91]   \n  IP-10 (median [IQR])      11.80 [6.71, 45.20]    16.95 [6.38, 84.45]  \n  MIG (median [IQR])       151.24 [47.17, 301.19] 129.69 [18.63, 365.59]"
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/project_b_birthcontrol_example_notebook.html#arm-cytokines",
    "href": "materials/1-workshop1/8-group-projects/project_b_birthcontrol_example_notebook.html#arm-cytokines",
    "title": "Birth Control vs Inflammation over menses",
    "section": "Arm cytokines",
    "text": "Arm cytokines\n\n\nCode\nmerged_dataset %&gt;%\n  filter(cytokine == \"TNFa\") %&gt;%\n  ggplot(aes(x=conc, y=arm)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.5) +\n  scale_x_log10(labels=scales::label_log()) +\n  facet_grid(rows=vars(time_point), scales=\"free\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nmerged_dataset %&gt;%\n  ggplot(aes(x=conc, y=arm)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.5) +\n  scale_x_log10(labels=scales::label_log()) +\n  facet_grid(rows=vars(time_point), cols=vars(cytokine), scales=\"free\")"
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/project_b_birthcontrol_example_notebook.html#time-series-visualization",
    "href": "materials/1-workshop1/8-group-projects/project_b_birthcontrol_example_notebook.html#time-series-visualization",
    "title": "Birth Control vs Inflammation over menses",
    "section": "Time series visualization",
    "text": "Time series visualization\n\n\nCode\nmerged_dataset %&gt;%\n  ggplot(aes(y=conc, x=time_point)) +\n  geom_boxplot() +\n  geom_point(alpha = 0.5) +\n  geom_line(alpha=0.5, aes(color=arm, group=pid)) +\n  scale_y_log10(labels=scales::label_log()) +\n  facet_grid(rows=vars(cytokine), scales=\"free\")"
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/project_b_birthcontrol_example_notebook.html#cytokines-vs-pcos",
    "href": "materials/1-workshop1/8-group-projects/project_b_birthcontrol_example_notebook.html#cytokines-vs-pcos",
    "title": "Birth Control vs Inflammation over menses",
    "section": "Cytokines vs pcos",
    "text": "Cytokines vs pcos\nHow does PCOS associate with inflammation over menstruation? First, let’s look at one of the cytokines at one of the timepoints.\n\n\nCode\nmerged_dataset %&gt;%\n  filter(time_point == \"week_prior\", cytokine == \"TNFa\") %&gt;%\n  ggplot(aes(x = pcos_status, y=conc, color = arm)) +\n  geom_boxplot() +\n  geom_point(position = position_jitterdodge()) +\n  scale_y_log10() +\n  facet_grid(cols=vars(time_point), rows=vars(cytokine), scales=\"free_y\")\n\n\n\n\n\n\n\n\n\nNow, let’s look at all the cytokines together with all time points.\n\n\nCode\nmerged_dataset %&gt;%\n  ggplot(aes(x = pcos_status, y=conc, color = arm)) +\n  geom_boxplot() +\n  geom_point(position = position_jitterdodge()) +\n  scale_y_log10() +\n  facet_grid(cols=vars(time_point), rows=vars(cytokine), scales=\"free_y\")"
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/project_b_birthcontrol_example_notebook.html#cytokines-vs-flow",
    "href": "materials/1-workshop1/8-group-projects/project_b_birthcontrol_example_notebook.html#cytokines-vs-flow",
    "title": "Birth Control vs Inflammation over menses",
    "section": "Cytokines vs flow",
    "text": "Cytokines vs flow\nAre there any relationships between cytokines and cd4 ratio?\n\n\nCode\nmerged_dataset %&gt;%\n  ggplot(aes(x=conc, y=cd4_ratio, color=arm, )) +\n  geom_point() +\n  scale_x_log10(labels=scales::label_log()) +\n  facet_wrap(~cytokine)"
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/index.html",
    "href": "materials/1-workshop1/8-group-projects/index.html",
    "title": "Group Projects",
    "section": "",
    "text": "For this group activity, the odd-numbered groups will be analyzing the data from Research Project A, and the even numbered groups from Research Project B.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Group Project</b> -Applying what you've learned to new data"
    ]
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/index.html#example-notebooks",
    "href": "materials/1-workshop1/8-group-projects/index.html#example-notebooks",
    "title": "Group Projects",
    "section": "Example notebooks",
    "text": "Example notebooks\nJoseph and Laura tried their hands at the group projects as well. Take a look at the notebooks below and see what you think of their attempts.\n\nLink to yogurt dataset example notebook\nlink to rendered html\n\n\nLink to birth control dataset example notebook\nlink to rendered html",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Group Project</b> -Applying what you've learned to new data"
    ]
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/index.html#research-project-a---does-eating-yogurt-change-the-vaginal-microbiome",
    "href": "materials/1-workshop1/8-group-projects/index.html#research-project-a---does-eating-yogurt-change-the-vaginal-microbiome",
    "title": "Group Projects",
    "section": "Research Project A - Does eating yogurt change the vaginal microbiome?",
    "text": "Research Project A - Does eating yogurt change the vaginal microbiome?\n\nStudy Description\nThis is a randomized controlled trial to study whether yogurt consumption has an effect on the microbiome post antibiotic treatment. Absolute abundance of bacteria (in gene copies / mL) was measured by 3 qPCR assays (for total, L. crispatus, L. iners). Cytokine levels (in ug/mL of vaginal fluid) were also measured by Luminex. We also looked at the number and type of immune cells in the vagina using Flow Cytometry. Data was collected at two timepoints, pre and post antibiotic treatment.\n\n\nPrimary Aim\nTo investigate whether the consumption of yogurt influences microbiome composition after antibiotics treatment.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Group Project</b> -Applying what you've learned to new data"
    ]
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/index.html#research-project-b---does-birth-control-change-inflammation-during-menses",
    "href": "materials/1-workshop1/8-group-projects/index.html#research-project-b---does-birth-control-change-inflammation-during-menses",
    "title": "Group Projects",
    "section": "Research Project B - Does birth control change inflammation during menses?",
    "text": "Research Project B - Does birth control change inflammation during menses?\n\nStudy Description\nThis is an observational study to evaluate the relationship between birth control and vaginal inflammation In response to menstruation. Absolute abundance of bacteria was measured by 3 qPCR assays (for total Bacteria, L. crispatus, L. iners). Cytokine levels (in ug/mL of vaginal fluid) were also measured by Luminex. We also looked at the number and type of immune cells in the vagina using Flow Cytometry. Data was collected at four timepoints; before, at the start, the end, and after menstruation.\n\n\nPrimary Aim\nTo investigate whether taking birth control is associated with vaginal inflammation throughout the menstrual cycle.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Group Project</b> -Applying what you've learned to new data"
    ]
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/index.html#wednesday-pm-session",
    "href": "materials/1-workshop1/8-group-projects/index.html#wednesday-pm-session",
    "title": "Group Projects",
    "section": "Wednesday PM Session",
    "text": "Wednesday PM Session\nSession Structure\n- 1:30-2:00:\n- Read Research Project description below for your project\n- Download project data files and put them in your folder\n- Make a new .qmd file for this project.\n- 2:00-3:00\n- Load the demographic table into an R tibble.\n- Make notes about what each column name is and what it means.\n- Examine and note basic parameters (using R commands) - number of participants, visit structure, number of features measured for each, type of data measured (binary, categorical)\n- Plot a histogram of all numeric features (eg, distribution of age) and bar plots for counts of categorical variables (eg. how many female).\n- 3:00 - 3:30 - Break\n- 3:30 - 5:00\n- Preproccessing the data to prepare for tableone\n- Use tableone for the dataset",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Group Project</b> -Applying what you've learned to new data"
    ]
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/index.html#thursday-pm-session",
    "href": "materials/1-workshop1/8-group-projects/index.html#thursday-pm-session",
    "title": "Group Projects",
    "section": "Thursday PM Session",
    "text": "Thursday PM Session\n\nYogurt Groups\nTomorrow after lunch, you will present some results to a neighboring group. The presentation will be ~30 minutes long, and at least 3 people from your group should speak on a section.\nYou want to present to them table 1, explain the study, explain your research question, and show visualizations and/or stats to explain your findings. The format will be a rendered qmd document (which will be an html file). You can use the module 6 corrected qmd as a template (now available from module 6).\nIn this session:\n\nLoad all 4 tables and familiarize yourself with the dataset, especially in the context of the research aim\n\nTranslate the research aim into a statistical hypothesis. Justify the choice.\n\nJoin qPCR with demographic data (wide/long)\n\nvisualization (boxplot) of qPCR variable levels by study arm\n\ndo you need to transform this data in any way? Justify the choice.\n\ndefine a metric to characterize (justify choice).\n\nSpecify lower and upper limits of detection by looking at the highest and lowest values of each qPCR value\n\nlongitudinal visualization of chosen cytokine for all participants.\n\nExtra Challenge - try incorporating the data from the luminex data table. Pick 1-3 cytokines that seem to be detected in many samples to analyze in combinatino with the qPCR and demographic variables.\n\nOverall, come up with and answer two main questions, one comparing one arm vs the other, and one comparing a demographic factor, looking at the qPCR variables. For extra challenge, also describe some relationships between the qPCR variables and cytokines.\n\n\nBirth Control Groups\nTomorrow after lunch, you will present some results to a neighboring group. The presentation will be ~30 minutes long, and at least 3 people from your group should speak on a section.\nYou want to present to them table 1, explain the study, explain your research question, and show visualizations and/or stats to explain your findings. The format will be a rendered qmd document (which will be an html file). You can use the module 6 corrected qmd as a template You can use the module 6 corrected qmd as a template (now available from module 6).\nIn this session:\n\nLoad all 4 tables and familiarize yourself with the dataset, especially in the context of the research aim\n\nTranslate the research aim into a statistical hypothesis. Justify the choice\n\nJoin cytokine with demographic data\nvisualization (boxplot) of cytokine levels by study arm\n\nPick one to three cytokines (justify choice). Specify lower and upper limits of detection\n\nLongitudinal visualization of chosen cytokines for all participants\n\nExtra Challenge - try incorporating the data from the flow_cytometry data table. Pick a population (or a ratio of two populations, such as CD4+/CD3+) of immune cells that seem to be associated with some cytokine and demographic variables.\n\nOverall, come up with and answer two main questions, one comparing one arm vs the other, and one comparing a demographic factor, looking at the cytokine variables. For extra challenge, also describe some relationships between the flow cytometry variables, and cytokines, and longitudinal patterns.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Group Project</b> -Applying what you've learned to new data"
    ]
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/index.html",
    "href": "materials/1-workshop1/7-custom-data-visualizations/index.html",
    "title": "Customizing data visualizations using colorspace, ggplot2, and patchwork",
    "section": "",
    "text": "Learn how to change ggplot’s default choices for color and style and take control of final figure presentation.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 7</b>: Customizing data visualizations using <code>colorspace</code>, <code>ggplot2</code>, and <code>patchwork</code>"
    ]
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/index.html#color-scales",
    "href": "materials/1-workshop1/7-custom-data-visualizations/index.html#color-scales",
    "title": "Customizing data visualizations using colorspace, ggplot2, and patchwork",
    "section": "Color Scales",
    "text": "Color Scales\nMake slides full screen\n\n\nCoding exercise 7.1\nIn this worksheet, we will discuss how to change and customize color scales.\nWe will be using the R package tidyverse, which includes ggplot() and related functions. We will also be using the R package colorspace for the scale functions it provides.\n\n# load required library\nlibrary(tidyverse)\nlibrary(colorspace)\n\ntemperatures &lt;- read_csv(\"https://wilkelab.org/SDS375/datasets/tempnormals.csv\") %&gt;%\n  mutate(\n    location = factor(\n      location, levels = c(\"Death Valley\", \"Houston\", \"San Diego\", \"Chicago\")\n    )\n  ) %&gt;%\n  select(location, day_of_year, month, temperature)\n\ntemps_months &lt;- read_csv(\"https://wilkelab.org/SDS375/datasets/tempnormals.csv\") %&gt;%\n  group_by(location, month_name) %&gt;%\n  summarize(mean = mean(temperature)) %&gt;%\n  mutate(\n    month = factor(\n      month_name,\n      levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n    ),\n    location = factor(\n      location, levels = c(\"Death Valley\", \"Houston\", \"San Diego\", \"Chicago\")\n    )\n  ) %&gt;%\n  select(-month_name)\n\nWe will be working with the dataset temperatures that we have used in previous worksheets. This dataset contains the average temperature for each day of the year for four different locations.\n\ntemperatures\n\n# A tibble: 1,464 × 4\n   location     day_of_year month temperature\n   &lt;fct&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 Death Valley           1 01           51  \n 2 Death Valley           2 01           51.2\n 3 Death Valley           3 01           51.3\n 4 Death Valley           4 01           51.4\n 5 Death Valley           5 01           51.6\n 6 Death Valley           6 01           51.7\n 7 Death Valley           7 01           51.9\n 8 Death Valley           8 01           52  \n 9 Death Valley           9 01           52.2\n10 Death Valley          10 01           52.3\n# ℹ 1,454 more rows\n\n\nWe will also be working with an aggregated version of this dataset called temps_months, which contains the mean temperature for each month for the same locations.\n\ntemps_months\n\n# A tibble: 48 × 3\n# Groups:   location [4]\n   location  mean month\n   &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;\n 1 Chicago   50.4 Apr  \n 2 Chicago   74.1 Aug  \n 3 Chicago   29   Dec  \n 4 Chicago   28.9 Feb  \n 5 Chicago   24.8 Jan  \n 6 Chicago   75.8 Jul  \n 7 Chicago   71.0 Jun  \n 8 Chicago   38.8 Mar  \n 9 Chicago   60.9 May  \n10 Chicago   41.6 Nov  \n# ℹ 38 more rows\n\n\nAs a challenge, try to create this above table yourself using group_by() and summarize() like we learned about Wednesday., and then make a month column which is a factor with levels froing from “Jan” to “Dec”, and make the location column a factor with levels “Death Valley”, “Houston”, “San Diego”, “Chicago”. If you are having trouble, the solution is at the end of this page, make sure you copy it into your code so the rest of the exercise works.\n\n# check solution at the end before moving on!\ntemps_months &lt;- read_csv(\"https://wilkelab.org/SDS375/datasets/tempnormals.csv\") %&gt;%\n  group_by(___) %&gt;%\n  summarize(___) %&gt;%\n  mutate(\n    month = factor(\n      month_name,\n      ___\n    ),\n    location = factor(\n      location, ___\n    )\n  ) %&gt;%\n  select(-month_name)\n\n\nBuilt in ggplot2 color scales\nWe will start with built-in ggplot2 color scales, which require no additional packages. The scale functions are always named scale_color_*() or scale_fill_*(), depending on whether they apply to the color or fill aesthetic. The * indicates some other words specifying the type of the scale, for example scale_color_brewer() or scale_color_distiller() for discrete or continuous scales from the ColorBrewer project, respectively. You can find all available built-in scales here: https://ggplot2.tidyverse.org/reference/index.html#section-scales\nNow consider the following plot.\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE)\n\n\n\n\n\n\n\n\nIf you wanted to change the color scale to one from the ColorBrewer project, which scale function would you have to add? scale_color_brewer(), scale_color_distiller(), scale_fill_brewer(), scale_fill_distiller()?\n\n # answer the question above to yourself\n\nNow try this out.\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE) +\n  ___\n\nMost color scale functions have additional customizations. How to use them depends on the specific scale function. For the ColorBrewer scales you can set direction = 1 or direction = -1 to set the direction of the scale (light to dark or dark to light). You can also set the palette via a numeric argument, e.g. palette = 1, palette = 2, palette = 3 etc.\nTry this out by setting the direction of the scale from light to dark and using palette #4.\n\n # build all the code for this exercise\n\nA popular set of scales are the viridis scales, which are provided by scale_*_viridis_c() for continuous data and scale_*_viridis_d() for discrete data. Change the above plot to use a viridis scale.\n\n # build all the code for this exercise\n\nThe viridis scales can be customized with direction (as before), option (which can be \"A\", \"B\", \"C\", \"D\", or \"E\"), and begin and end which are numerical values between 0 and 1 indicating where in the color scale the data should begin or end. For example, begin = 0.2 means that the lowest data value is mapped to the 20th percentile in the scale.\nTry different choices for option, begin, and end to see how they change the plot.\n\n # build all the code for this exercise\n\n\n\nCustomizing scale title and labels\nIn a previous worksheet, we used arguments such as name, breaks, labels, and limits to customize the axis. For color scales, instead of an axis we have a legend, and we can use the same arguments inside the scale function to customize how the legend looks.\nTry this out. Set the scale limits from 10 to 110 and set the name of the scale and the breaks as you wish.\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE) +\n  scale_fill_viridis_c(\n    name = ___,\n    breaks = ___,\n    limits = ___\n  )\n\nNote: Color scales ignore the expand argument, so you cannot use it to expand the scale beyond the data values as you can for position scales.\n\n\nBinned scales\nResearch into human perception has shown that continuous coloring can be difficult to interpret. Therefore, it is often preferable to use a small number of discrete colors to indicate ranges of data values. You can do this in ggplot with binned scales. For example, scale_fill_viridis_b() provides a binned version of the viridis scale. Try this out.\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE) +\n  ___\n\nYou can change the number of bins by providing the n.breaks argument or alternatively by setting breaks explicitly. Try this out.\n\n # build all the code for this exercise\n\n\n\nScales from the colorspace package\nThe color scales provided by the colorspace package follow a simple naming scheme of the form scale_&lt;aesthetic&gt;_&lt;datatype&gt;_&lt;colorscale&gt;(), where &lt;aesthetic&gt; is the name of the aesthetic (fill, color, colour), &lt;datatype&gt; indicates the type of variable plotted (discrete, continuous, binned), and colorscale stands for the type of the color scale (qualitative, sequential, diverging, divergingx).\nFor the mean temperature plot we have been using throughout this worksheet, which 2 color scales from the colorspace package is/are appropriate?\nscale_fill_binned_sequential(), scale_fill_discrete_qualitative(), scale_fill_continuous_sequential(), scale_color_continuous_sequential(), scale_color_continuous_diverging()&gt;\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE) +\n  ___\n\nYou can customize the colorspace scales with the palette argument, which takes the name of a palette (e.g., \"Inferno\", \"BluYl\", \"Lajolla\"). Try this out. Also try reversing the scale direction with rev = TRUE or rev = FALSE. (The colorspace scales use rev instead of direction.) You can find the names of all supported scales here (consider specifically single-hue and multi-hue sequential palettes).\n\n # build all the code for this exercise\n\nYou can also use begin and end just like in the viridis scales.\n\n\nManual scales\nFor discrete data with a small number of categories, it’s usually best to set colors manually. This can be done with the scale functions scale_*_manual(). These functions take an argument values that specifies the color values to use.\nTo see how this works, let’s go back to this plot of temperatures over time for four locations.\n\nggplot(temperatures, aes(day_of_year, temperature, color = location)) +\n  geom_line(size = 1.5)\n\n\n\n\n\n\n\n\nLet’s use the following four colors: \"gold2\", \"firebrick\", \"blue3\", \"springgreen4\". We can visualize this palette using the function swatchplot() from the colorspace package.\n\ncolorspace::swatchplot(c(\"gold2\", \"firebrick\", \"blue3\", \"springgreen4\"))\n\n\n\n\n\n\n\n\nNow apply this color palette to the temperatures plot, by using the manual color scale. Hint: use the values argument to provide the colors to the manual scale.\n\nggplot(temperatures, aes(day_of_year, temperature, color = location)) +\n  geom_line(size = 1.5) +\n  ___\n\nOne problem with this approach is that we can’t easily control which data value gets assigned to which color. What if we wanted San Diego to be shown in green and Chicago in blue? The simplest way to resolve this issue is to use a named vector. A named vector in R is a vector where each value has a name. Named vectors are created by writing c(name1 = value1, name2 = value2, ...). See the following example.\n\n# regular vector\nc(\"cat\", \"mouse\", \"house\")\n\n[1] \"cat\"   \"mouse\" \"house\"\n\n# named vector\nc(A = \"cat\", B = \"mouse\", C = \"house\")\n\n      A       B       C \n  \"cat\" \"mouse\" \"house\" \n\n\nThe names in the second example are A, B, and C. Notice that the names are not in quotes. However, if you need a name containing a space (such as Death Valley), you need to enclose the name in backticks. Thus, our named vector of colors could be written like so:\n\nc(`Death Valley` = \"gold2\", Houston = \"firebrick\", Chicago = \"blue3\", `San Diego` = \"springgreen4\")\n\n  Death Valley        Houston        Chicago      San Diego \n       \"gold2\"    \"firebrick\"        \"blue3\" \"springgreen4\" \n\n\nNow try to use this color vector in the figure showing temperatures over time.\n\n # build all the code for this exercise\n\nTry some other colors also. For example, you could use the Okabe-Ito colors:\n\n# Okabe-Ito colors\ncolorspace::swatchplot(c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#000000\"))\n\n\n\n\n\n\n\n\nAlternatively, you can find a list of all named colors here. You can also run the command colors() in your R console to get a list of all available color names.\nHint: It’s a good idea to never use the colors \"red\", \"green\", \"blue\", \"cyan\", \"magenta\", \"yellow\". They are extreme points in the RGB color space and tend to look unnatural and too crazy. Try this by making a swatch plot of these colors, and compare for example to the color scale containing the colors \"firebrick\", \"springgreen4\", \"blue3\", \"turquoise3\", \"darkorchid2\", \"gold2\". Do you see the difference?\n\n # build all the code for this exercise\n\nSolution to the challenge to make the summary table of mean temperature by month:\n\n# paste this below the \"temperatures\" code-chunk\ntemps_months &lt;- read_csv(\"https://wilkelab.org/SDS375/datasets/tempnormals.csv\") %&gt;%\n  group_by(location, month_name) %&gt;%\n  summarize(mean = mean(temperature)) %&gt;%\n  mutate(\n    month = factor(\n      month_name,\n      levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n    ),\n    location = factor(\n      location, levels = c(\"Death Valley\", \"Houston\", \"San Diego\", \"Chicago\")\n    )\n  ) %&gt;%\n  select(-month_name)\n\nColor Scales Exercise Solutions",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 7</b>: Customizing data visualizations using <code>colorspace</code>, <code>ggplot2</code>, and <code>patchwork</code>"
    ]
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/index.html#figure-design",
    "href": "materials/1-workshop1/7-custom-data-visualizations/index.html#figure-design",
    "title": "Customizing data visualizations using colorspace, ggplot2, and patchwork",
    "section": "Figure Design",
    "text": "Figure Design\nMake slides full screen\n\n\nCoding exercise 7.2\nIn this worksheet, we will discuss how to change and customize themes.\nWe will be using the R package tidyverse, which includes ggplot() and related functions. We will also be using the packages cowplot for themes and the package palmerpenguins for the penguins dataset.\n\n# load required library\nlibrary(tidyverse)\nlibrary(cowplot)\nlibrary(palmerpenguins)\n\nWe will be working with the dataset penguins containing data on individual penguins on Antarctica.\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nReady-made themes\nLet’s start with this simple plot with no specific styling.\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE)  # na.rm = TRUE prevents warning about missing values\n\n\n\n\n\n\n\n\nThe default ggplot theme is theme_gray(). Verify that adding this theme to the plot makes no difference in the output. Then change the overall font size by providing the theme function with a numeric font size argument, e.g. theme_gray(16).\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE) +\n  ___\n\nThe ggplot2 package has many built-in themes, including theme_minimal(), theme_bw(), theme_void(), theme_dark(). Try these different themes on the above plot. Also try again changing the font size. You can see all themes provided by ggplot2 here: https://ggplot2.tidyverse.org/reference/ggtheme.html\n\n # build all the code for this exercise\n\nMany other packages also provide themes. For example, the cowplot package provides themes theme_half_open(), theme_minimal_grid(), theme_minimal_hgrid(), and theme_minimal_vgrid(). You can see all cowplot themes here: https://wilkelab.org/cowplot/articles/themes.html\n\n # build all the code for this exercise\n\nCompare the visual appearance of theme_minimal() from ggplot2 to theme_minimal_grid() from cowplot. What similarities and differences to you notice? Which do you prefer? (There is no correct answer here, just be aware of the differences and of your preferences.)\n\n # build all the code for this exercise\n\n\n\nModifying theme elements\nYou can modify theme elements by adding a theme() call to the plot. Inside the theme() call you specify which theme element you want to modify (e.g., axis.title, axis.text.x, panel.background, etc) and what changes you want to make. For example, to make axis titles blue, you would write:\n\ntheme(\n  axis.title = element_text(color = \"blue\")\n)\n\nThere are many theme settings, and for each one you need to know what type of an element it is (element_text(), element_line(), element_rect() for text, lines, or rectangles, respectively). A complete description of the available options is available at the ggplot2 website: https://ggplot2.tidyverse.org/reference/theme.html\nHere, we will only try a few simple things. For example, see if you can make the legend title blue and the legend text red.\n\n# make the legend title blue and the legend text red\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE)\n\n\n\n\n\n\n\n\nNow color the area behind the legend in \"aliceblue\". Hint: The theme element you need to change is called legend.background. There is also an element legend.box.background but it is only visible if legend.background is not shown, and in the default ggplot2 themes that is not the case.\n\n # build all the code for this exercise\n\nAnother commonly used feature in themes are margins. Many parts of the plot theme can understand customized margins, which control how much spacing there is between different parts of a plot. Margins are typically specified with the function margin(), which takes four numbers specifying the margins in points, in the order top, right, bottom, left. So, margin(10, 5, 5, 10) would specify a top margin of 10pt, a right margin of 5pt, a bottom margin of 5pt, and a left margin of 10pt.\nTry this out by setting the legend margin (element legend.margin) such that there is no top and no bottom margin but 10pt left and right margin.\n\n # build all the code for this exercise\n\nThere are many other things you can do. Try at least some of the following:\n\nChange the horizontal or vertical justification of text with hjust and vjust.\nChange the font family with family.1\nChange the panel grid. For example, create only horizontal lines, or only vertical lines.\nChange the overall margin of the plot with plot.margin.\nMove the position of the legend with legend.position and legend.justification.\nTurn off some elements by setting them to element_blank().\n\n1 Getting fonts to work well can be tricky in R. Which specific fonts work depends on the graphics device and the operating system. You can try some of the following fonts and see if they work on app.terra.bio: \"Palatino\", \"Times\", \"Helvetica\", \"Courier\", \"ITC Bookman\", \"ITC Avant Garde Gothic\", \"ITC Zapf Chancery\".\n\n\nWriting your own theme\nYou can write a theme by\n\ntheme_colorful &lt;-\n  theme_bw() +\n  theme(\n    text = element_text(color = \"mediumblue\"),\n    axis.text = element_text(color = \"springgreen4\"),\n    legend.text = element_text(color = \"firebrick4\")\n  )\n\nHint: Do you have to add theme_colorful or theme_colorful()? Do you understand which option is correct and why? If you are unsure, try both and see what happens.\n\n # build all the code for this exercise\n\nNow write your own theme and then add it to the penguing plot.\n\n # build all the code for this exercise\n\nFigure Design Exercise Solutions",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 7</b>: Customizing data visualizations using <code>colorspace</code>, <code>ggplot2</code>, and <code>patchwork</code>"
    ]
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/index.html#compound-figures",
    "href": "materials/1-workshop1/7-custom-data-visualizations/index.html#compound-figures",
    "title": "Customizing data visualizations using colorspace, ggplot2, and patchwork",
    "section": "Compound Figures",
    "text": "Compound Figures\nMake slides full screen\n\n\nCoding exercise 7.3\nIn this worksheet, we will discuss how to combine several ggplot2 plots into one compound figure.\nWe will be using the R package tidyverse, which includes ggplot() and related functions. We will also be using the package patchwork for plot composition.\n\n# load required library\nlibrary(tidyverse)\nlibrary(patchwork)\n\nWe will be working with the dataset mtcars, which contains fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).\n\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\n\nCombining plots\nFirst we set up four different plots that we will subsequently combine. The plots are stored in variables p1, p2, p3, p4.\n\np1 &lt;- ggplot(mtcars) + \n  geom_point(aes(mpg, disp))\np1  \n\n\n\n\n\n\n\np2 &lt;- ggplot(mtcars) + \n  geom_boxplot(aes(gear, disp, group = gear))\np2\n\n\n\n\n\n\n\np3 &lt;- ggplot(mtcars) + \n  geom_smooth(aes(disp, qsec))\np3\n\n\n\n\n\n\n\np4 &lt;- ggplot(mtcars) + \n  geom_bar(aes(carb))\np4\n\n\n\n\n\n\n\n\nTo show plots side-by-side, we use the operator |, as in p1 | p2. Try this by making a compound plot of plots p1, p2, p3 side-by-side.\n\n # build all the code for this exercise\n\nTo show plots on top of one-another, we use the operator /, as in p1 / p2. Try this by making a compound plot of plots p1, p2, p3 on top of each other.\n\n # build all the code for this exercise\n\nWe can also use parentheses to group plots with respect to the operators | and /. For example, we can place several plots side-by-side and then place this entire row of plots on top of another plot. Try putting p1, p2, p3, on the top row, and p4 on the bottom row.\n\n(___) / ___\n\n\n\nPlot annotations\nThe patchwork package provides a powerful annotation system via the plot_annotation() function that can be added to a plot assembly. For example, we can add plot tags (the labels in the upper left corner identifying the plots) via the plot annotation tag_levels. You can set tag_levels = \"A\" to generate tags A, B, C, etc. Try this out.\n\n(p1 | p2 | p3 ) / p4\n\n\n\n\n\n\n\n\nTry also tag levels such as \"a\", \"i\", or \"1\".\nYou can also add elements such as titles, subtitles, and captions, by setting the title, subtitle, or caption argument in plot_annotation(). Try this out by adding an overall title to the figure from the previous exercise.\n\n # build all the code for this exercise\n\nAlso set a subtitle and a caption.\nFinally, you can change the theme of all plots in the plot assembly via the & operator, as in (p1 | p2) & theme_bw(). Try this out.\n\n # build all the code for this exercise\n\nWhat happens if you write this expression without parentheses? Do you understand why?\n(Big) Challenge Problem:\nIf you have time this morning, or if you want to work on it this afternoon, try analyzing a new dataset to test your R skills. First, learn what the columns mean, what missing values you see, and then start to ask questions about patterns in the data by making plots.\nYou can browse the datasets at https://github.com/rfordatascience/tidytuesday/tree/master/data (click on a year folder and go to the README to read about the datasets). Or pick from one of the ones below:\n\n# fish consumption in different countries \nconsumption &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-12/fish-and-seafood-consumption-per-capita.csv')\n\n# world cup Cricket matches from 1996 to 2005\nmatches &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-11-30/matches.csv')\n\n# malaria deaths by age across the world and time. \n malaria_deaths&lt;- readr::read_csv('https://github.com/rfordatascience/tidytuesday/blob/master/data/2018/2018-11-13/malaria_deaths_age.csv')\n\n#meteorites and/or volcanos:\n# note to plot a map, try the following:\ncountries_map &lt;- ggplot2::map_data(\"world\")\nworld_map&lt;-ggplot() + \n  geom_map(data = countries_map, \n           map = countries_map,aes(x = long, y = lat, map_id = region, group = group),\n           fill = \"white\", color = \"black\", size = 0.1) # then add geom_point() to this map to add lat/long points\n\nmeteorites &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-11/meteorites.csv\")\nvolcano &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-12/volcano.csv\")\n\n# or pick any dataset from https://github.com/rfordatascience/tidytuesday/tree/master/data , \n# just click on a year folder and go to the README to read about the datasets\n# if you have trouble loading a dataset there, ask for help!\n\nCompound Figures Exercise Solutions",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 7</b>: Customizing data visualizations using <code>colorspace</code>, <code>ggplot2</code>, and <code>patchwork</code>"
    ]
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_exercise_solutions.html",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_exercise_solutions.html",
    "title": "Figure Design Exercise Solutions",
    "section": "",
    "text": "Coding exercise 7.2\nIn this worksheet, we will discuss how to change and customize themes.\nWe will be using the R package tidyverse, which includes ggplot() and related functions. We will also be using the packages cowplot for themes and the package palmerpenguins for the penguins dataset.\n\n# load required library\nlibrary(tidyverse)\nlibrary(cowplot)\nlibrary(palmerpenguins)\n\nWe will be working with the dataset penguins containing data on individual penguins on Antarctica.\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nReady-made themes\nLet’s start with this simple plot with no specific styling.\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE)  # na.rm = TRUE prevents warning about missing values\n\n\n\n\n\n\n\n\nThe default ggplot theme is theme_gray(). Verify that adding this theme to the plot makes no difference in the output. Then change the overall font size by providing the theme function with a numeric font size argument, e.g. theme_gray(16).\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE) +\n  ___\n\n\n# solution\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE) +\n  theme_gray()\n\n\n\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE) +\n  theme_gray(16)\n\n\n\n\n\n\n\n\nThe ggplot2 package has many built-in themes, including theme_minimal(), theme_bw(), theme_void(), theme_dark(). Try these different themes on the above plot. Also try again changing the font size. You can see all themes provided by ggplot2 here: https://ggplot2.tidyverse.org/reference/ggtheme.html\n\n # build all the code for this exercise\n #| eval: TRUE\n#| echo: TRUE\n\n # solution\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE) +\n  theme_bw(12)\n\n\n\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE) +\n  theme_minimal(14)\n\n\n\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE) +\n  theme_void()\n\n\n\n\n\n\n\n\nMany other packages also provide themes. For example, the cowplot package provides themes theme_half_open(), theme_minimal_grid(), theme_minimal_hgrid(), and theme_minimal_vgrid(). You can see all cowplot themes here: https://wilkelab.org/cowplot/articles/themes.html\n\n #| eval: TRUE\n#| echo: TRUE\n # build all the code for this exercise\n\n  # solution\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE) +\n  theme_half_open()\n\n\n\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE) +\n  theme_minimal_grid()\n\n\n\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE) +\n  theme_minimal_hgrid()\n\n\n\n\n\n\n\n\nCompare the visual appearance of theme_minimal() from ggplot2 to theme_minimal_grid() from cowplot. What similarities and differences to you notice? Which do you prefer? (There is no correct answer here, just be aware of the differences and of your preferences.)\n\n #| eval: TRUE\n#| echo: TRUE\n # build all the code for this exercise\n\n  # solution\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE) +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE) +\n  theme_minimal_grid()\n\n\n\n\n\n\n\n\n\n\nModifying theme elements\nYou can modify theme elements by adding a theme() call to the plot. Inside the theme() call you specify which theme element you want to modify (e.g., axis.title, axis.text.x, panel.background, etc) and what changes you want to make. For example, to make axis titles blue, you would write:\n\ntheme(\n  axis.title = element_text(color = \"blue\")\n)\n\nThere are many theme settings, and for each one you need to know what type of an element it is (element_text(), element_line(), element_rect() for text, lines, or rectangles, respectively). A complete description of the available options is available at the ggplot2 website: https://ggplot2.tidyverse.org/reference/theme.html\nHere, we will only try a few simple things. For example, see if you can make the legend title blue and the legend text red.\n\n# make the legend title blue and the legend text red\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE)\n\n\n\n\n\n\n\n\nNow color the area behind the legend in \"aliceblue\". Hint: The theme element you need to change is called legend.background. There is also an element legend.box.background but it is only visible if legend.background is not shown, and in the default ggplot2 themes that is not the case.\n\n # build all the code for this exercise\n\n # solution\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE) +\n  theme(\n    legend.background = element_rect(fill = \"aliceblue\")\n  )\n\n\n\n\n\n\n\n\nAnother commonly used feature in themes are margins. Many parts of the plot theme can understand customized margins, which control how much spacing there is between different parts of a plot. Margins are typically specified with the function margin(), which takes four numbers specifying the margins in points, in the order top, right, bottom, left. So, margin(10, 5, 5, 10) would specify a top margin of 10pt, a right margin of 5pt, a bottom margin of 5pt, and a left margin of 10pt.\nTry this out by setting the legend margin (element legend.margin) such that there is no top and no bottom margin but 10pt left and right margin.\n\n # build all the code for this exercise\n\n # solution \nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE) +\n  theme(\n    legend.background = element_rect(fill = \"aliceblue\"),\n    legend.margin = margin(0, 10, 0, 10)\n  )\n\n\n\n\n\n\n\n\nThere are many other things you can do. Try at least some of the following:\n\nChange the horizontal or vertical justification of text with hjust and vjust.\nChange the font family with family.1\nChange the panel grid. For example, create only horizontal lines, or only vertical lines.\nChange the overall margin of the plot with plot.margin.\nMove the position of the legend with legend.position and legend.justification.\nTurn off some elements by setting them to element_blank().\n\n1 Getting fonts to work well can be tricky in R. Which specific fonts work depends on the graphics device and the operating system. You can try some of the following fonts and see if they work on app.terra.bio: \"Palatino\", \"Times\", \"Helvetica\", \"Courier\", \"ITC Bookman\", \"ITC Avant Garde Gothic\", \"ITC Zapf Chancery\".\n\n\nWriting your own theme\nYou can write a theme by\n\ntheme_colorful &lt;-\n  theme_bw() +\n  theme(\n    text = element_text(color = \"mediumblue\"),\n    axis.text = element_text(color = \"springgreen4\"),\n    legend.text = element_text(color = \"firebrick4\")\n  )\n\nHint: Do you have to add theme_colorful or theme_colorful()? Do you understand which option is correct and why? If you are unsure, try both and see what happens.\n\n # build all the code for this exercise\n\n # solution\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE) +\n  theme_colorful\n\n\n\n\n\n\n\n\nNow write your own theme and then add it to the penguing plot.\n\n # build all the code for this exercise\n\n # solution\n\nmytheme &lt;- theme_minimal_grid() +\n  theme(\n    panel.border = element_rect(size = 1, color = \"black\"),\n    legend.box.background = element_rect(size = 0.5, color = \"black\")\n  )\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point(na.rm = TRUE) +\n  mytheme"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_exercise_solutions.html",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_exercise_solutions.html",
    "title": "Compound Figures Exercise Solutions",
    "section": "",
    "text": "Coding exercise 7.3\nIn this worksheet, we will discuss how to combine several ggplot2 plots into one compound figure.\nWe will be using the R package tidyverse, which includes ggplot() and related functions. We will also be using the package patchwork for plot composition.\n\n# load required library\nlibrary(tidyverse)\nlibrary(patchwork)\n\nWe will be working with the dataset mtcars, which contains fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).\n\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\n\nCombining plots\nFirst we set up four different plots that we will subsequently combine. The plots are stored in variables p1, p2, p3, p4.\n\np1 &lt;- ggplot(mtcars) + \n  geom_point(aes(mpg, disp))\np1  \n\n\n\n\n\n\n\np2 &lt;- ggplot(mtcars) + \n  geom_boxplot(aes(gear, disp, group = gear))\np2\n\n\n\n\n\n\n\np3 &lt;- ggplot(mtcars) + \n  geom_smooth(aes(disp, qsec))\np3\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\np4 &lt;- ggplot(mtcars) + \n  geom_bar(aes(carb))\np4\n\n\n\n\n\n\n\n\nTo show plots side-by-side, we use the operator |, as in p1 | p2. Try this by making a compound plot of plots p1, p2, p3 side-by-side.\n\n # build all the code for this exercisew\n\n  # solution \n p1 | p2 | p3\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTo show plots on top of one-another, we use the operator /, as in p1 / p2. Try this by making a compound plot of plots p1, p2, p3 on top of each other.\n\n # build all the code for this exercise\n\n  p1 / p2 / p3\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can also use parentheses to group plots with respect to the operators | and /. For example, we can place several plots side-by-side and then place this entire row of plots on top of another plot. Try putting p1, p2, p3, on the top row, and p4 on the bottom row.\n\n(___) / ___\n\n\n# solution\n(p1 | p2 | p3 ) / p4\n\n\n\nPlot annotations\nThe patchwork package provides a powerful annotation system via the plot_annotation() function that can be added to a plot assembly. For example, we can add plot tags (the labels in the upper left corner identifying the plots) via the plot annotation tag_levels. You can set tag_levels = \"A\" to generate tags A, B, C, etc. Try this out.\n\n(p1 | p2 | p3 ) / p4\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# solution\n(p1 | p2 | p3 ) / p4 +\n  plot_annotation(\n    tag_levels = \"A\"\n  )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTry also tag levels such as \"a\", \"i\", or \"1\".\nYou can also add elements such as titles, subtitles, and captions, by setting the title, subtitle, or caption argument in plot_annotation(). Try this out by adding an overall title to the figure from the previous exercise.\n\n # build all the code for this exercise\n\n # solution\n(p1 | p2 | p3 ) / p4 +\n  plot_annotation(\n    tag_levels = \"A\",\n    title = \"Various observations about old cars\"\n  )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAlso set a subtitle and a caption.\nFinally, you can change the theme of all plots in the plot assembly via the & operator, as in (p1 | p2) & theme_bw(). Try this out.\n\n # build all the code for this exercise\n\n# solution\n(p1 | p2) & theme_bw()\n\n\n\n\n\n\n\n\nWhat happens if you write this expression without parentheses? Do you understand why?\n(Big) Challenge Problem:\nIf you have time this morning, or if you want to work on it this afternoon, try analyzing a new dataset to test your R skills. First, learn what the columns mean, what missing values you see, and then start to ask questions about patterns in the data by making plots.\nYou can browse the datasets at https://github.com/rfordatascience/tidytuesday/tree/master/data (click on a year folder and go to the README to read about the datasets). Or pick from one of the ones below:\n\n# fish consumption in different countries \nconsumption &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-12/fish-and-seafood-consumption-per-capita.csv')\n\nRows: 11028 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Entity, Code\ndbl (2): Year, Fish, Seafood- Food supply quantity (kg/capita/yr) (FAO, 2020)\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# world cup Cricket matches from 1996 to 2005\nmatches &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-11-30/matches.csv')\n\nRows: 1237 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (19): match_id, team1, team2, team1_away_or_home, team2_home_away, winne...\ndbl  (5): score_team1, score_team2, wickets_team1, wickets_team2, margin\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# malaria deaths by age across the world and time. \n malaria_deaths&lt;- readr::read_csv('https://github.com/rfordatascience/tidytuesday/blob/master/data/2018/2018-11-13/malaria_deaths_age.csv')\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 1359 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): &lt;!DOCTYPE html&gt;\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#meteorites and/or volcanos:\n# note to plot a map, try the following:\ncountries_map &lt;- ggplot2::map_data(\"world\")\nworld_map&lt;-ggplot() + \n  geom_map(data = countries_map, \n           map = countries_map,aes(x = long, y = lat, map_id = region, group = group),\n           fill = \"white\", color = \"black\", size = 0.1) # then add geom_point() to this map to add lat/long points\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning in geom_map(data = countries_map, map = countries_map, aes(x = long, :\nIgnoring unknown aesthetics: x and y\n\nmeteorites &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-11/meteorites.csv\")\n\nRows: 45716 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): name, name_type, class, fall, geolocation\ndbl (5): id, mass, year, lat, long\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nvolcano &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-12/volcano.csv\")\n\nRows: 958 Columns: 26\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (18): volcano_name, primary_volcano_type, last_eruption_year, country, r...\ndbl  (8): volcano_number, latitude, longitude, elevation, population_within_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# or pick any dataset from https://github.com/rfordatascience/tidytuesday/tree/master/data , \n# just click on a year folder and go to the README to read about the datasets\n# if you have trouble loading a dataset there, ask for help!"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_exercise_solutions.html",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_exercise_solutions.html",
    "title": "Color Scales Exercise Solutions",
    "section": "",
    "text": "Coding exercise 7.1\nIn this worksheet, we will discuss how to change and customize color scales.\nWe will be using the R package tidyverse, which includes ggplot() and related functions. We will also be using the R package colorspace for the scale functions it provides.\n\n# load required library\nlibrary(tidyverse)\nlibrary(colorspace)\n\ntemperatures &lt;- read_csv(\"https://wilkelab.org/SDS375/datasets/tempnormals.csv\") %&gt;%\n  mutate(\n    location = factor(\n      location, levels = c(\"Death Valley\", \"Houston\", \"San Diego\", \"Chicago\")\n    )\n  ) %&gt;%\n  select(location, day_of_year, month, temperature)\n\ntemps_months &lt;- read_csv(\"https://wilkelab.org/SDS375/datasets/tempnormals.csv\") %&gt;%\n  group_by(location, month_name) %&gt;%\n  summarize(mean = mean(temperature)) %&gt;%\n  mutate(\n    month = factor(\n      month_name,\n      levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n    ),\n    location = factor(\n      location, levels = c(\"Death Valley\", \"Houston\", \"San Diego\", \"Chicago\")\n    )\n  ) %&gt;%\n  select(-month_name)\n\nWe will be working with the dataset temperatures that we have used in previous worksheets. This dataset contains the average temperature for each day of the year for four different locations.\n\ntemperatures\n\n# A tibble: 1,464 × 4\n   location     day_of_year month temperature\n   &lt;fct&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 Death Valley           1 01           51  \n 2 Death Valley           2 01           51.2\n 3 Death Valley           3 01           51.3\n 4 Death Valley           4 01           51.4\n 5 Death Valley           5 01           51.6\n 6 Death Valley           6 01           51.7\n 7 Death Valley           7 01           51.9\n 8 Death Valley           8 01           52  \n 9 Death Valley           9 01           52.2\n10 Death Valley          10 01           52.3\n# ℹ 1,454 more rows\n\n\nWe will also be working with an aggregated version of this dataset called temps_months, which contains the mean temperature for each month for the same locations.\n\ntemps_months\n\n# A tibble: 48 × 3\n# Groups:   location [4]\n   location  mean month\n   &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;\n 1 Chicago   50.4 Apr  \n 2 Chicago   74.1 Aug  \n 3 Chicago   29   Dec  \n 4 Chicago   28.9 Feb  \n 5 Chicago   24.8 Jan  \n 6 Chicago   75.8 Jul  \n 7 Chicago   71.0 Jun  \n 8 Chicago   38.8 Mar  \n 9 Chicago   60.9 May  \n10 Chicago   41.6 Nov  \n# ℹ 38 more rows\n\n\nAs a challenge, try to create this above table yourself using group_by() and summarize() like we learned about Wednesday., and then make a month column which is a factor with levels froing from “Jan” to “Dec”, and make the location column a factor with levels “Death Valley”, “Houston”, “San Diego”, “Chicago”. If you are having trouble, the solution is at the end of this page, make sure you copy it into your code so the rest of the exercise works.\n\n# check solution at the end before moving on!\ntemps_months &lt;- read_csv(\"https://wilkelab.org/SDS375/datasets/tempnormals.csv\") %&gt;%\n  group_by(___) %&gt;%\n  summarize(___) %&gt;%\n  mutate(\n    month = factor(\n      month_name,\n      ___\n    ),\n    location = factor(\n      location, ___\n    )\n  ) %&gt;%\n  select(-month_name)\n\n\n# solution\ntemps_months &lt;- read_csv(\"https://wilkelab.org/SDS375/datasets/tempnormals.csv\") %&gt;%\n  group_by(location, month_name) %&gt;%\n  summarize(mean = mean(temperature)) %&gt;%\n  mutate(\n    month = factor(\n      month_name,\n      levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n    ),\n    location = factor(\n      location, levels = c(\"Death Valley\", \"Houston\", \"San Diego\", \"Chicago\")\n    )\n  ) %&gt;%\n  select(-month_name)\n\n\nBuilt in ggplot2 color scales\nWe will start with built-in ggplot2 color scales, which require no additional packages. The scale functions are always named scale_color_*() or scale_fill_*(), depending on whether they apply to the color or fill aesthetic. The * indicates some other words specifying the type of the scale, for example scale_color_brewer() or scale_color_distiller() for discrete or continuous scales from the ColorBrewer project, respectively. You can find all available built-in scales here: https://ggplot2.tidyverse.org/reference/index.html#section-scales\nNow consider the following plot.\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE)\n\n\n\n\n\n\n\n\nIf you wanted to change the color scale to one from the ColorBrewer project, which scale function would you have to add? scale_color_brewer(), scale_color_distiller(), scale_fill_brewer(), scale_fill_distiller()?\n\n # answer the question above to yourself\n\n #solution: \n scale_fill_distiller()\n\nNow try this out.\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE) +\n  ___\n\n\n # solution\n  ggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE) +\n  scale_fill_distiller()\n\n\n\n\n\n\n\n\nMost color scale functions have additional customizations. How to use them depends on the specific scale function. For the ColorBrewer scales you can set direction = 1 or direction = -1 to set the direction of the scale (light to dark or dark to light). You can also set the palette via a numeric argument, e.g. palette = 1, palette = 2, palette = 3 etc.\nTry this out by setting the direction of the scale from light to dark and using palette #4.\n\n # build all the code for this exercise\n\n# solution \nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE) +\n  scale_fill_distiller(direction = 1, palette = 4)\n\n\n\n\n\n\n\n\nA popular set of scales are the viridis scales, which are provided by scale_*_viridis_c() for continuous data and scale_*_viridis_d() for discrete data. Change the above plot to use a viridis scale.\n\n # build all the code for this exercise\n\n  # solution \n    ggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE) +\n  scale_fill_viridis_c()\n\n\n\n\n\n\n\n\nThe viridis scales can be customized with direction (as before), option (which can be \"A\", \"B\", \"C\", \"D\", or \"E\"), and begin and end which are numerical values between 0 and 1 indicating where in the color scale the data should begin or end. For example, begin = 0.2 means that the lowest data value is mapped to the 20th percentile in the scale.\nTry different choices for option, begin, and end to see how they change the plot.\n\n # build all the code for this exercise\n\n  # solution\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE) +\n  scale_fill_viridis_c(option = \"B\", begin = 0.15)\n\n\n\n\n\n\n\n\n\n\nCustomizing scale title and labels\nIn a previous worksheet, we used arguments such as name, breaks, labels, and limits to customize the axis. For color scales, instead of an axis we have a legend, and we can use the same arguments inside the scale function to customize how the legend looks.\nTry this out. Set the scale limits from 10 to 110 and set the name of the scale and the breaks as you wish.\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE) +\n  scale_fill_viridis_c(\n    name = ___,\n    breaks = ___,\n    limits = ___\n  )\n\n\n# solution\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE) +\n  scale_fill_viridis_c(\n    name = \"temperature (F)\",\n    breaks = c(25, 50, 75, 100),\n    limits = c(10, 110)\n  )\n\n\n\n\n\n\n\n\nNote: Color scales ignore the expand argument, so you cannot use it to expand the scale beyond the data values as you can for position scales.\n\n\nBinned scales\nResearch into human perception has shown that continuous coloring can be difficult to interpret. Therefore, it is often preferable to use a small number of discrete colors to indicate ranges of data values. You can do this in ggplot with binned scales. For example, scale_fill_viridis_b() provides a binned version of the viridis scale. Try this out.\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE) +\n  ___\n\n\n# solution\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE) +\n  scale_fill_viridis_b()\n\n\n\n\n\n\n\n\nYou can change the number of bins by providing the n.breaks argument or alternatively by setting breaks explicitly. Try this out.\n\n # build all the code for this exercise\n \n  # solution\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE) +\n  scale_fill_viridis_b(\n    n.breaks = 7\n  )\n\n\n\n\n\n\n\n\n\n\nScales from the colorspace package\nThe color scales provided by the colorspace package follow a simple naming scheme of the form scale_&lt;aesthetic&gt;_&lt;datatype&gt;_&lt;colorscale&gt;(), where &lt;aesthetic&gt; is the name of the aesthetic (fill, color, colour), &lt;datatype&gt; indicates the type of variable plotted (discrete, continuous, binned), and colorscale stands for the type of the color scale (qualitative, sequential, diverging, divergingx).\nFor the mean temperature plot we have been using throughout this worksheet, which 2 color scales from the colorspace package is/are appropriate?\nscale_fill_binned_sequential(), scale_fill_discrete_qualitative(), scale_fill_continuous_sequential(), scale_color_continuous_sequential(), scale_color_continuous_diverging()&gt;\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE) +\n  ___\n\n\n # solution\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE) +\n  scale_fill_continuous_sequential()\n\n\n\n\n\n\n\n\nYou can customize the colorspace scales with the palette argument, which takes the name of a palette (e.g., \"Inferno\", \"BluYl\", \"Lajolla\"). Try this out. Also try reversing the scale direction with rev = TRUE or rev = FALSE. (The colorspace scales use rev instead of direction.) You can find the names of all supported scales here (consider specifically single-hue and multi-hue sequential palettes).\n\n # build all the code for this exercise\n\n  # solution\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile() + \n  coord_fixed(expand = FALSE) +\n  scale_fill_continuous_sequential(\n    palette = \"Lajolla\"\n  )\n\n\n\n\n\n\n\n\nYou can also use begin and end just like in the viridis scales.\n\n\nManual scales\nFor discrete data with a small number of categories, it’s usually best to set colors manually. This can be done with the scale functions scale_*_manual(). These functions take an argument values that specifies the color values to use.\nTo see how this works, let’s go back to this plot of temperatures over time for four locations.\n\nggplot(temperatures, aes(day_of_year, temperature, color = location)) +\n  geom_line(size = 1.5)\n\n\n\n\n\n\n\n\nLet’s use the following four colors: \"gold2\", \"firebrick\", \"blue3\", \"springgreen4\". We can visualize this palette using the function swatchplot() from the colorspace package.\n\ncolorspace::swatchplot(c(\"gold2\", \"firebrick\", \"blue3\", \"springgreen4\"))\n\n\n\n\n\n\n\n\nNow apply this color palette to the temperatures plot, by using the manual color scale. Hint: use the values argument to provide the colors to the manual scale.\n\nggplot(temperatures, aes(day_of_year, temperature, color = location)) +\n  geom_line(size = 1.5) +\n  ___\n\n\n# solution\nggplot(temperatures, aes(day_of_year, temperature, color = location)) +\n  geom_line(size = 1.5) +\n  scale_color_manual(\n    values = c(\"gold2\", \"firebrick\", \"blue3\", \"springgreen4\")\n  )\n\n\n\n\n\n\n\n\nOne problem with this approach is that we can’t easily control which data value gets assigned to which color. What if we wanted San Diego to be shown in green and Chicago in blue? The simplest way to resolve this issue is to use a named vector. A named vector in R is a vector where each value has a name. Named vectors are created by writing c(name1 = value1, name2 = value2, ...). See the following example.\n\n# regular vector\nc(\"cat\", \"mouse\", \"house\")\n\n[1] \"cat\"   \"mouse\" \"house\"\n\n# named vector\nc(A = \"cat\", B = \"mouse\", C = \"house\")\n\n      A       B       C \n  \"cat\" \"mouse\" \"house\" \n\n\nThe names in the second example are A, B, and C. Notice that the names are not in quotes. However, if you need a name containing a space (such as Death Valley), you need to enclose the name in backticks. Thus, our named vector of colors could be written like so:\n\nc(`Death Valley` = \"gold2\", Houston = \"firebrick\", Chicago = \"blue3\", `San Diego` = \"springgreen4\")\n\n  Death Valley        Houston        Chicago      San Diego \n       \"gold2\"    \"firebrick\"        \"blue3\" \"springgreen4\" \n\n\nNow try to use this color vector in the figure showing temperatures over time.\n\n # build all the code for this exercise\n\n # solution\n\n ggplot(temperatures, aes(day_of_year, temperature, color = location)) +\n  geom_line(size = 1.5) +\n  scale_color_manual(\n    values = c(\n      `Death Valley` = \"gold2\",\n      Houston = \"firebrick\",\n      Chicago = \"blue3\",\n      `San Diego` = \"springgreen4\"\n    )\n  )\n\n\n\n\n\n\n\n\nTry some other colors also. For example, you could use the Okabe-Ito colors:\n\n# Okabe-Ito colors\ncolorspace::swatchplot(c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#000000\"))\n\n\n\n\n\n\n\n\nAlternatively, you can find a list of all named colors here. You can also run the command colors() in your R console to get a list of all available color names.\nHint: It’s a good idea to never use the colors \"red\", \"green\", \"blue\", \"cyan\", \"magenta\", \"yellow\". They are extreme points in the RGB color space and tend to look unnatural and too crazy. Try this by making a swatch plot of these colors, and compare for example to the color scale containing the colors \"firebrick\", \"springgreen4\", \"blue3\", \"turquoise3\", \"darkorchid2\", \"gold2\". Do you see the difference?\n\n # build all the code for this exercise\n\nSolution to the challenge to make the summary table of mean temperature by month:\n\n# paste this below the \"temperatures\" code-chunk\ntemps_months &lt;- read_csv(\"https://wilkelab.org/SDS375/datasets/tempnormals.csv\") %&gt;%\n  group_by(location, month_name) %&gt;%\n  summarize(mean = mean(temperature)) %&gt;%\n  mutate(\n    month = factor(\n      month_name,\n      levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n    ),\n    location = factor(\n      location, levels = c(\"Death Valley\", \"Houston\", \"San Diego\", \"Chicago\")\n    )\n  ) %&gt;%\n  select(-month_name)"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html",
    "href": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html",
    "title": "Hypothesis testing, data transformation, compositional data",
    "section": "",
    "text": "Code\nlibrary(tidyverse) # loads the packages from the tidyverse suite\nlibrary(pander) # allows to display pretty tables \nlibrary(patchwork) # allows to combine ggplot (see Module 7)\ntheme_set(theme_light())\nset.seed(123) # set the \"random seed\" for reproducibility"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#t-tests",
    "href": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#t-tests",
    "title": "Hypothesis testing, data transformation, compositional data",
    "section": "\\(t\\)-tests",
    "text": "\\(t\\)-tests\nBecause we have more than 40 samples in each dataset, we can use a \\(t\\)-test.\nIn R, \\(t\\)-tests can be done with the t.test function. Note that there are several ways to use the t.test function and several options we need to be careful about. To read more about these options, type ?t.test in your console.\nSpecifically, we need to be careful about specifying what our Null and alternative hypotheses are.\nThis is specified using the alternative option of the t.test function.\nThe option corresponding to\n\\[\\begin{align*}\nH_0:& \\ \\mu_1 = \\mu_2 \\\\\nH_a:& \\ \\mu_1 \\neq \\mu_2\n\\end{align*}\\]\nis alternative = \"two-sided\" which is the default option of t.test (that is, if we don’t specify the alternative, the function automatically assumes that we want to perform a “two-sided” test). It is \"two-sided\" because \\(\\mu_2\\) can be smaller OR larger in the alternative hypothesis.\nLet’s say that we do not have any a priori on the alternative and perform a two-sided test on the means of datasets 1 and 2.\n\n\n\nCode\nt.test(value ~ dataset, data = X |&gt; filter(dataset %in% 1:2))\n\n\n\n    Welch Two Sample t-test\n\ndata:  value by dataset\nt = -0.61157, df = 97.951, p-value = 0.5422\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.9508970  0.5028781\nsample estimates:\nmean in group 1 mean in group 2 \n      0.0688071       0.2928165 \n\n\nCode\n# this is the same as :\n# t.test(x = X$value[X$dataset == 1], y = X$value[X$dataset == 2])\n\n\nWe see that the probability to observe the \\(t\\) value is quite large. So, we do NOT reject the null hypothesis that the two means are the same.\nNow, if we do this test comparing the means of datasets 1 and 3, what do we get? What do you conclude?\n\n\nCode\nt.test(value ~ dataset, data = X |&gt; filter(dataset %in% c(1,3)))\n\n\n\n    Welch Two Sample t-test\n\ndata:  value by dataset\nt = -3.714, df = 97.572, p-value = 0.0003399\nalternative hypothesis: true difference in means between group 1 and group 3 is not equal to 0\n95 percent confidence interval:\n -2.1839829 -0.6628012\nsample estimates:\nmean in group 1 mean in group 3 \n      0.0688071       1.4921991 \n\n\nAs explained above, the tests we have done so far are “two-sided”. That means that the null hypothesis is that the two means are the same, and the alternative is that they are different.\n\\[\nH_0: \\mu_1 = \\mu_2\n\\]\n\\[\nH_a: \\mu_1 \\neq \\mu_2\n\\]\nBut sometimes, we have an a priori that the alternative is that one of the two means is larger. This is where we need a “one-sided” test.\nWe do this in R with the alternative option of the t.test function.\nRemember: to obtain the documentation about the t.test function, you can type ?t.test in the console.\nHow do we need to change the alternative option to perform the test corresponding to this pair of hypotheses ?\n\\[\\begin{align*}\nH_0:&\\  \\mu_1 \\geq \\mu_3 \\\\\nH_a:&\\  \\mu_1 &lt; \\mu_3 ( \\iff \\mu_1 - \\mu_3 &lt; 0 )\n\\end{align*}\\]\n\n\nCode\nt.test(value ~ dataset, data = X |&gt; filter(dataset %in% c(1,3)),\n       alternative = \"less\")\n\n\n\n    Welch Two Sample t-test\n\ndata:  value by dataset\nt = -3.714, df = 97.572, p-value = 0.0001699\nalternative hypothesis: true difference in means between group 1 and group 3 is less than 0\n95 percent confidence interval:\n       -Inf -0.7869575\nsample estimates:\nmean in group 1 mean in group 3 \n      0.0688071       1.4921991 \n\n\nHow do the \\(t\\) and \\(p\\) values compare in the two-sided vs one-sided test?\nWhy?\n\nQQ-plots: checking compatibility with a (normal) distribution\nRemember that the \\(t\\)-test requires, for small sample sizes, for the data to be drawn from a normal distribution. Usually, with real data, we don’t always know what distribution the data are samples from.\nWe can always check that our data is compatible with a normal distribution by performing a QQ-plot:\n\n\nCode\nexample_data &lt;- \n  tibble(norm = rnorm(20), lnorm = rlnorm(20))\n# just like `rnorm` samples from a normal distribution, \n# `rlnorm` samples from a log-normal distribution\n\ng_norm &lt;- \n  ggplot(example_data, aes(sample = norm)) +\n  geom_qq() +\n  geom_qq_line() +\n  ggtitle(\"Sampled from a\\nnormal distribution\")\n\n\ng_lnorm &lt;- \n  ggplot(example_data, aes(sample = lnorm)) +\n  geom_qq() +\n  geom_qq_line() +\n  ggtitle(\"Sampled from a\\n*log*normal distribution\")\n\ng_norm + g_lnorm\n\n\n\n\n\n\n\n\n\nWe see that, on the left plot, the dots are close to the line, and on the right plot, some dots are very far from the line. These dots far from the line are a warning that the distribution is probably not normal."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#impact-of-sample-size-on-p-values",
    "href": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#impact-of-sample-size-on-p-values",
    "title": "Hypothesis testing, data transformation, compositional data",
    "section": "Impact of sample size on p-values",
    "text": "Impact of sample size on p-values\n\nSmall dataset\nLet’s re-do our analysis but decrease the sample size to 10 samples per dataset.\n\n\nCode\n# another way to simulate data is to do the following\n# check what the expand_grid and rowwise function do\nX_small_sample &lt;- \n  expand_grid(\n    dataset = (1:3) |&gt; factor(), \n    sample = 1:10\n  ) %&gt;% \n  mutate(\n    pop_true_mean = ifelse(dataset == 3, 2, 0),\n    pop_true_var = 2\n  ) %&gt;% \n  rowwise() %&gt;% \n  mutate(\n    value = \n      rnorm(1, mean = pop_true_mean, sd = sqrt(pop_true_var))\n  ) %&gt;% \n  ungroup()\n\nX_small_sample |&gt; head() |&gt; pander()\n\n\n\n\n\n\n\n\n\n\n\n\ndataset\nsample\npop_true_mean\npop_true_var\nvalue\n\n\n\n\n1\n1\n0\n2\n0.3033\n\n\n1\n2\n0\n2\n-0.4592\n\n\n1\n3\n0\n2\n0.1338\n\n\n1\n4\n0\n2\n-1.266\n\n\n1\n5\n0\n2\n-1.854\n\n\n1\n6\n0\n2\n2.824\n\n\n\n\n\n\n\nCode\nggplot(X_small_sample, aes(x = value, fill = dataset)) +\n  geom_histogram(bins = 50) +\n  facet_grid(dataset ~ ., labeller = label_both) +\n  guides(fill = \"none\") + \n  theme(strip.text.y = element_text(angle = 0, hjust = 0)) \n\n\n\n\n\n\n\n\n\nWe can still use a \\(t\\)-test because we know that our samples are drawn from a Normal distribution.\n\n\nCode\nt.test(value ~ dataset, \n       data = X_small_sample |&gt; filter(dataset %in% c(1,3)))\n\n\n\n    Welch Two Sample t-test\n\ndata:  value by dataset\nt = -3.2548, df = 17.988, p-value = 0.004402\nalternative hypothesis: true difference in means between group 1 and group 3 is not equal to 0\n95 percent confidence interval:\n -3.4653702 -0.7465168\nsample estimates:\nmean in group 1 mean in group 3 \n      -0.377852        1.728091 \n\n\nWe see that the \\(p\\)-value is now much larger because, with the small datasets, we have a lot more uncertainty on the true means of the populations.\n\n\nLarge datasets\nLet’s now redo the same again but with a very large sample size for each dataset. For example N = 1000.\n\n\nCode\nX_large_sample &lt;- \n  expand_grid(\n    dataset = (1:3) |&gt; factor(), \n    sample = 1:1000\n    ) %&gt;% \n  mutate(\n    pop_true_mean = ifelse(dataset == 3, 2, 0),\n    pop_true_var = 2\n  ) %&gt;% \n  rowwise() %&gt;% \n  mutate(\n    value = \n      rnorm(1, mean = pop_true_mean, sd = sqrt(pop_true_var))\n  ) %&gt;% \n  ungroup()\n\nX_large_sample |&gt; head() |&gt; pander()\n\n\n\n\n\n\n\n\n\n\n\n\ndataset\nsample\npop_true_mean\npop_true_var\nvalue\n\n\n\n\n1\n1\n0\n2\n-0.8117\n\n\n1\n2\n0\n2\n0.874\n\n\n1\n3\n0\n2\n1.57\n\n\n1\n4\n0\n2\n1.001\n\n\n1\n5\n0\n2\n-0.5143\n\n\n1\n6\n0\n2\n0.0845\n\n\n\n\n\n\n\nCode\nggplot(X_large_sample, aes(x = value, fill = dataset)) +\n  geom_histogram(bins = 50) +\n  facet_grid(dataset ~ ., labeller = label_both) +\n  guides(fill = \"none\") + \n  theme(strip.text.y = element_text(angle = 0, hjust = 0)) \n\n\n\n\n\n\n\n\n\n\n\nCode\nt.test(value ~ dataset, \n       data = X_large_sample |&gt; filter(dataset %in% c(1,3)))\n\n\n\n    Welch Two Sample t-test\n\ndata:  value by dataset\nt = -30.732, df = 1997.9, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group 1 and group 3 is not equal to 0\n95 percent confidence interval:\n -2.075287 -1.826308\nsample estimates:\nmean in group 1 mean in group 3 \n     0.03031352      1.98111096 \n\n\nThe \\(p\\)-value is as small as it can be.\nLet’s also re-do the test comparing datasets 1 and 2\n\n\nCode\nt.test(value ~ dataset, \n       data = X_large_sample |&gt; filter(dataset %in% c(1,2)))\n\n\n\n    Welch Two Sample t-test\n\ndata:  value by dataset\nt = 0.075717, df = 1996.9, p-value = 0.9397\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.1186693  0.1282006\nsample estimates:\nmean in group 1 mean in group 2 \n     0.03031352      0.02554787 \n\n\nWe still don’t reject the null hypothesis, which is what we expect.\n\n\nClinical vs Statistical significance\nHowever, sometimes, we need to be careful with large sample sizes because tiny effects can still lead to very small p-values. However, these tiny effects don’t have much but these do not have much clinical relevance.\nLet’s verify that with a difference in means of 0.2\n\n\nCode\nt.test(\n  x = rnorm(1000, mean = 0.0, sd = 2),\n  y = rnorm(1000, mean = 0.2, sd = 2)\n)\n\n\n\n    Welch Two Sample t-test\n\ndata:  rnorm(1000, mean = 0, sd = 2) and rnorm(1000, mean = 0.2, sd = 2)\nt = -2.18, df = 1998, p-value = 0.02937\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.36547886 -0.01931448\nsample estimates:\n  mean of x   mean of y \n-0.02210611  0.17029056 \n\n\nThe \\(p\\)-value is lower than 1/20, but the effect (i.e., the mean in differences), compared to the standard deviations is very small.\nThis is what we are talking about when discussing the “clinical” vs “statistical” significance."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#cytokine-data-exploration",
    "href": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#cytokine-data-exploration",
    "title": "Hypothesis testing, data transformation, compositional data",
    "section": "Cytokine data exploration",
    "text": "Cytokine data exploration\nOur first task will be to display the distribution of the “IL-1\\(\\beta\\)” cytokine.\nTo do so, we load the cytokine data and filter for the cytokine of interest.\n\n\nCode\n# notice where I had stored my files.\n# Make sure to modify the path to your files\n\nelisa &lt;- \n  read_csv(\n    file = \"data/03_elisa_cytokines_UKZN_workshop_2023.csv\",\n    show_col_types = FALSE\n  )\n\nIL1b &lt;- elisa |&gt; filter(cytokine == \"IL-1b\") \n\n\nWe display its distribution with the geom_histogram function, and we can use the color (fill) option to flag whether the concentration value was within or out of the limits of detection.\n\n\nCode\ncytokine_lim_cols &lt;-\n  c(\"within limits\" = \"navy\", \"out of range\" = \"indianred1\")\n\nggplot(IL1b, aes(x = conc, fill = limits)) +\n  geom_histogram(bins = 30) +\n  ggtitle(\"IL-1b concentrations\") +\n  scale_fill_manual(values = cytokine_lim_cols)\n\n\n\n\n\n\n\n\n\nWhat do we observe?\nDo we think that the distribution of IL-1\\(\\beta\\) concentations follow a normal distribution?\nLet’s still check with a QQ-plot:\n\n\nCode\nggplot(\n  IL1b, \n  aes(sample = conc)\n) +\n  geom_qq_line() +\n  geom_qq(size = 0.75, alpha = 0.5)\n\n\n\n\n\n\n\n\n\nLook back at the QQ-plots we did earlier on simulated data. Does this QQ-plot look like one of the simulated one?"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#log-transformation",
    "href": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#log-transformation",
    "title": "Hypothesis testing, data transformation, compositional data",
    "section": "Log-transformation",
    "text": "Log-transformation\nLet’s now repeat the last two displays, using the log of the concentration instead of the concentration itself. Remember, we can use the mutate function to add a new variable to our data.frame.\n\n\nCode\nIL1b &lt;- IL1b |&gt; mutate(logconc = log10(conc))\n\n\n\n\nCode\nggplot(IL1b, aes(x = logconc, fill = limits)) +\n  geom_histogram(bins = 30) +\n  ggtitle(\"IL-1b concentrations (log 10)\") +\n  xlab(\"log10(conc)\") +\n  scale_fill_manual(values = cytokine_lim_cols)\n\n\n\n\n\n\n\n\n\nNow, the “within-range” data is more compatible with a Normal distribution.\nLet’s check with a QQ-plot:\n\n\nCode\nggplot(\n  IL1b, \n  aes(sample = logconc)\n) +\n  geom_qq_line() +\n  geom_qq(size = 0.75, alpha = 0.5) \n\n\n\n\n\n\n\n\n\nIt is not perfect, but it is much better."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#association-with-bv",
    "href": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#association-with-bv",
    "title": "Hypothesis testing, data transformation, compositional data",
    "section": "Association with BV",
    "text": "Association with BV\nNow, we are interested in testing whether a BV (Bacterial Vaginosis) diagnosis is associated with different mean concentrations of IL-1\\(\\beta\\).\nRemember, the BV diagnosis is contained in the Clinical Measurements table. So we first need to load that table in R.\n\n\nCode\nclin &lt;- \n  read_csv(\n    \"data/02_visit_clinical_measurements_UKZN_workshop_2023.csv\",\n    show_col_types = FALSE\n  )\n\n\nThe first 6 rows of the clinical data are:\n\n\nCode\nclin |&gt; head() |&gt; pander()\n\n\n\n\n\n\n\n\n\n\n\n\n\npid\ntime_point\narm\nnugent_score\ncrp_blood\nph\n\n\n\n\npid_01\nbaseline\nplacebo\n8\n0.44\n5.7\n\n\npid_01\nweek_1\nplacebo\n7\n1.66\n5.2\n\n\npid_01\nweek_7\nplacebo\n7\n1.44\n5.4\n\n\npid_02\nbaseline\nplacebo\n7\n1.55\n5.2\n\n\npid_02\nweek_1\nplacebo\n7\n0.75\n4.8\n\n\npid_02\nweek_7\nplacebo\n4\n1.17\n4.2\n\n\n\n\n\nThere are several ways to diagnose BV. One of them is to check whether a person has a Nugent score of 7 or more.\nLet’s create a new BV variable that says whether a person was diagnosed with BV at each visit.\n\n\nCode\nclin &lt;- \n  clin |&gt; \n  mutate(BV = ifelse(nugent_score &gt;= 7, \"BV\", \"Healthy\")) \n\n\nSince we want to compare the IL-1\\(\\beta\\) concentrations of individuals with or without BV, we need to join the clinical data with the IL-1\\(\\beta\\) concentration data.\nHint: to do so, we need a table describing how the sample IDs and participant x visit IDs are linked together - this info is in the “Sample ID” table.\n\n\nCode\nsample_info &lt;- \n  read_csv(\n    file = \"data/00_sample_ids_UKZN_workshop_2023.csv\",\n    show_col_types = FALSE\n  )\n\n\nThe first 6 rows of the sample info table are:\n\n\nCode\nsample_info |&gt; head() |&gt; pander()\n\n\n\n\n\n\n\n\n\n\n\npid\ntime_point\narm\nsample_id\n\n\n\n\npid_17\nbaseline\ntreatment\nSAMP001\n\n\npid_22\nweek_1\ntreatment\nSAMP002\n\n\npid_25\nweek_1\ntreatment\nSAMP003\n\n\npid_41\nweek_1\ntreatment\nSAMP004\n\n\npid_44\nweek_7\ntreatment\nSAMP005\n\n\npid_16\nweek_1\ntreatment\nSAMP006\n\n\n\n\n\nWe see that we can use the sample_id column to bind the cytokine concentrations with the sample_info table, then bind on the participant and visit ID with the clinical data.\n\n\nCode\nIL1b &lt;- \n  IL1b |&gt; \n  left_join(sample_info, by = join_by(sample_id)) |&gt; \n  left_join(clin, by = join_by(pid, time_point, arm))\n\n\nNow that our data are joined, let’s display the histogram of IL-1\\(\\beta\\) concentrations, including those out-of-range, colored by BV diagnosis.\n\n\nCode\nBV_colors &lt;- \n  c(\"BV\" = \"darkgoldenrod1\", \"Healthy\" = \"cornflowerblue\")\n\nggplot(\n  IL1b, #|&gt; filter(limits == \"within limits\"),\n  aes(x = logconc, fill = BV)\n) +\n  geom_histogram(bins = 30, alpha = 0.5, position = \"identity\") +\n  xlab(\"log10(conc)\") +\n  ggtitle(\"IL-1b concentration by BV diagnosis\") +\n  scale_fill_manual(values = BV_colors)\n\n\n\n\n\n\n\n\n\nFrom this visualization, it looks like the distribution is lower in healthy participants than in participants with BV.\nWhat do we think of the “out-of-range” values? Should we include them in our analysis?\nLet’s do a statistical test to see if these differences in concentrations could have happened by chance.\nSince our data, once log-transformed, look normal, (with the exclusion of the out-of-range samples) it makes sense to use a \\(t\\)-test. We can also check how many samples we have in each group to check if, regardless of the underlying distribution, there are enough samples to use a \\(t\\)-test.\n\n\nCode\nIL1b |&gt; \n  select(BV) |&gt; \n  table()\n\n\nBV\n     BV Healthy \n     46      86 \n\n\nWe have many samples, so we could, regardless of the underlying distribution, use a \\(t\\)-test.\n\n\nCode\nt.test(logconc ~ BV, data = IL1b)\n\n\n\n    Welch Two Sample t-test\n\ndata:  logconc by BV\nt = 6.5449, df = 116.4, p-value = 1.672e-09\nalternative hypothesis: true difference in means between group BV and group Healthy is not equal to 0\n95 percent confidence interval:\n 0.5332052 0.9959381\nsample estimates:\n     mean in group BV mean in group Healthy \n            1.1245273             0.3599557 \n\n\nWhat do you conclude?"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#the-wilcoxon-rank-sum-test",
    "href": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#the-wilcoxon-rank-sum-test",
    "title": "Hypothesis testing, data transformation, compositional data",
    "section": "The Wilcoxon rank-sum test",
    "text": "The Wilcoxon rank-sum test\nThe Wilcoxon rank-sum test is a non-parametric test to compare two independent datasets. The null hypothesis is that, for randomly selected values \\(X\\) and \\(Y\\) from two populations, the probability of \\(X\\) being greater than \\(Y\\) is equal to the probability of \\(Y\\) being greater than \\(X\\) (see Wikipedia), regardless of the distribution \\(X\\) and \\(Y\\) are drawn from.\nLet’s try the test on our small dataset:\n\n\nCode\nwilcox.test(value ~ dataset, \n            data = X_small_sample |&gt; filter(dataset %in% c(1,3)))\n\n\n\n    Wilcoxon rank sum exact test\n\ndata:  value by dataset\nW = 13, p-value = 0.003886\nalternative hypothesis: true location shift is not equal to 0\n\n\nAnd compare it to the \\(t\\)-test (we can because we know this data is drawn from a Normal).\n\n\nCode\nt.test(value ~ dataset, \n       data = X_small_sample |&gt; filter(dataset %in% c(1,3)))\n\n\n\n    Welch Two Sample t-test\n\ndata:  value by dataset\nt = -3.2548, df = 17.988, p-value = 0.004402\nalternative hypothesis: true difference in means between group 1 and group 3 is not equal to 0\n95 percent confidence interval:\n -3.4653702 -0.7465168\nsample estimates:\nmean in group 1 mean in group 3 \n      -0.377852        1.728091 \n\n\nWe see that both provides small probabilites to observe the data under the null hypothesis.\nSo, why not always using a non-parametric test if they work in all situations and not worry about normality of the data?\nBecause, non-parametric tests have less power to detect small effects.\nTo check that, we can rely on simulations.\nHere, we simulate 1000 times two small datasets of 10 samples with a relatively small difference between their means (relative to the standard deviation) and perform both a \\(t\\) test and a Wilcoxon rank sum test.\nWe then count how many times each test rejected the null (as they should) and see if one of the two tests reject more often.\n\n\nCode\nset.seed(123)\nsimulate_and_run_both_test &lt;- function(){\n  x1 &lt;- rnorm(10, mean = 0, sd = 2)\n  x2 &lt;- rnorm(10, mean = 1.5, sd = 2)\n  \n  ttest &lt;- t.test(x1, x2)\n  wtest &lt;- wilcox.test(x1, x2)\n  tibble(ttest_pvalue = ttest$p.value, wtest_pvalue = wtest$p.value)\n}\n\nsimulation_results &lt;- replicate(1000, simulate_and_run_both_test())\n\n\napply(simulation_results &lt;= 0.05, MARGIN = 2, FUN = mean)\n\n\nttest_pvalue wtest_pvalue \n       0.354        0.319 \n\n\nCode\n# apply is a function that allows to \"apply\" a given function (here the function \"mean\") to all rows or columns of a table.\n# To chose if to apply to rows or to columns, we use 1 (rows) or 2 (columns) in the 2nd argument of the apply function. The first argument is the table we apply the function to.\n\n\nWe see that the \\(t\\)-test more frequently detected that the two samples were drawn from distribution with different means/locations."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#simulations",
    "href": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#simulations",
    "title": "Hypothesis testing, data transformation, compositional data",
    "section": "Simulations",
    "text": "Simulations\nAs we discussed above, the \\(p\\)-value of a test is the probability, under the null hypothesis, to observe a value of the test statistics as extreme as the one we observe. Usually, if that probability is less than 0.05 (we have less than one/20 chances to observe that value), we reject the null hypothesis.\nSo, if we repeat a test many many times on data generated under the null hypothesis (so we should not reject it), we will obtain a small \\(p\\)-value a few times.\nTo verify this, let’s do a simulation experiment and repeat the following procedure a 1000 times: we draw two small datasets from that same population (= the null hypothesis is true) and perform a \\(t\\)-test. In theory, we should get 0.05 x 1000 = 50 experiments with a \\(p\\)-value smaller than 0.05.\n\n\nCode\nsimulate_and_test &lt;- function(){\n  x1 &lt;- rnorm(10)\n  x2 &lt;- rnorm(10) # same mean\n  ttest &lt;- t.test(x1, x2)\n  ttest$p.value # we return the p-value\n}\n\nnull_p.values &lt;- replicate(1000, simulate_and_test())\n\n\nLet’s count how many times we have a p-value smaller than 0.05 under the null hypothesis:\n\n\nCode\nsum(null_p.values &lt; 0.05)\n\n\n[1] 52\n\n\nWhich corresponds to a rate of\n\n\nCode\nmean(null_p.values &lt; 0.05)\n\n\n[1] 0.052\n\n\nwhich is, indeed, very close to the \\(p\\)-value threshold that we’ve selected.\nThis is because, under the null hypothesis, the distribution of \\(p\\)-values is uniform:\n\n\nCode\nnull_p.values_tbl &lt;- tibble(p_values = null_p.values)\n\nggplot(null_p.values_tbl, aes(x = p_values)) +\n  geom_histogram(bins = 20)\n\n\n\n\n\n\n\n\n\nThis is a good opportunity to learn how to perform a QQ-plot for another distribution than the Normal distribution. Here, to check that the p-values follow a uniform distribution, we can use the distribution argument from the geom_qq functions and do the following QQ-plot:\n\n\nCode\nggplot(null_p.values_tbl, aes(sample = p_values)) +\n  geom_qq_line(distribution = qunif) +\n  geom_qq(distribution = qunif, size = 0.5, alpha = 0.5)\n\n\n\n\n\n\n\n\n\nThis is very convincing that the \\(p\\)-values under the null follow a uniform distribution.\nNow, let’s come back to the interpretation/consequences of this uniform distribution: if the distribution of \\(p\\)-values under the Null is uniform, this means that we will reject the null hypothesis with a rate of \\(\\alpha\\) if \\(\\alpha\\) is the rejection threshold.\nThis will also apply to real data if we perform the same test several times.\nFor example, let’s say that we were interested in not just IL-1\\(\\beta\\) but all cytokines and are to repeat the \\(t\\)-test we did above for all cytokines, since we have 10 of them, it’s not unlikely that we’d have small \\(p\\)-values just by chance.\nSo, whenever we do “multiple testing”, we need to adjust for that multiple testing.\nThere are several ways to do “multiple testing adjustments” but the explanations of these methods are outside the scope of this class. Many of these methods have conveniently been implemented in the p.adjust function.\nOne of these methods that “controls for the”false discovery rate” is the Benjamini-Hochberg adjustment.\nLet’s apply it to our simulated \\(p\\)-values and check now how many samples are still considered to have a significant adjusted \\(p\\)-value:\n\n\nCode\nnull_p.values_tbl &lt;- \n  null_p.values_tbl |&gt; \n  mutate(q_values = p.adjust(p_values, method = \"BH\"))\n\n\nmean(null_p.values_tbl$q_values &lt;= 0.05)\n\n\n[1] 0\n\n\nNone of them!\nWhich is what it should be as we simulated our data under the null hypothesis, we should never reject the Null.\n\n\nCode\nggplot(null_p.values_tbl, aes(x = q_values)) +\n  geom_histogram(bins = 20) +\n  expand_limits(x = 0) +\n  xlab(\"BH adjusted p-values\")"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#cytokines",
    "href": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#cytokines",
    "title": "Hypothesis testing, data transformation, compositional data",
    "section": "Cytokines",
    "text": "Cytokines\nLet’s now test which cytokines have different means in BV and non-BV study participants.\nFirst, we need to take the log-concentration of all cytokines and join with the clinical data via the “sample info”.\n\n\nCode\nelisa &lt;- \n  elisa |&gt; \n  mutate(logconc = log10(conc)) |&gt; \n  left_join(sample_info, by = join_by(sample_id)) |&gt; \n  left_join(clin, by = join_by(pid, time_point, arm))\n\n\nWe can also display concentration by BV status for each cytokine:\n\n\nCode\nelisa |&gt; \n  ggplot(aes(x = logconc, fill = BV)) +\n  geom_histogram(bins = 30, position = \"identity\", alpha = 0.5) +\n  facet_wrap(cytokine ~ .) +\n  xlab('log10(concentrations)') +\n  scale_fill_manual(values = BV_colors)\n\n\n\n\n\n\n\n\n\nAnother way to look at this is to display the data as “boxplot”:\n\n\nCode\nelisa |&gt; \n  ggplot(aes(y = logconc, fill = BV, col = BV, x = cytokine)) +\n  geom_boxplot(varwidth = TRUE, outlier.size = 0.5, alpha = 0.5) +\n  scale_fill_manual(values = BV_colors) +\n  scale_color_manual(values = BV_colors)\n\n\n\n\n\n\n\n\n\nLooking at these visual displays of the data. Do you think it makes sense to do the test for all cytokines? What about cytokines that have a lot of samples with concentrations lower than the limit of detection?\nShould we exclude these cytokines? Remove the out-of-range values? Or keep them?\nIdeally, we would only want to do the test for variables where we are confident that we have representative samples.\nThere is a little bit of subjectivity in terms of what we deem representative, but, for now, let’s say that we want the 1st and 3rd quartile to be within the range of detection for each cytokine and each group.\nLet’s thus compute the 1st and 3rd quartiles (or, equivalently the 25th and 75th percentiles) for each cytokines and each group:\n\n\nCode\ninterquartiles &lt;- \n  elisa |&gt; \n  # we also need to compute the LLOQ and ULOQ for each cytokine\n  group_by(cytokine) |&gt; \n  mutate(LLOQ = min(logconc), ULOQ = max(logconc)) |&gt;\n  group_by(cytokine, LLOQ, ULOQ, BV) |&gt; \n  summarize(\n    `1st quartile` = quantile(logconc, 0.25),\n    `3rd quartile` = quantile(logconc, 0.75)\n  ) \n\n\n`summarise()` has grouped output by 'cytokine', 'LLOQ', 'ULOQ'. You can\noverride using the `.groups` argument.\n\n\nCode\ninterquartiles |&gt; \n  pander()\n\n\n\n\n\n\n\n\n\n\n\n\n\ncytokine\nLLOQ\nULOQ\nBV\n1st quartile\n3rd quartile\n\n\n\n\nIFN-Y\n-0.4641\n1.369\nBV\n-0.4641\n-0.1024\n\n\nIFN-Y\n-0.4641\n1.369\nHealthy\n-0.4641\n-0.4641\n\n\nIL-10\n-0.1152\n1.482\nBV\n-0.1152\n0.3345\n\n\nIL-10\n-0.1152\n1.482\nHealthy\n-0.1152\n-0.1152\n\n\nIL-1a\n0.143\n3.011\nBV\n1.681\n2.238\n\n\nIL-1a\n0.143\n3.011\nHealthy\n1.067\n1.976\n\n\nIL-1b\n-0.6012\n2.347\nBV\n0.8021\n1.334\n\n\nIL-1b\n-0.6012\n2.347\nHealthy\n-0.2347\n0.9864\n\n\nIL-6\n-1.029\n2.238\nBV\n0.08762\n0.7945\n\n\nIL-6\n-1.029\n2.238\nHealthy\n-0.364\n0.6946\n\n\nIL-8\n-0.3279\n3.398\nBV\n2.164\n2.916\n\n\nIL-8\n-0.3279\n3.398\nHealthy\n1.575\n2.71\n\n\nIP-10\n-0.5157\n3.699\nBV\n-0.5157\n0.06446\n\n\nIP-10\n-0.5157\n3.699\nHealthy\n-0.1079\n1.945\n\n\nMIG\n0.1872\n4.398\nBV\n0.1872\n1.07\n\n\nMIG\n0.1872\n4.398\nHealthy\n0.6586\n2.023\n\n\nMIP-3a\n0.7137\n2.516\nBV\n0.7137\n0.7137\n\n\nMIP-3a\n0.7137\n2.516\nHealthy\n0.7137\n1.168\n\n\nTNFa\n-0.327\n1.631\nBV\n-0.327\n0.5051\n\n\nTNFa\n-0.327\n1.631\nHealthy\n-0.327\n0.1446\n\n\n\n\n\nLet’s filter for cytokines with an interquartile range within the limits of quantification for both groups:\n\n\nCode\ninterquartiles |&gt; \n  filter(\n    `1st quartile` &gt; LLOQ,\n    `3rd quartile` &lt; ULOQ\n  ) |&gt; \n  group_by(cytokine) |&gt;\n  summarize(n = n(), .groups = \"drop\") |&gt; \n  filter(n == 2) |&gt;\n  pander()\n\n\n\n\n\n\n\n\n\ncytokine\nn\n\n\n\n\nIL-1a\n2\n\n\nIL-1b\n2\n\n\nIL-6\n2\n\n\nIL-8\n2\n\n\n\n\n\nThat only leaves us with 4 cytokines. Our criteria might be a little too stringent, and, as long as you justify it properly, you could loosen these criteria. We just want to make sure that you always take a critical look at your data before running a test.\nWe will now filter our data to only keep these 4 cytokines and perform a statistical test for each of theses.\n\n\nCode\nttest_res &lt;- \n  elisa |&gt; \n  group_by(cytokine) |&gt; \n  filter(\n    cytokine %in% c(\"IL-1a\",\"IL-1b\", \"IL-6\", \"IL-8\")\n  ) |&gt;\n  summarize(\n    `p-value` = t.test(logconc ~ BV)$p.value,\n    `p &lt; 0.05` = ifelse(`p-value` &lt; 0.05, \"yes\", \"\")\n  ) \n\nttest_res |&gt; pander()\n\n\n\n\n\n\n\n\n\n\ncytokine\np-value\np &lt; 0.05\n\n\n\n\nIL-1a\n8.492e-08\nyes\n\n\nIL-1b\n1.672e-09\nyes\n\n\nIL-6\n0.08282\n\n\n\nIL-8\n0.00825\nyes\n\n\n\n\n\nWe see that 3/4 cytokines have p-values smaller than 0.05. But remember, we need to adjust for multiple testing.\n\n\nCode\nttest_res &lt;- \n  ttest_res |&gt; \n  mutate(\n    `adj. p-value` = p.adjust(`p-value`, method = \"BH\"),\n    `statistical\\nsignificance` = \n      ifelse(`adj. p-value` &lt; 0.05, \"yes\",\"\")\n  )\n\n\nttest_res |&gt; pander()\n\n\n\n\n\n\n\n\n\n\n\n\ncytokine\np-value\np &lt; 0.05\nadj. p-value\nstatistical significance\n\n\n\n\nIL-1a\n8.492e-08\nyes\n1.698e-07\nyes\n\n\nIL-1b\n1.672e-09\nyes\n6.689e-09\nyes\n\n\nIL-6\n0.08282\n\n0.08282\n\n\n\nIL-8\n0.00825\nyes\n0.011\nyes\n\n\n\n\n\nThe three cytokines that had a p-value smaller than 0.05 are still significant after adjusting for multiple testing.\nRedo this analysis, but with a less stringent criteria for representativeness. For example, let’s only request from our cytokines that their 1st and 3rd quartiles are different.\n\n\nCode\nselected_cytokines &lt;- \n  interquartiles |&gt; \n  filter(\n    `1st quartile` &lt; `3rd quartile`\n  ) |&gt; \n  group_by(cytokine) |&gt;\n  summarize(n = n(), .groups = \"drop\") |&gt; \n  filter(n == 2) |&gt;\n  select(cytokine) |&gt; unlist()\n\nselected_cytokines\n\n\ncytokine1 cytokine2 cytokine3 cytokine4 cytokine5 cytokine6 cytokine7 \n  \"IL-1a\"   \"IL-1b\"    \"IL-6\"    \"IL-8\"   \"IP-10\"     \"MIG\"    \"TNFa\" \n\n\n\n\nCode\nttest_res &lt;- \n  elisa |&gt; \n  filter(cytokine %in% selected_cytokines) |&gt; \n  group_by(cytokine) |&gt;\n  summarize(\n    `p-value` = t.test(logconc ~ BV)$p.value,\n    `p &lt; 0.05` = ifelse(`p-value` &lt; 0.05, \"yes\", \"\")\n  )  |&gt; \n  mutate(\n    `adj. p-value` = p.adjust(`p-value`, method = \"BH\"),\n    `statistical\\nsignificance` = \n      ifelse(`adj. p-value` &lt; 0.05, \"yes\",\"\")\n  )\n\n\nttest_res |&gt; pander()\n\n\n\n\n\n\n\n\n\n\n\n\ncytokine\np-value\np &lt; 0.05\nadj. p-value\nstatistical significance\n\n\n\n\nIL-1a\n8.492e-08\nyes\n1.486e-07\nyes\n\n\nIL-1b\n1.672e-09\nyes\n5.853e-09\nyes\n\n\nIL-6\n0.08282\n\n0.08282\n\n\n\nIL-8\n0.00825\nyes\n0.01155\nyes\n\n\nIP-10\n1.974e-12\nyes\n1.381e-11\nyes\n\n\nMIG\n4.902e-08\nyes\n1.144e-07\nyes\n\n\nTNFa\n0.02701\nyes\n0.03151\nyes\n\n\n\n\n\nWhat do you conclude from this analysis?\nAs an exercise, you can also re-do the analyses using a non-parametric test and discuss the differences in the results."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#paired-tests",
    "href": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#paired-tests",
    "title": "Hypothesis testing, data transformation, compositional data",
    "section": "Paired tests",
    "text": "Paired tests\nOne question that this study may have aimed to answer is whether treatment altered how the cytokine concentrations changed over time.\nSpecifically, we may have been interested in comparing the changes between the baseline visit and the week 7 visit.\nLet’s do the same visualization, but excluding week 1:\n\n\nCode\nelisa |&gt; \n  filter(cytokine == \"IL-1b\", time_point != \"week 1\") |&gt; \n  ggplot(aes(x = time_point, y = logconc, color = arm)) +\n  geom_line(aes(group = pid), alpha = 0.75) +\n  geom_point(alpha = 0.5) +\n  xlab(\"Visits\") +\n  ylab(\"log10(IL-1b concentration)\") +\n  scale_color_manual(\"Arm\", values = arm_color)\n\n\n\n\n\n\n\n\n\nWe see that everyone starts with a different level of IL-1b and the ranking in the baseline visit is somewhat similar to the ranking in the week 7 visit, especially in the treatment arm.\nTo test this specifically, we can do a “paired test”.\nIt is “paired” because for each observation at the baseline visit, there is a “sister” observation at the week 7 visit (= the sample from the same participant).\nHere, because we have many samples, we can use a paired \\(t\\)-test, but if we had only one sample per participant, we would have to use a paired Wilcoxon test.\nIn a “real” analysis, we would only do one of the two tests, but for demonstration purposes, let’s do both (without adjusting for multiple testing because it’s just illustrative).\n\n\nCode\nelisa |&gt; \n  filter(cytokine == \"IL-1b\", time_point != \"week 1\") |&gt; \n  select(cytokine, arm, pid, time_point, logconc) |&gt;\n  pivot_wider(names_from = time_point, values_from = logconc) |&gt;\n  group_by(arm) |&gt; \n  summarize(\n    `p (t-test)` = \n      t.test(x = baseline, y = `week 7`, paired = TRUE)$p.value,\n    `p (signed rank)` = \n      wilcox.test(x = baseline, y = `week 7`, paired = TRUE)$p.value\n  ) |&gt; \n  mutate(\n    `p &lt; 0.05 (t)` = ifelse(`p (t-test)` &lt; 0.05, \"yes\", \"\"),\n    `p &lt; 0.05 (w)` = ifelse(`p (signed rank)` &lt; 0.05, \"yes\", \"\")\n  ) |&gt; \n  pander()\n\n\nWarning: There was 1 warning in `summarize()`.\nℹ In argument: `p (signed rank) = wilcox.test(x = baseline, y = `week 7`,\n  paired = TRUE)$p.value`.\nℹ In group 2: `arm = treatment`.\nCaused by warning in `wilcox.test.default()`:\n! cannot compute exact p-value with ties\n\n\n\n\n\n\n\n\n\n\n\n\narm\np (t-test)\np (signed rank)\np &lt; 0.05 (t)\np &lt; 0.05 (w)\n\n\n\n\nplacebo\n0.3998\n0.41\n\n\n\n\ntreatment\n0.0007528\n0.001226\nyes\nyes\n\n\n\n\n\nWe see that the two tests agree in this case (as they should because the effects are quite large in the treatment arm).\nExercise: repeat the same visualization and analysis for IL-6 and IL-8. Do you need to adjust for multiple testing?"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#flow-cytometry-data",
    "href": "materials/1-workshop1/6-hypothesis-testing/module-6-hands-on.html#flow-cytometry-data",
    "title": "Hypothesis testing, data transformation, compositional data",
    "section": "Flow cytometry data",
    "text": "Flow cytometry data\nFor now, let’s explore the flow cytometry data.\n\n\nCode\nflow &lt;- \n  read_csv(\n    \"data/04_flow_cytometry_UKZN_workshop_2023.csv\"\n    )\n\n\nRows: 132 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): sample_id\ndbl (9): live_cd19_negative, cd45_negative, cd45_positive, neutrophils, non_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe have one row per sample. And remember, we have three sample per patient (one for each visit).\nOne of the first things we observe is that all columns (except sample_id) are (large) integer numbers. We say that we have “count data”.\nRemember, that these are the number of cells observed in each “gating” categories. Also, remember that some cell categories are subset of other categories.\nFor example, “live CD19-” cells are roughly divided into “CD45+” and “CD45-” cells.\n\n\nCode\nggplot(flow, \n       aes(y = live_cd19_negative, \n           x = cd45_positive + cd45_negative)\n       ) +\n  geom_abline(intercept = 0, slope = 1, col = \"purple1\") +\n  geom_point(alpha = 0.5) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\n\n\nAnd “CD3+” are roughly divided into “CD4 T” and “CD8 T” cells.\n\n\nCode\nggplot(flow, \n       aes(x = cd3_positive, \n           y = cd4_t_cells + cd8_t_cells)\n       ) +\n  geom_abline(intercept = 0, slope = 1, col = \"purple1\") +\n  geom_point(alpha = 0.5) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\n\n\nSo, if we show the fraction of “CD4 T” cells against that of “CD8 T” cells among all T cells (the “CD3+” cells).\n\n\nCode\nggplot(flow, \n       aes(x = cd4_t_cells/cd3_positive, \n           y = cd8_t_cells/cd3_positive )\n       ) +\n  geom_point(alpha = 0.5) +\n  xlab(\"CD4 T cells (fraction of all T cells)\") +\n  ylab(\"CD8 T cells (fraction of all T cells)\")\n\n\n\n\n\n\n\n\n\nWe see that we have a negative correlation: when the proportion of CD8 T cells increases, the proportion of CD4 T cells decreases.\nWhat are the consequences of that?\nLet’s say that we are interested in looking at the treatment effect on the proportion of different T cell sub-types. If we were to forget that these were compositional data, we could do the same as what we did for the cytokines, and perform a test for each cell sub-type.\nBut since we know that they are anti-correlated, if there is an effect on one sub-type, there might be an opposite effect on the other sub-type.\nLet’s see if that’s the case.\nFirst, let’s create a simplified flow cytometry table with just the T cell data and compute the proportion of the CD4 and CD8 T cells.\n\n\nCode\nflow_T &lt;- \n  flow |&gt; \n  select(sample_id, cd4_t_cells, cd8_t_cells, cd3_positive) |&gt;\n  mutate(\n    cd4_t_cells_f = cd4_t_cells/cd3_positive,\n    cd8_t_cells_f = cd8_t_cells/cd3_positive,\n    sum = cd4_t_cells_f + cd8_t_cells_f,\n    remaining_t_cells_f = 1 - sum\n    ) \n\nhead(flow_T) |&gt; pander()\n\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\nsample_id\ncd4_t_cells\ncd8_t_cells\ncd3_positive\ncd4_t_cells_f\n\n\n\n\nSAMP094\n409\n444\n988\n0.414\n\n\nSAMP052\n7126\n6009\n15078\n0.4726\n\n\nSAMP017\n36212\n13434\n53758\n0.6736\n\n\nSAMP046\n2872\n2629\n6254\n0.4592\n\n\nSAMP025\n6384\n3039\n11355\n0.5622\n\n\nSAMP050\n2405\n1310\n4076\n0.59\n\n\n\n\n\n\n\n\n\n\n\ncd8_t_cells_f\nsum\nremaining_t_cells_f\n\n\n\n\n0.4494\n0.8634\n0.1366\n\n\n0.3985\n0.8711\n0.1289\n\n\n0.2499\n0.9235\n0.07649\n\n\n0.4204\n0.8796\n0.1204\n\n\n0.2676\n0.8299\n0.1701\n\n\n0.3214\n0.9114\n0.08857\n\n\n\n\n\nWe also computed the sum of the fractions of the CD4 and CD8 T cells and see that this sum is not always 1. That means that some cells are not CD4 or CD8 T cells. The distribution of the sum of their fraction goes as:\n\n\nCode\nggplot(flow_T, aes(x = sum)) +\n  geom_histogram(bins = 30) +\n  geom_vline(xintercept = 1, col = \"red\", linetype = 2) +\n  xlab(\"sum of the fractions of CD4 and CD8 T cells\") \n\n\n\n\n\n\n\n\n\nAs said above, one question we may have is whether the proportions of CD4 and CD8 T cells are different between the two arms at the end of the intervention.\nTo be able to answer this question, we first need to join the flow cytometry data with the sample information (the sample_info table) so we know which sample is from which visit and which participant and their arm.\n\n\nCode\nflow_T_with_info &lt;- \n  flow_T |&gt; \n  left_join(sample_info, by = \"sample_id\")\n\nflow_T_with_info |&gt; head() |&gt; pander()\n\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\nsample_id\ncd4_t_cells\ncd8_t_cells\ncd3_positive\ncd4_t_cells_f\n\n\n\n\nSAMP094\n409\n444\n988\n0.414\n\n\nSAMP052\n7126\n6009\n15078\n0.4726\n\n\nSAMP017\n36212\n13434\n53758\n0.6736\n\n\nSAMP046\n2872\n2629\n6254\n0.4592\n\n\nSAMP025\n6384\n3039\n11355\n0.5622\n\n\nSAMP050\n2405\n1310\n4076\n0.59\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncd8_t_cells_f\nsum\nremaining_t_cells_f\npid\ntime_point\narm\n\n\n\n\n0.4494\n0.8634\n0.1366\npid_01\nbaseline\nplacebo\n\n\n0.3985\n0.8711\n0.1289\npid_01\nweek_1\nplacebo\n\n\n0.2499\n0.9235\n0.07649\npid_01\nweek_7\nplacebo\n\n\n0.4204\n0.8796\n0.1204\npid_02\nbaseline\nplacebo\n\n\n0.2676\n0.8299\n0.1701\npid_02\nweek_1\nplacebo\n\n\n0.3214\n0.9114\n0.08857\npid_02\nweek_7\nplacebo\n\n\n\n\n\nLet’s now display the proportions of these cells at the week 7 visit:\n\n\nCode\nflow_T_with_info_long &lt;- \n  flow_T_with_info |&gt; \n  select(sample_id, pid, time_point, arm, ends_with(\"_f\")) |&gt; \n  pivot_longer(\n    cols = ends_with(\"_f\"),\n    names_to = \"cell_type\",\n    values_to = \"fraction of T cells\"\n    )\n  \nflow_T_with_info_long |&gt; \n  ggplot(aes(x = cell_type, y = `fraction of T cells`,\n             fill = arm)) +\n  geom_boxplot() +\n  scale_fill_manual(values = arm_color) \n\n\n\n\n\n\n\n\n\nIt looks like there might be some differences: a lower proportion of CD4 T cells in the intervention arm than in the placebo arm and and slightly higher proportions in the two others.\nBut from these compositional data, it is impossible to know if one of the cell type is driving these effects or if it is a combination of several changes in each cell types.\nOne could still do a test to see if the proportions are different in the two arms, but this is outside the scope of this workshop. Come back to the next ones for more :)"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/index.html",
    "href": "materials/1-workshop1/6-hypothesis-testing/index.html",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "",
    "text": "Make slides full screen",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 6</b>: Data transformations and more statistics"
    ]
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/index.html#slides",
    "href": "materials/1-workshop1/6-hypothesis-testing/index.html#slides",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "",
    "text": "Make slides full screen",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 6</b>: Data transformations and more statistics"
    ]
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/index.html#for-reference---link-to-html-output---please-dont-use-during-the-teahcing-portion",
    "href": "materials/1-workshop1/6-hypothesis-testing/index.html#for-reference---link-to-html-output---please-dont-use-during-the-teahcing-portion",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "For Reference - Link to html output - please don’t use during the teahcing portion",
    "text": "For Reference - Link to html output - please don’t use during the teahcing portion",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 6</b>: Data transformations and more statistics"
    ]
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/index.html",
    "href": "materials/1-workshop1/5-tableone/index.html",
    "title": "tableone and its Basic Statistics",
    "section": "",
    "text": "Make slides full screen",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 5</b>: <code>tableone</code> and its Basic Statistics"
    ]
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/index.html#slides",
    "href": "materials/1-workshop1/5-tableone/index.html#slides",
    "title": "tableone and its Basic Statistics",
    "section": "",
    "text": "Make slides full screen",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 5</b>: <code>tableone</code> and its Basic Statistics"
    ]
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/index.html#research-questions",
    "href": "materials/1-workshop1/5-tableone/index.html#research-questions",
    "title": "tableone and its Basic Statistics",
    "section": "Research Questions",
    "text": "Research Questions",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 5</b>: <code>tableone</code> and its Basic Statistics"
    ]
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/index.html#potential-outcomes-could-include",
    "href": "materials/1-workshop1/5-tableone/index.html#potential-outcomes-could-include",
    "title": "tableone and its Basic Statistics",
    "section": "Potential outcomes could include",
    "text": "Potential outcomes could include\nChanges in inflammation levels, as measured by the elisa_cytokines and flow_cytometry tables. Changes in clinical measurements such as Nugent Score, C-reactive protein blood test (CRP), and vaginal pH, as recorded in the visit_clinical_measurements table.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 5</b>: <code>tableone</code> and its Basic Statistics"
    ]
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/index.html#potential-research-question-objective-and-statistical-approach",
    "href": "materials/1-workshop1/5-tableone/index.html#potential-research-question-objective-and-statistical-approach",
    "title": "tableone and its Basic Statistics",
    "section": "Potential Research Question, Objective and Statistical Approach",
    "text": "Potential Research Question, Objective and Statistical Approach\nEfficacy of the Treatment\n\nQuestion: How effective is the drug in reducing inflammation at different time points (week 1 and week 7) compared to the baseline? Is there a significant difference between the treatment and placebo groups?\nObjective: Determine the effectiveness of the drug in reducing inflammation.\nStatistical Analysis: Use a repeated measures ANOVA or mixed-effects model to compare inflammation levels at different time points between the treatment and placebo groups.\n\nImpact of Smoking\n\nQuestion: Does smoking status affect the efficacy of the drug? Is there a significant difference in outcomes between smokers and non-smokers?\nObjective: Investigate the influence of smoking on the efficacy of the drug.\nStatistical Analysis: Perform subgroup analysis or interaction analysis to compare outcomes between smokers and non-smokers.\n\nAge and Drug Efficacy\n\nQuestion: Does age influence the effectiveness of the drug? Are there differences in outcomes among different age groups?\nObjective: Explore the influence of age on the effectiveness of the drug.\nStatistical Analysis: Use regression analysis to examine the relationship between age and treatment outcomes.\n\nEducation Level and Treatment Outcome\n\nQuestion: Is there a correlation between the level of education and the treatment outcome? Do individuals with higher education levels have better outcomes?\nObjective: Investigate whether education level correlates with treatment outcomes.\nStatistical Analysis: Perform a chi-square test or Fisher’s exact test to examine the association between education level and treatment outcomes.\n\nCytokine Concentration Analysis\n\nQuestion: How do cytokine concentrations change over time in response to the treatment? Are certain cytokines more responsive to the treatment than others?\nObjective: Analyze changes in cytokine concentrations over time in response to treatment.\nStatistical Analysis: Use a repeated measures ANOVA or mixed-effects model to compare cytokine concentrations at different time points.\n\nCell Count Analysis\n\nQuestion: How do cell counts (from flow cytometry data) change over time in response to the treatment? Are certain cell types more affected by the treatment than others?\nObjective: Analyze changes in cell counts over time in response to treatment.\nStatistical Analysis: Use a repeated measures ANOVA or mixed-effects model to compare cell counts at different time points.\n\nCorrelation between Clinical Measurements and Outcomes\n\nQuestion: Is there a correlation between Nugent Score, C-reactive protein blood test (CRP), vaginal pH, and treatment outcomes?\nObjective: Examine correlations between clinical measurements (Nugent Score, CRP, vaginal pH) and treatment outcomes.\nStatistical Analysis: Use correlation analysis or multiple regression analysis to examine these relationships.\n\nLong-term Efficacy of the Drug\nQuestion: Does the drug’s efficacy persist over time (from week 1 to week 7), or does it diminish? Objective: Assess whether the drug’s efficacy persists over time. Statistical Analysis: Use a repeated measures ANOVA or mixed-effects model to compare treatment outcomes at week 1 and week 7.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 5</b>: <code>tableone</code> and its Basic Statistics"
    ]
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/index.html#refresher",
    "href": "materials/1-workshop1/5-tableone/index.html#refresher",
    "title": "tableone and its Basic Statistics",
    "section": "Refresher",
    "text": "Refresher",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 5</b>: <code>tableone</code> and its Basic Statistics"
    ]
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/index.html#notes",
    "href": "materials/1-workshop1/5-tableone/index.html#notes",
    "title": "tableone and its Basic Statistics",
    "section": "Notes",
    "text": "Notes",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 5</b>: <code>tableone</code> and its Basic Statistics"
    ]
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/index.html#introduction",
    "href": "materials/1-workshop1/5-tableone/index.html#introduction",
    "title": "tableone and its Basic Statistics",
    "section": "Introduction",
    "text": "Introduction\nIn biomedical research, it is often necessary to compare the effects of different interventions or exposures on the health outcomes of interest. However, before making any causal inference, it is important to evaluate the baseline characteristics of the patients who participate in the study. Baseline characteristics are the demographic and clinical features of the participants at the start of a trial or a study, such as age, sex, disease severity, comorbidities, etc. They provide information about the population that is being studied and the context of the research question (Schulz, Altman, & Moher, 2010).\nEvaluating baseline characteristics of patients in different treatment groups is important for several reasons:\n\nIt allows readers to assess the external validity of the trial results, which is the extent to which the results can be generalised to other settings and populations. For example, if the trial participants are very different from the target population in terms of age, sex, or other factors that may affect the outcome, then the results may not be applicable to the target population (Altman, 1990).\nIt allows researchers to check the internal validity of the trial results, which is the extent to which the results are free from bias and confounding. Bias is a systematic error that leads to a deviation from the true effect of the intervention or exposure. Confounding is a situation where a third variable is associated with both the intervention or exposure and the outcome, and may distort the true effect of the intervention or exposure. For example, if the treatment groups are not balanced in terms of baseline characteristics that may influence the outcome, then there may be a bias or a confounding effect that needs to be adjusted for in the statistical analysis (Matthews, 2006).\nIt allows researchers to explore the heterogeneity of the treatment effect, which is the variation in the effect across different subgroups of participants. Heterogeneity can be due to biological, clinical, or methodological factors that may modify or interact with the intervention or exposure. For example, if the treatment effect differs by age, sex, or disease severity, then it may be useful to perform subgroup analyses to identify which subgroups benefit more or less from the intervention or exposure (Matthews, 2006).\n\nTo evaluate baseline characteristics, researchers need to use appropriate methods of summarisation, comparison, and adjustment, depending on the type and distribution of the variables. Summarisation involves describing the distribution and characteristics of the variables using descriptive statistics such as means, standard deviations, medians, interquartile ranges, counts, and percentages. Comparison involves testing whether there are significant differences or associations between the groups or variables using statistical tests such as t-tests, chi-squared tests, Fisher’s exact tests, and rank sum tests. Adjustment involves controlling for the potential confounding factors using methods such as stratification, matching, covariate adjustment, or weighting (Altman, 1990).",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 5</b>: <code>tableone</code> and its Basic Statistics"
    ]
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/index.html#types-of-variables",
    "href": "materials/1-workshop1/5-tableone/index.html#types-of-variables",
    "title": "tableone and its Basic Statistics",
    "section": "Types of variables",
    "text": "Types of variables\n\nContinuous\nContinuous variables are variables that can take any numeric value within a range, such as age, weight, height, blood pressure, etc. They are usually measured or calculated using a scale or a device.\n\n\nCategorical\nCategorical variables are variables that can take only a limited number of values, such as sex, race, diagnosis, treatment group, etc. They are usually assigned or observed based on some criteria or classification.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 5</b>: <code>tableone</code> and its Basic Statistics"
    ]
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/index.html#descriptive-statistics",
    "href": "materials/1-workshop1/5-tableone/index.html#descriptive-statistics",
    "title": "tableone and its Basic Statistics",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\nDescriptive statistics are numerical summaries that describe the distribution and characteristics of a variable. They include measures of central tendency (such as mean, median, mode), measures of variability (such as standard deviation, interquartile range, range), and measures of frequency (such as count, percentage, proportion).\n\nMean\nThe mean is the sum of all the values in a data set divided by the number of values. The formula is:\n\\[\\bar{x} = \\frac{\\sum x}{n}\\]\nwhere \\(\\bar{x}\\) is the mean, \\(\\sum x\\) is the sum of all the values, and \\(n\\) is the number of values.\n\n\nMedian\nThe median is the middle value in an ordered data set. To find the median, arrange the values from smallest to largest and then locate the middle one. If there are an even number of values, the median is the average of the two middle ones. The formula is:\n\\[\\tilde{x} = \\begin{cases}\nx_{(n+1)/2}, & \\text{if } n \\text{ is odd} \\\\\n\\frac{x_{n/2} + x_{(n/2)+1}}{2}, & \\text{if } n \\text{ is even}\n\\end{cases}\\]\nwhere\n\n\\(\\tilde{x}\\) is the median,\n\\(x_i\\) is the \\(i\\)th value in the ordered data set, and\n\\(n\\) is the number of values.\n\n\n\nMode\nThe mode is the most frequent value in a data set. There can be more than one mode if there are multiple values with the same frequency. There is no formula for the mode, but it can be found by counting how many times each value or category occurs and then selecting the one(s) with the highest frequency.\n\n\nStandard deviation\nThe variance is a measure of how much the values vary from the mean while the standard deviation is a measure of how spread out the values are from the mean. It is calculated by taking the square root of the variance. The formula is:\n\\[s = \\sqrt{\\frac{\\sum (x - \\bar{x})^2}{n-1}}\\]\nwhere\n\n\\(s\\) is the standard deviation,\n\\(x\\) is a value in the data set,\n\\(\\bar{x}\\) is the mean, and\n\\(n\\) is the number of values.\n\n\n\nCount\nThe count is the number of times a value or a category occurs in a data set. There is no formula for the count, but it can be found by counting how many times each value or category appears in the data set.\n\n\nPercentage\nThe proportion is the ratio of a part to the whole expressed as a fraction between 0 and 1 whereas the percentage is the ratio of a part to the whole expressed as a fraction of 100. The formula is:\n\\[p = \\frac{n}{N} \\times 100\\]\nwhere\n\n\\(p\\) is the percentage,\n\\(n\\) is the count of a value or a category, and\n\\(N\\) is the total count of all values or categories.\n\nThese are some of the basic formulas for descriptive statistics. You can find more information and examples on this website.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 5</b>: <code>tableone</code> and its Basic Statistics"
    ]
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/index.html#types-of-statistical-tests",
    "href": "materials/1-workshop1/5-tableone/index.html#types-of-statistical-tests",
    "title": "tableone and its Basic Statistics",
    "section": "Types of statistical tests",
    "text": "Types of statistical tests\nOne way to expand the sentence on baseline characteristics of patients in biomedical research papers is:\nStatistical tests are methods to evaluate whether there is a significant difference or association between two or more groups or variables. They are often used to compare the baseline characteristics of patients in different treatment groups, such as intervention and control groups, in biomedical research papers (Altman, 1990). They usually involve calculating a test statistic, which is a numerical value that summarizes the strength and direction of the relationship between the groups or variables. Then they compare the test statistic with a critical value or a p-value, which are thresholds that indicate the level of significance of the test. The level of significance is the probability of obtaining a test statistic as extreme or more extreme than the observed one, if the null hypothesis of no difference or no association is true. If the test statistic exceeds the critical value or is lower than the p-value, then the null hypothesis can be rejected and the alternative hypothesis of a difference or an association can be accepted. Otherwise, the null hypothesis cannot be rejected and the alternative hypothesis cannot be accepted.\nDepending on the type and distribution of the variables, different statistical tests can be performed. For continuous variables, such as age, weight, height, blood pressure, etc., t-tests (for normally distributed data) or rank sum tests (for non-normally distributed data) can be performed. T-tests compare the means of two groups and assume that the data are normally distributed and have equal variances. Rank sum tests compare the medians of two groups and do not assume any distribution or variance equality (Altman, 1990). For categorical variables, such as sex, race, diagnosis, treatment group, etc., chi-squared tests (for large sample sizes) or Fisher’s exact tests (for small sample sizes) can be performed. Chi-squared tests compare the frequencies or proportions of two or more groups and assume that the expected frequencies are sufficiently large. Fisher’s exact tests compare the frequencies or proportions of two groups and do not assume any frequency requirement (Altman, 1990).\nAnother aspect that can be considered when comparing baseline characteristics is the standardized mean difference (SMD), It is a unitless measure that can be used to compare the magnitude of group differences across different variables. It can also be used to combine results from different studies that measure the same outcome but use different scales (Schulz, Altman, & Moher, 2010).",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 5</b>: <code>tableone</code> and its Basic Statistics"
    ]
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/index.html#tableone-package",
    "href": "materials/1-workshop1/5-tableone/index.html#tableone-package",
    "title": "tableone and its Basic Statistics",
    "section": "tableone package",
    "text": "tableone package\n\nIntroduction\nR package tableone is a package by Yoshida & Victorina (2021) that eases the construction of “Table 1: Baseline demographics and clinical characteristics”. The package can handle both continuous and categorical variables, and provide descriptive statistics such as means, standard deviations, medians, interquartile ranges, counts, and percentages. It can also perform statistical tests to compare groups, such as t-tests, chi-squared tests, Fisher’s exact tests, and rank sum tests. It can also calculate standardized mean differences to measure the effect size of group differences. tableone can handle weighted data using the survey package, which allows researchers to account for complex sampling designs and adjust for confounding factors. tableone has a simple and flexible syntax, and can produce nice-looking tables using the kableone function.\n\n\nHow tableone works\n\nTo use tableone, you need to install it from CRAN or GitHub (Yoshida & Victorina, 2021).\nYou need to load the package using library(tableone).\nYou need to specify the variables you want to summarize using the vars argument. You can also specify which variables are categorical using the factorVars argument.\nYou need to provide the data frame that contains the variables using the data argument. You can also provide a grouping variable using the strata argument.\nYou need to create a tableone object using the Createtableone`` function. This object contains all the summary statistics and test results for each variable and group.\nYou can print or export the tableone object using the print or kableone functions.\n\n\n\nOther packages for creating tables\nBesides tableone, there are many other R packages that can be used to create tables in various output formats, such as PDF, HTML and Word. Some of them are: flextable (Gohel & Skintzos, 2023) and huxtable (Hugh-Jones, 2022).",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 5</b>: <code>tableone</code> and its Basic Statistics"
    ]
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/index.html#references",
    "href": "materials/1-workshop1/5-tableone/index.html#references",
    "title": "tableone and its Basic Statistics",
    "section": "References",
    "text": "References\nAltman, D. G. (1990). Practical statistics for medical research. Chapman & Hall/CRC.\nMatthews, J. N. (2006). Introduction to randomized controlled clinical trials. CRC Press.\nSchulz, K. F., Altman, D. G., & Moher, D.; CONSORT Group. (2010). CONSORT 2010 statement: updated guidelines for reporting parallel group randomised trials. BMJ, 340, c332.\nTherneau, T. M., & Grambsch, P. M. (2000). Modeling survival data: Extending the Cox model. Springer.\nYoshida, K., & Victorina, L. K. (2021). tableone: An R package for creating ‘Table 1’. R package version 0.12.0.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 5</b>: <code>tableone</code> and its Basic Statistics"
    ]
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/index.html",
    "href": "materials/1-workshop1/4-tidyverse-201/index.html",
    "title": "More data wrangling and data visualization with the\ntidyverse",
    "section": "",
    "text": "Make slides full screen",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 4</b>: Data wrangling and more data visualization"
    ]
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/index.html#slides",
    "href": "materials/1-workshop1/4-tidyverse-201/index.html#slides",
    "title": "More data wrangling and data visualization with the\ntidyverse",
    "section": "",
    "text": "Make slides full screen",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 4</b>: Data wrangling and more data visualization"
    ]
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/index.html#exercise-data-wrangling-2",
    "href": "materials/1-workshop1/4-tidyverse-201/index.html#exercise-data-wrangling-2",
    "title": "More data wrangling and data visualization with the\ntidyverse",
    "section": "Exercise: Data wrangling 2",
    "text": "Exercise: Data wrangling 2\nIn this exercise, we will continue with basic data manipulations, now moving on to grouping and summarizing, making data tables wider or longer, and joining data tables.\nWe will be using the R package, tidyverse for the data manipulation functions %&gt;%, group_by(), summarize(), pivot_wider(), pivot_longer(), and join functions such as left_join()\nPaste the following into the top code chunk of your qmd file.\nDownload the data files from the dataset page and place all 5 files into this directory.\nPaste the following code chunk into a new qmd file in this project:\nlibrary(tidyverse)\n\ntable_01 &lt;- read_csv(\"01_participant_metadata_UKZN_workshop_2023.csv\")\n\ntable_02 &lt;- read_csv(\"02_visit_clinical_measurements_UKZN_workshop_2023.csv\")\n\nAnalyzing subsets\nIn many data analysis settings, we want to break a dataset into subsets and then perform some summary calculation on each subset. The simplest example is counting, which we have done previously with the count() function.\n\n```{r}\ntable_01 %&gt;%\n  count(arm)\n```\n\n# A tibble: 2 × 2\n  arm           n\n  &lt;chr&gt;     &lt;int&gt;\n1 placebo      23\n2 treatment    21\n\n\nThis function subdivides the penguins dataset into subsets for each species and then calculates the number n for each subset, which is the number of observations in each subset.\nThe function count() here does all the work for us, but what if instead of counting we wanted to calculate the mean weight of the penguins for each species, or calculate the mean weight and count at the same time? We need a general framework that allows us to do these kinds of calculations with maximum flexibility.\nThe tidyverse approach is to first group a dataset with group_by() and then to calculate grouped summaries with summarize().\n\n\nGrouping\nLet’s first consider just grouping. If we look at the raw R output of just the penguins table or the penguins table after running it through group_by(arm), we see that the table is the same, except in the second case there is a line # Groups: arm [2] which indicates that the table is grouped by arm and there are two groups. (Here, we need to pipe the tables into the print() function to see the raw R output instead of a formatted table that would hide the grouping information.)\n\ntable_01 %&gt;%\n  print()\n\n# A tibble: 44 × 6\n   pid    arm       smoker       age education                     sex  \n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                         &lt;lgl&gt;\n 1 pid_01 placebo   non-smoker    26 grade 10-12, matriculated     FALSE\n 2 pid_02 placebo   smoker        33 grade 10-12, matriculated     FALSE\n 3 pid_03 placebo   smoker        30 post-secondary                FALSE\n 4 pid_04 placebo   non-smoker    34 grade 10-12, not matriculated FALSE\n 5 pid_05 treatment non-smoker    29 grade 10-12, matriculated     FALSE\n 6 pid_06 placebo   smoker        34 post-secondary                FALSE\n 7 pid_07 placebo   non-smoker    31 grade 10-12, not matriculated FALSE\n 8 pid_08 placebo   smoker        30 grade 10-12, not matriculated FALSE\n 9 pid_09 treatment non-smoker    35 grade 10-12, not matriculated FALSE\n10 pid_10 treatment non-smoker    32 less than grade 9             FALSE\n# ℹ 34 more rows\n\n\n\ntable_01 %&gt;%\n  group_by(arm) %&gt;%\n  print()\n\n# A tibble: 44 × 6\n# Groups:   arm [2]\n   pid    arm       smoker       age education                     sex  \n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                         &lt;lgl&gt;\n 1 pid_01 placebo   non-smoker    26 grade 10-12, matriculated     FALSE\n 2 pid_02 placebo   smoker        33 grade 10-12, matriculated     FALSE\n 3 pid_03 placebo   smoker        30 post-secondary                FALSE\n 4 pid_04 placebo   non-smoker    34 grade 10-12, not matriculated FALSE\n 5 pid_05 treatment non-smoker    29 grade 10-12, matriculated     FALSE\n 6 pid_06 placebo   smoker        34 post-secondary                FALSE\n 7 pid_07 placebo   non-smoker    31 grade 10-12, not matriculated FALSE\n 8 pid_08 placebo   smoker        30 grade 10-12, not matriculated FALSE\n 9 pid_09 treatment non-smoker    35 grade 10-12, not matriculated FALSE\n10 pid_10 treatment non-smoker    32 less than grade 9             FALSE\n# ℹ 34 more rows\n\n\nWe can also group by multiple data columns at once, and we can undo any grouping with ungroup().\n\ntable_01 %&gt;%\n  group_by(arm, education) %&gt;%\n  print()\n\n# A tibble: 44 × 6\n# Groups:   arm, education [8]\n   pid    arm       smoker       age education                     sex  \n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                         &lt;lgl&gt;\n 1 pid_01 placebo   non-smoker    26 grade 10-12, matriculated     FALSE\n 2 pid_02 placebo   smoker        33 grade 10-12, matriculated     FALSE\n 3 pid_03 placebo   smoker        30 post-secondary                FALSE\n 4 pid_04 placebo   non-smoker    34 grade 10-12, not matriculated FALSE\n 5 pid_05 treatment non-smoker    29 grade 10-12, matriculated     FALSE\n 6 pid_06 placebo   smoker        34 post-secondary                FALSE\n 7 pid_07 placebo   non-smoker    31 grade 10-12, not matriculated FALSE\n 8 pid_08 placebo   smoker        30 grade 10-12, not matriculated FALSE\n 9 pid_09 treatment non-smoker    35 grade 10-12, not matriculated FALSE\n10 pid_10 treatment non-smoker    32 less than grade 9             FALSE\n# ℹ 34 more rows\n\n\n\ntable_01 %&gt;%\n  group_by(arm, education) %&gt;%\n  ungroup() %&gt;%\n  print()\n\n# A tibble: 44 × 6\n   pid    arm       smoker       age education                     sex  \n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                         &lt;lgl&gt;\n 1 pid_01 placebo   non-smoker    26 grade 10-12, matriculated     FALSE\n 2 pid_02 placebo   smoker        33 grade 10-12, matriculated     FALSE\n 3 pid_03 placebo   smoker        30 post-secondary                FALSE\n 4 pid_04 placebo   non-smoker    34 grade 10-12, not matriculated FALSE\n 5 pid_05 treatment non-smoker    29 grade 10-12, matriculated     FALSE\n 6 pid_06 placebo   smoker        34 post-secondary                FALSE\n 7 pid_07 placebo   non-smoker    31 grade 10-12, not matriculated FALSE\n 8 pid_08 placebo   smoker        30 grade 10-12, not matriculated FALSE\n 9 pid_09 treatment non-smoker    35 grade 10-12, not matriculated FALSE\n10 pid_10 treatment non-smoker    32 less than grade 9             FALSE\n# ℹ 34 more rows\n\n\nNow try this yourself. Group the table_01 dataset by education_level and smoker.\n\ntable_01 %&gt;%\n  ___ %&gt;%\n  print()\n\nAnswer for yourself How many distinct groups did this produce?\nNow undo the previous grouping.\n\n```{r}\n# build all the code for this exercise\n```\n\nAlso verify what the output looks like when you omit the print() function at the end.\n\n\nPerforming summaries\nOnce we have set up a grouping for a data table, we can then calculate summary data with the summarize() function. This function works similarly to mutate(), in that we provide it with statements of the form &lt;new column name&gt; = &lt;computation&gt;, where &lt;new column name&gt; stands for the name of the new column that is being created and &lt;computation&gt; stands for the computation that is used to generate the values in the new column.\nAs an example, using table_02 we want to calculate the median ph of participants, by arm, we could write summarise(median_ph = median(ph)), and this would create a new column called median_ph\nTry this out. First group by arm and then make the new column:\n\ntable_02 %&gt;%\n  group_by(____) %&gt;%\n  summarise(___)\n\nNow see what it looks like if you instead group by timepoint\n\ntable_02 %&gt;%\n  group_by(____) %&gt;%\n  summarise(___)\n\nNow try grouping by both timepoint and arm\n\ntable_02 %&gt;%\n  group_by(__, __) %&gt;%\n  summarise(___)\n\nWe can perform multiple summaries at once by adding more statements inside the summarise() function, separated by a ,. To try this out, calculate the median nugent in addition to the median ph\n\ntable_02 %&gt;%\n  group_by(____) %&gt;%\n  summarise(___, ___)\n\nWhen performing summaries, we often want to know how many observations there are in each group (i.e., we want to count). We can do this with the function n(), which inside summarise() gives us the group size. So, we can count by adding a statement such as count = n() inside `summarise(). Try this out.\n\ntable_02 %&gt;%\n  group_by(____) %&gt;%\n  summarise(___, ___)\n\n\n\nMaking tables wider or longer\nFor efficient data processing, we usually want tables in long form, where each columns is one variable and each row is one observation. However, in some applications, for example when making a table easier to read for humans, a wide format can be preferred. In a wide format, some variables are displayed as column names, and other variables are distributed over multiple columns.\nFirst, make a summary table that shows median ph by arm and time_point, just like you did above, and save it to a variable ph_summary_long\n\nph_summary_long &lt;- table_02 %&gt;%\n  group_by(___) %&gt;%\n  summarise(___)\n\nNow, try using pivot_wider() to make a column for each arm. Remember, use ?pivot_wider if you want help, and try asking google or chatGPT if you get stuck.\n\nph_summary_long %&gt;%\n  pivot_wider(____)\n\nWhat if you wanted to instead make a column for each time point, and have the arms be different rows?\n\nph_summary_long %&gt;%\n  pivot_wider(____)\n\n\n\nCombining datasets with joins\nFinally, we sometimes encounter the situation where we have two data sets that contain different pieces of information about the same subjects or objects, and we need to merge these tables for further analysis. In this situation, we need to perform a join, and there are multiple different types of joins available: left_join(), right_join(), inner_join(), full_join(). These joins all differ in how they handle cases where an observation is present in only one of the two tables but missing in the other.\nOur instructional dataset has no missing values, so all types of joins are actually equivalent. Try to join table_01 and table_02 using left_join()\n\n```{r}\n# join table_01 and table_02\n```",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 4</b>: Data wrangling and more data visualization"
    ]
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/index.html",
    "href": "materials/1-workshop1/3-tidyverse-101/index.html",
    "title": "Intro to data visualization and data wrangling with the\ntidyverse",
    "section": "",
    "text": "Learn the elements of scientific data visualization and how to use them with ggplot2",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 3</b>: Intro to data visualization and data wrangling with the <code>tidyverse</code>"
    ]
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/index.html#slides",
    "href": "materials/1-workshop1/3-tidyverse-101/index.html#slides",
    "title": "Intro to data visualization and data wrangling with the\ntidyverse",
    "section": "Slides",
    "text": "Slides\nMake slides full screen",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 3</b>: Intro to data visualization and data wrangling with the <code>tidyverse</code>"
    ]
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/index.html#data-visualization-activity",
    "href": "materials/1-workshop1/3-tidyverse-101/index.html#data-visualization-activity",
    "title": "Intro to data visualization and data wrangling with the\ntidyverse",
    "section": "Data Visualization Activity",
    "text": "Data Visualization Activity\nreproduced from a Schmidt Science Fellowship activity, thanks to Fatima Hussain\n\nPatterns are the essence of data exploration and our eyes’ ability to pick them out is integral to data understanding. Much of the data we work with, however, do not have a natural form and we need to make decisions about how they are to be represented. Try different ways to visualize the datasets so meaningful patterns may be found.\n\n\nGenetic profiles of cancer\nThese datasets contains 10 cancer samples. Table 1 describes the mutational status for a set of genes (A-E) and whether a mutation if absent (0) or present (1). Table 2 summarizes the expression levels of those genes, ranging from no expression (0) to high expression (3).\n\n\n\nTable 1: Mutational status for a set of genes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample -&gt;\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nGene A\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nGene B\n0\n0\n0\n0\n1\n1\n1\n0\n1\n1\n\n\nGene C\n0\n0\n1\n0\n0\n0\n1\n1\n1\n1\n\n\nGene D\n1\n1\n0\n0\n1\n1\n0\n0\n0\n0\n\n\nGene E\n0\n1\n1\n0\n1\n0\n0\n0\n1\n0\n\n\n\n\n\n\n\n\n\nTable 2: Expression levels for a set of genes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample -&gt;\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nGene A\n2\n1\n1\n2\n2\n0\n2\n1\n1\n2\n\n\nGene B\n1\n1\n2\n1\n0\n0\n0\n2\n0\n0\n\n\nGene C\n1\n1\n3\n1\n2\n2\n3\n0\n3\n0\n\n\nGene D\n0\n0\n2\n1\n3\n3\n2\n1\n1\n1\n\n\nGene E\n1\n3\n3\n1\n3\n1\n2\n1\n3\n2\n\n\n\n\n\n\n\n\n          1. Think about the problem on your own for 5 minutes.\n          2. In your groups, discuss and create different visualizations to highlight underlying patterns\n          3. Summarize the group’s approach\n          4. Elect/volunteer a spokesperson to present the solution\n\n\nConsider the following concepts when creating your visualizations\n\n\n\n\nPatterns\nPatterns are the essence of data exploration. What kinds of representation will produce the most meaningful insights?\n   \n\n\nEncodings\nSome visual estimations are easier to make than others. How might you use encodings that are less accurate but otherwise better at conveying overall trends?\n  \n\n\n\n\nColor\nColor is a powerful encoding that presents several challenges. Have you chosen a color scale that is optimal for that data type?\n   \n\n\nSalience and Relevance\nPop-out effects enable quick recognition. Are the most noticeable elements of your visualizations also the most relevant?",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 3</b>: Intro to data visualization and data wrangling with the <code>tidyverse</code>"
    ]
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/index.html#coding-exercise-3.1",
    "href": "materials/1-workshop1/3-tidyverse-101/index.html#coding-exercise-3.1",
    "title": "Intro to data visualization and data wrangling with the\ntidyverse",
    "section": "Coding exercise 3.1",
    "text": "Coding exercise 3.1\nFor this exercise we’ll be using the instructional dataset. Download the dataset here.\nIn this worksheet, we will discuss a core concept of ggplot, the mapping of data values onto aesthetics.\nWe will be using the R package tidyverse, which includes ggplot() and related functions.\nCopy the following code chunk into your quarto document. Notice the error in the read_csv() line - it wants you to supply the file name to read. Fix the error!\n\nlibrary(tidyverse) # load the tidyverse library\n\n# we want to use the data in the visit_clinical_measurements file\nclinical_measurements &lt;- read_csv() # read in your data \n\nError in read_csv(): argument \"file\" is missing, with no default\n\n#then show the first few rows\nhead(clinical_measurements)\n\nError in eval(expr, envir, enclos): object 'clinical_measurements' not found\n\n\n\nBasic use of ggplot\nIn the most basic use of ggplot, we call the ggplot() function with a dataset and an aesthetic mapping (created with aes()), and then we add a geom, such as geom_line() to draw lines or geom_point() to draw points.\nTry this for yourself. Map the column ph onto the x axis and the column crp_blood onto the y axis, and use geom_line() to display the data.\nWhenever you see ___ in the code below, that means you should swap it in with your own code.\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\nTry again. Now use geom_point() instead of geom_line().\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\nAnd now swap which column you map to x and which to y.\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\n\n\nMore complex geoms\nYou can use other geoms to make different types of plots. For example, geom_boxplot() will make boxplots. For boxplots, we frequently want categorical data on the x or y axis. For example, we might want a separate boxplot for each month. Try this out. Put nugent_score on the x axis, ph on the y axis, and use geom_boxplot().\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\nNow try swapping the x and y axis geom_jitter()\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\nNow try swapping the x and y axis\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\n\n\nAdding color\nTry again with geom_jitter(), this time using ph as the location along the y axis and arm for the color. Remember to check the ggplot cheat sheet, or type ?geom_jitter() in the console to and look at the “Aesthetics” section if you get stuck.\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\n(Hint: Try adding size = 3 as a parameter to the geom_jitter() to create larger points.)\n\n\nUsing aesthetics as parameters\nMany of the aesthetics (such as color, fill, and also size to change line size or point thickness) can be used as parameters inside a geom rather than inside an aes() statement. The difference is that when you use an aesthetic as a parameter, you specify a specific value, such as color = \"blue\", rather than a mapping, such as aes(color = arm). Notice the difference: Inside the aes() function, we don’t actually specify the specific color values, ggplot does that for us. We only say that we want the data values of the arm column to correspond to different colors. (We will learn later how to tell ggplot to use specific colors in this mapping.)\nTry this with the boxplot example from the previous section. Map arm onto the fill aesthetic but set the color of the lines to \"navyblue\".\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\nNow do the reverse. Map arm onto the line colors of the boxplot, but will the box with the color \"navyblue\".\n\nggplot(clinical_measurements, aes(x = ___, y = ___)) +\n  ___()\n\nGreat, that’s all for now! If you are done, but a green sticky note on your laptop!",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 3</b>: Intro to data visualization and data wrangling with the <code>tidyverse</code>"
    ]
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/index.html",
    "href": "materials/1-workshop1/2-intro-to-r/index.html",
    "title": "Intro to R, RStudio, and Quarto",
    "section": "",
    "text": "Let’s learn how to use RStudio to run R code and render documents with Quarto.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 2</b>: Intro to R, Rstudio, and Quarto"
    ]
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/index.html#slides-intro-to-rstudio-quarto-and-r",
    "href": "materials/1-workshop1/2-intro-to-r/index.html#slides-intro-to-rstudio-quarto-and-r",
    "title": "Intro to R, RStudio, and Quarto",
    "section": "Slides: Intro to RStudio, Quarto, and R",
    "text": "Slides: Intro to RStudio, Quarto, and R\nMake slides full screen",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 2</b>: Intro to R, Rstudio, and Quarto"
    ]
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/index.html#worksheet-introduction-to-r-and-quarto",
    "href": "materials/1-workshop1/2-intro-to-r/index.html#worksheet-introduction-to-r-and-quarto",
    "title": "Intro to R, RStudio, and Quarto",
    "section": "Worksheet: Introduction to R and Quarto",
    "text": "Worksheet: Introduction to R and Quarto\n\n1. Operators and Functions in R\nIn order to get familiar with the R language, we shall start by using simple operators and functions that are intuitive to us in our everyday lives.\n\n1.1 Arithmetic operators\n\n+                addition\n-                subtraction\n*                multiplication\n/                division\n^ or **          power\n\nx %*% y          matrix multiplication c(5, 3) %*% c(2, 4) == 22\nx %% y           modulo (x mod y) 5 %% 2 == 1\nx %/% y          whole number division: 5 %/% 2 == 2\n\nNote that while the first half is self-explanatory, the second is more specific to programming and/or R.\nLet’s try a few examples in R. First, always have a code chunk at the top that loads the libraries you need:\n\n```{r}\nlibrary(tidyverse)\n```\n\nThen, copy the following code chunk into your quarto document.\n\n99 + 1 + -1\n\n64 / 4\n\n64 / (2+2)\n\n64 / 2 + 2\n\nNotice the difference in the last two commands and it’s effect on the output.\n\n\n1.2 Logical operators and functions\n\n&lt;                less than\n&lt;=               less than or equal to\n&gt;                greater than\n&gt;=               greater than or equal to\n==               equal\n!=               not equal\n!x               not x (negation)\nx | y            x OR y\nx & y            x AND y\nxor(x, y)        exclusive OR (either in x or y, but not in both)\nisTRUE(x)        truth test for x\n\nFew examples for you to copy in your quarto document or directly on the console,\n\n99 &lt; 1\n\n!(99 &lt; 1)\n\n64 == 8*8\n\n(3 &gt; 2) | (4 &gt; 5)\n\n\n\n1.3 Numeric functions\n\nabs(x)             absolute value\nsqrt(x)            square root\nceiling(x)         round up: ceiling(3.475) is 4\nfloor(x)           round down: floor(3.475) is 3\nround(x, digits=n) round: round(3.475, digits=2) is 3.48\ncos(x), sin(x), tan(x), acos(x), cosh(x), acosh(x) etc.\nlog(x)             natural logarithm\nlog(10, base = n)  base n logarithm\nlog2(x)            base 2 logarithm\nlog10(x)           base 10 logarithm\nexp(x)             exponential function: e^x\n\nFew examples for you to copy in your quarto document or directly on the console,\n\nabs(99)\n\nabs(-99)\n\nsqrt(64)\n\nfloor(6.789)\n\n\n\n1.4 Statistical functions\nBelow is a list of statistical functions. These functions can have the argument na.rm, which is set to FALSE by default. This will let you deal with missing values (na = not available). If set to false, these are not removed (rm = remove).\n\nmean(x, na.rm = FALSE)  mean\nsd(x)                   standard deviation\nvar(x)                  variance\n\nmedian(x)               median\nquantile(x, probs)      quantile of x.  probs: vector of probabilities\n\nsum(x)                  sum\nmin(x)                  minimal value of x (x_min)\nmax(x)                  xaximal value of x (x_max)\nrange(x)                x_min und x_max\n\n# if center  = TRUE: subtract mean\n# if scale   = TRUE: divide by sd\nscale(x, center = TRUE, scale = TRUE)   center and standardize\n\n# weighted sampling with argument prob:\nsample(x, size, replace = FALSE, prob)  sampling with or without replacement. prob: vector of weights\n\nTo get help with a function, type ?function_name in the console.\nFor example, try typing ?mean into the console. Check out this guide on how to read the help page.\n\n\n1.5 Other useful functions\n\nc()                    combine: used to create a vector\nseq(from, to, by)      generates a sequence\n:                      colon operator: generates a 'regular' sequence in increments of 1\nrep(x, times, each)    repeats x\n                          times: sequence is repeated n times\n                          each: each element is repeated n times\n\nhead(x, n = 6)         first 6 elements of x\ntail(x, n = 6)         last 6 elements of x\n\nFew examples for you to copy in your quarto document or directly on the console,\n\nc(1, 2, 3, 4, 5, 6)\n\nmean(c(1, 2, 3, 4, 5, 6))\n\nmean(c(1, NA, 3, 4, 5, 6), na.rm = TRUE)\n\nmean(c(1, NA, 3, 4, 5, 6), na.rm = FALSE)\n\nsum(c(1, 2, 3, 4, 5, 6))\n\nseq(from = 1, to = 6, by = 1)\n\nseq(from = 1, to = 6, by = 2)\n\nrep(1:6, times = 2, each = 2)\n\nReady to put these new skills to use? Here’s an exercise.\n\n\n\nExercise 1\nWrite R commands to calculate the following:\n\nThe sum of your birth day, month and year\n250 divided by the product of 4 and 5\nHalf of the sum of 37.5, 51.3, and 101.7\n\\(\\frac{1}{3} * { (1+3+5+7+2) \\over (3+5+4)}\\)\n\\(\\sqrt[3]{8}\\)\n\\(\\sin\\pi\\), \\(\\cos\\pi\\), \\(\\tan\\pi\\)\nCalculate the 10-based logarithm of 100, and multiply the result with the cosine of pi. Hint: see ?log and ?pi.\nCalculate the mean, sd and range of the vector [1, 3, 4, 7, 11, 16]\nGenerate the following output: 4 4 4 5 5 5 6 6 6 7 7 7\nGenerate the following output: 2 4 6 8 10 12\n\n\n\nChallenge Exercise 1\nWrite R commands to evaluate the following:\n\nYou have 83 chocolates in a bag. You would like to divide them into smaller bags of 8.\n1.1 How many small bags will you need?\n1.2 After the bags are filled, how many extra chocolates will you have remaining?\nYou are planning a research study with the following eligibility criteria:\n\n\nStudy participant should be between 18-25 years old (variable: age)\n\nHemoglobin should be over 10 (variable: hgb)\n\nStudy participant should not weigh over 50 (variable: wgt)\n\nStudy location should be X or Y (variable: loc)\n\n\nI have the following observations from a clinical parameter – x &lt;- c(23.1924, 21.4545, 24.6778) However, I would prefer to limit the values to 1 decimal point. How would you do that?\nLook up the rnorm() function in the Help Viewer. What arguments does this function take? Any default values?\nTry to nest a function within another\n\nAssuming you get all the information in forms of the above variables, write an R command to determine eligibility.\n\n\n2. Objects and Vectors in R\n\n2.1 Objects / Variables\nSo far, we have been happy to run functions and read the results on the screen. What if you’d like to read results later? You will need to save them by creating Objects.\nCopy the following code chunk into your quarto document\n\nnumber1 &lt;- 9\nsqrt(number1)\n\n[1] 3\n\nnumber2 &lt;- 10\n\nnumber1 * number2\n\n[1] 90\n\n\nWhat do you think happens when you use the same object name to another value? Try it!\n\nnewnumber &lt;- 100\nnewnumber &lt;- 150\n\n#What does R pick?\nnewnumber\n\nObject/Variable names\nYou can choose almost any name you like for an object, as long as the name does not begin with a number or a special character like +, -, *, /, ^, !, @, or &.\n\n# good variable names\nx_mean\nx_sd\n\nnum_people\nage\n\n# not so good\np\na\n\n# bad variable names\nx.mean\nsd.of-x\n\n\n\n2.2 Vectors\nVectors are the fundamental data type in R - all other data types are composed of vectors. These can subdivided into:\nnumeric vectors: a further subdivision is into integer (whole numbers) and double (floating point numbers).\ncharacter vectors: these consist of characters strings and are surrounded by quotes, either single ' or double \", e.g. 'word' or \"word\".\nlogical vectors: these can take three values: TRUE, FALSE or NA.\nVectors consist of elements of the same type, i.e., we cannot combine logical and character elements in a vector. Vectors have three properties:\nType: typeof(): what is it? Length: length(): how many elements? Attribute: attributes(): additional metadata\nVectors are created using the c() function or by using special function, such as seq() or rep().\n\n# Numeric vectors\nnum_vec &lt;- c(2.1,2.2,2.3,2.4,2.5)\nlength(num_vec)\n\n[1] 5\n\n# Character vectors\nchar_vec &lt;- c(\"hello\",\"hi\")\nlength(char_vec)\n\n[1] 2\n\n# Logical vectors\nlog_vec &lt;- c(TRUE,FALSE,TRUE)\ntypeof(log_vec)\n\n[1] \"logical\"\n\n\nSome fun with vectors\n\nnum_vec[1] # what's the first element of num_vec?\n\nnum_vec[4] # 4th?\n\nnum_vec[-1] # what does -1 do?\n\nnum_vec[1:3]\n\nnum_vec[-c(1, 3)]\n\nTry this on your own with the character vectors. What differences and similarities do you notice?\nExercise time!\n\n\n\nExercise 2\n\nCalculate the mean and standard deviation of a numeric vector of the first 6 multiples of 5.\nWhat happens if you attempt at arithmetic function on a numeric vector? (Example: num_vec + 1)\nWhat happens if you attempt the same on a character vector?\nExplore the str_length() function on a character vector.\nWhat happens if you try to make a vector consisting of different data types?\nMetadata hygiene: Give examples of good and bad ways to make variable names for the following columns in the Metadata spreadsheet –\n\nParticipant ID\nDate of Sample Collection\nType of Sample\nMean of past 3 weights\nNumber of people\n\n\n\n\nChallenge Exercise 2\n\nCalculate the mean of the sum of the first 6 even numbers - using one R command.\nfirst_name and last_name are two separate variables. Make one variable with both together, calling it full_name. Hint: it is a function\n\n\n\n3. Dataframes in R\nFor data analysis and statistics, data frames are important objects of data representation. A data frame is a two dimensional structure with rows and columns, like a table. You can think of it as a collection of vectors. Let us try making one.\n\n# creating a vector with an ID \nid &lt;- c(1, 2, 3)\n\n# creating another vector with names\nname &lt;- c(\"PersonA\", \"PersonB\", \"PersonC\")\n\n# creating another vector with year of birth \nyear_of_birth &lt;- c(1990, 1995, 2000)\n\n# creating another vector with favourite colour\nfav_colour &lt;- c(\"red\", \"green\", \"yellow\")\n\n#Make a dataframe from the above\n# note, make sure you've added the tidyverse library\nlibrary(tidyverse)\ndf &lt;- tibble(id, name, year_of_birth, fav_colour)\n\ndf\n\n# A tibble: 3 × 4\n     id name    year_of_birth fav_colour\n  &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;     \n1     1 PersonA          1990 red       \n2     2 PersonB          1995 green     \n3     3 PersonC          2000 yellow    \n\n\nYou just made a dataframe!\nNow, normally you will be importing one and using it to get useful information. You can extract useful information like number of rows, columns, details of each, summary of values, etc\nHere are a few helpful functions -\n\nattributes(df)\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n[1] 1 2 3\n\n$names\n[1] \"id\"            \"name\"          \"year_of_birth\" \"fav_colour\"   \n\nrownames(df)\n\n[1] \"1\" \"2\" \"3\"\n\ncolnames(df)\n\n[1] \"id\"            \"name\"          \"year_of_birth\" \"fav_colour\"   \n\n# We didn't assign any yet! Let us try to assign column names\ncolnames(df) &lt;- c(\"ID\",\"name\",\"year_of_birth\",\"favourite_colour\")\ncolnames(df)\n\n[1] \"ID\"               \"name\"             \"year_of_birth\"    \"favourite_colour\"\n\n## Note: When importing an existing data file, depending on your it's structure, you can ask R to import with or without row and column names)\n\nnrow(df)\n\n[1] 3\n\nncol(df)\n\n[1] 4\n\n# Data frame subsetting (I want only a few columns of my interest)\nselect(df, name)\n\n# A tibble: 3 × 1\n  name   \n  &lt;chr&gt;  \n1 PersonA\n2 PersonB\n3 PersonC\n\nselect(df, year_of_birth)\n\n# A tibble: 3 × 1\n  year_of_birth\n          &lt;dbl&gt;\n1          1990\n2          1995\n3          2000\n\n#extract row 2 only\nslice(df, 2)\n\n# A tibble: 1 × 4\n     ID name    year_of_birth favourite_colour\n  &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;           \n1     2 PersonB          1995 green           \n\n#extract rows 1, and 3\nslice(df, c(1,3))\n\n# A tibble: 2 × 4\n     ID name    year_of_birth favourite_colour\n  &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;           \n1     1 PersonA          1990 red             \n2     3 PersonC          2000 yellow          \n\n#extract column 3 only\nselect(df, 3)\n\n# A tibble: 3 × 1\n  year_of_birth\n          &lt;dbl&gt;\n1          1990\n2          1995\n3          2000\n\n#extract rows based on one condition\nfilter(df, year_of_birth &gt; 1995)\n\n# A tibble: 1 × 4\n     ID name    year_of_birth favourite_colour\n  &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;           \n1     3 PersonC          2000 yellow          \n\n#extract rows based on multiple conditions\nfilter(df, year_of_birth &gt; 1995 & favourite_colour==\"red\")\n\n# A tibble: 0 × 4\n# ℹ 4 variables: ID &lt;dbl&gt;, name &lt;chr&gt;, year_of_birth &lt;dbl&gt;,\n#   favourite_colour &lt;chr&gt;\n\nfilter(df, year_of_birth &gt; 1995 | favourite_colour==\"red\")\n\n# A tibble: 2 × 4\n     ID name    year_of_birth favourite_colour\n  &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;           \n1     1 PersonA          1990 red             \n2     3 PersonC          2000 yellow          \n\n#Note the difference in output upon use of different logical operators AND and OR\n\n\n\nExercise 3\n\nMake an expression to get only rows when the name is Person C or the favorite color is green.\nMake a new dataframe with 6 rows and 5 columns (get creative!)\nAdd different combinations of data to be able to use the above functions and compare output.\nTry above functions on your new dataframe and note any interesting observations.\nTry some other functions: str(), head(), view().\n\n\n\nChallenge Exercise 3\n\nCan you think of other simple questions you may need to query your dataset?\nTry to look your query up on google and see if you can find a function that addresses your need! (add ‘in R’ at the end for relevant answers!)\n\nCongratulations! You have successfully reached the end of this exercise. You now possess the most important skill: google your query!",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 2</b>: Intro to R, Rstudio, and Quarto"
    ]
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/index.html#end-of-session-worksheet-introduction-to-r-and-quarto",
    "href": "materials/1-workshop1/2-intro-to-r/index.html#end-of-session-worksheet-introduction-to-r-and-quarto",
    "title": "Intro to R, RStudio, and Quarto",
    "section": "End of session worksheet: Introduction to R and Quarto",
    "text": "End of session worksheet: Introduction to R and Quarto",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 2</b>: Intro to R, Rstudio, and Quarto"
    ]
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/index.html#introduction",
    "href": "materials/1-workshop1/2-intro-to-r/index.html#introduction",
    "title": "Intro to R, RStudio, and Quarto",
    "section": "Introduction",
    "text": "Introduction\nNow that we practiced a bit on mock and small datasets, let us try a ‘real’ one. In this worksheet, we will look at the Ice Breaker Poll from this morning and learn how to perform basic data manipulations, such as filtering data rows that meet certain conditions, choosing data columns, and arranging data in ascending or descending order.\nFirst, download the icebreaker poll file containing the dataset. Download the dataset here and move it to your project directory.\nUse the following command to read in the data from the Ice Breaker poll. ::: {.cell}\nice_breaker_df &lt;- read_csv(\"Ice Breaker Survey (Responses) - Form Responses 1.csv\") # make sure this file name exists in your current directory!\n\nRows: 5 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (9): Timestamp, first_thing_in_morning, vanilla_chocolate, superpower, s...\ndbl (3): num_languages, num_browser_tabs, height_cm\nlgl (1): years_current_country\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n:::\nWe will be using the R package tidyverse for the data manipulation functions %&gt;%, filter(), select(), arrange(), count(), and mutate().\n\nlibrary(tidyverse)\n\n\nThe pipe (%&gt;%, read: “and then”)\nWhen writing complex data analysis pipelines, we frequently use the pipe operator %&gt;% to move data from one analysis step to the next. The pipe is pronounced “and then”, and it takes the data on its left and uses it as the first argument for the function on its right.\nFor example, to see the first few lines of a dataset, we often write head(dataframe). Instead, we can write dataframe %&gt;% head().\nTry this yourself. Write code that displays the first few lines of the ice_breaker_df dataset, using %&gt;% and head():\n\n # build all the code for this exercise\n\nNow get all the column names using the colnames() function on the ice_breaker_df.\n\n # build all the code for this exercise\n\n\n\nChoosing data rows\nThe function filter() allows you to find rows in a dataset that meet one or more specific conditions. The syntax is dataframe %&gt;% filter(condition), where condition is a logical condition. For example, filter(x &gt; 5) would pick all rows for which the value in column x is greater than 5.\nAs an example, the following code picks all survey responses where people prefer chocolate over vanilla ice cream:\n\nice_breaker_df %&gt;%\n  filter(vanilla_chocolate == \"Chocolate\")\n\n# A tibble: 2 × 13\n  Timestamp    first_thing_in_morning num_languages vanilla_chocolate superpower\n  &lt;chr&gt;        &lt;chr&gt;                          &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;     \n1 10/5/2023 1… Go back to sleep                3.25 Chocolate         Shape shi…\n2 10/5/2023 1… Go back to sleep                2.5  Chocolate         Flight    \n# ℹ 8 more variables: social_media &lt;chr&gt;, num_browser_tabs &lt;dbl&gt;,\n#   height_cm &lt;dbl&gt;, procrastinate &lt;chr&gt;, extreme_sport &lt;chr&gt;,\n#   r_experience &lt;chr&gt;, travel_to_workshop &lt;chr&gt;, years_current_country &lt;lgl&gt;\n\n\nCan you tell how many people that is from looking at the size of the tibble?\nNow it’s your turn to try one. Pick all responses where people would like to try Skydiving.\n\nice_breaker_df %&gt;%\n  filter(___)\n\n\nFiltering for multiple conditions\nYou can also state multiple conditions, separated by a comma. For example, filter(x &gt; 5, y &lt; 2) would pick all rows for which the value in the column x is greater than 5 and the value in the column y is less than 2. Note that the conditions are combined via logical and, both need to be satisfied for the row to be picked.\nTo try this out, pick all survey responses where people taller than XXX cm would like to retain their Facebook.\n\n # build all the code for this exercise\n\n\n\n\nChoosing data columns\nThe function select() allows you to pick specific data columns by name. This is frequently useful when a dataset has many more columns than we are interested in at the time. For example, if we are only interested in the responses regarding what people do first thing in the morning, what superpower they would like, and how they procrastinate, we could select just those three columns:\n\nice_breaker_df %&gt;%\n  select(first_thing_in_morning, superpower, procrastinate)\n\n# A tibble: 5 × 3\n  first_thing_in_morning superpower     procrastinate        \n  &lt;chr&gt;                  &lt;chr&gt;          &lt;chr&gt;                \n1 Check text messages    Flight         Watching TV          \n2 Go back to sleep       Shape shifting Eating snacks        \n3 Go back to sleep       Flight         Watching TV          \n4 Turn off the alarm     Flight         Browsing the internet\n5 Turn off the alarm     &lt;NA&gt;           &lt;NA&gt;                 \n\n\nTry this yourself, picking the columns representing responses to how many browser tabs people have open right now and what social media they would like to keep.\n\n # build all the code for this exercise\n\n\n\nChoosing columns for removal\nAnother situation that arises frequently is one where we want to remove specific columns. We can also do this with select(), but now write select(-column) to remove one or more columns.\nTry this. Remove the column num_browser_tabs.\n\n # build all the code for this exercise\n\nAnd now try removing both num_browser_tabs and procrastinate.\n\n\nSorting data\nThe function arrange() allows you to sort data by one or more columns. For example, dataframe %&gt;% arrange(x) would sort the data by increasing values of x, and dataframe %&gt;% arrange(x, y) would sort the data first by x and then, for ties in x, by y.\nAs an example, the following code sorts responses by the person’s height:\n\nice_breaker_df %&gt;%\n  arrange(height_cm)\n\n# A tibble: 5 × 13\n  Timestamp    first_thing_in_morning num_languages vanilla_chocolate superpower\n  &lt;chr&gt;        &lt;chr&gt;                          &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;     \n1 10/5/2023 1… Go back to sleep                2.5  Chocolate         Flight    \n2 10/5/2023 1… Go back to sleep                3.25 Chocolate         Shape shi…\n3 10/5/2023 1… Turn off the alarm              2    Vanilla           Flight    \n4 10/5/2023 1… Check text messages             1    Vanilla           Flight    \n5 10/15/2023 … Turn off the alarm             NA    &lt;NA&gt;              &lt;NA&gt;      \n# ℹ 8 more variables: social_media &lt;chr&gt;, num_browser_tabs &lt;dbl&gt;,\n#   height_cm &lt;dbl&gt;, procrastinate &lt;chr&gt;, extreme_sport &lt;chr&gt;,\n#   r_experience &lt;chr&gt;, travel_to_workshop &lt;chr&gt;, years_current_country &lt;lgl&gt;\n\n\nNow it’s your turn. Sort responses by the number of languages people can speak:\n\n # build all the code for this exercise\n\n\nArranging in descending order\nTo arrange data in descending order, enclose the data column in desc(). For example, dataframe %&gt;% arrange(desc(x)) would sort the data by decreasing values of x. (desc stands for “descending”.)\nTry this out. Sort the responses by height again, this time from largest to smallest:\n\n # build all the code for this exercise\n\n\n\n\nCounting\nWe frequently want to count how many times a particular value or combination of values occurs in a dataset. We do this using the count() function. For example, the following code counts how many of each number we got for the number of languages people can speak.\n\n\n# A tibble: 5 × 2\n  num_languages     n\n          &lt;dbl&gt; &lt;int&gt;\n1          1        1\n2          2        1\n3          2.5      1\n4          3.25     1\n5         NA        1\n\n\nNow try this yourself. Count how many prefer vanilla ice cream and how many chocolate.\n\n # build all the code for this exercise\n\n\n\nChaining analysis steps into pipelines\nWe can chain multiple analysis steps into a pipeline by continuing to add “and then” statements. For example, dataframe %&gt;% count(...) %&gt;% arrange(...) would first count and then sort the data.\nTry this out by counting the number of responses of languages spoken and and then sorting by the number.\n\n # build all the code for this exercise\n\n\n\nCreating new data columns\nThe function mutate() allows you to add new columns to a data table. For example, dataframe %&gt;% mutate(sum = x + y) would create a new column sum that is the sum of the columns x and y:\n\ndata &lt;- tibble(x = 1:3, y = c(10, 20, 30))\ndata\n\n# A tibble: 3 × 2\n      x     y\n  &lt;int&gt; &lt;dbl&gt;\n1     1    10\n2     2    20\n3     3    30\n\n\n\ndata %&gt;%\n  mutate(sum = x + y)\n\n# A tibble: 3 × 3\n      x     y   sum\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1    10    11\n2     2    20    22\n3     3    30    33\n\n\nNote that the part to the left of the equals sign (here, sum) is the name of the new column, and the part to the right of the equals sign (here, x + y) is an R expression that evaluates to the values in the new column.\nNow apply this concept to the ice_breaker_df dataset. Add a new column browsing by language that is the ratio of number of browser tabs currently open and number of languages spoken:\n\n # build all the code for this exercise\n\n\nCounting with custom conditions\nIt is quite common that we want to count items that meet a specific condition. For example, let’s say we want to count how many people are taller than 155 cm. To do this efficiently, we first create a new column that indicates whether the condition is met or not, and we then use count with that indicator column.\nThe easiest way to create indicator columns is via the function if_else(), which takes three arguments: a condition, a result if the condition is met, and a result if the condition is not met. The following example shows how to create an indicator column showing whether a variable is positive or negative:\n\ndata &lt;- tibble(x = c(-0.5, 2.3, 50, -1.4))\ndata\n\n# A tibble: 4 × 1\n      x\n  &lt;dbl&gt;\n1  -0.5\n2   2.3\n3  50  \n4  -1.4\n\n\n\ndata %&gt;%\n  mutate(\n    sign_of_x = if_else(x &gt;= 0, \"positive\", \"negative\")\n  ) %&gt;%\n  count(sign_of_x)\n\n# A tibble: 2 × 2\n  sign_of_x     n\n  &lt;chr&gt;     &lt;int&gt;\n1 negative      2\n2 positive      2\n\n\nNow try this yourself. Count how many people are taller than 155 cm. Then sort your results.\nHere are a few additional exercises that you can work on to practice and learn more about survey responses from everyone in this room!\n\n\n\nExercise - fun with the survey\nWrite R commands for the following -\n1. How many people took this survey?\n2. How many questions did we ask?\n3. What questions did we ask?\n4. Give a few examples of the data types captured in the questions\n5. Look at responses of questions 4-6 from all participants\n6. Try to rename a column (question)\n7. Make a new dataframe of 5 questions of your choice.\n8. Can you get the height of the tallest person in this room?\n9. How many people speak more than 2 languages?\n10. Select the question about R experience and sort by the kind of R background and experience in this room.\n12. What is the ratio of people who took a plane to this workshop vs those who walked?",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 2</b>: Intro to R, Rstudio, and Quarto"
    ]
  },
  {
    "objectID": "materials/1-workshop1/1-data-generation/index.html",
    "href": "materials/1-workshop1/1-data-generation/index.html",
    "title": "How data analysis is informed by data generation",
    "section": "",
    "text": "Let’s learn about the types of data we’ll use for this workshop",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 1</b>: How data analysis is informed by data generation"
    ]
  },
  {
    "objectID": "materials/1-workshop1/1-data-generation/index.html#slides",
    "href": "materials/1-workshop1/1-data-generation/index.html#slides",
    "title": "How data analysis is informed by data generation",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 1</b>: How data analysis is informed by data generation"
    ]
  },
  {
    "objectID": "materials/1-workshop1/0-welcome/index.html",
    "href": "materials/1-workshop1/0-welcome/index.html",
    "title": "Welcome to the workshop",
    "section": "",
    "text": "Get to know your neighbors and instructors and learn what to expect from this workshop",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 0:</b> Welcome to the workshop"
    ]
  },
  {
    "objectID": "materials/1-workshop1/0-welcome/index.html#slides",
    "href": "materials/1-workshop1/0-welcome/index.html#slides",
    "title": "Welcome to the workshop",
    "section": "Slides",
    "text": "Slides\nMake slides full screen",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 0:</b> Welcome to the workshop"
    ]
  },
  {
    "objectID": "materials/1-workshop1/0-welcome/index.html#before-the-workshop-installing-r-and-rstudio",
    "href": "materials/1-workshop1/0-welcome/index.html#before-the-workshop-installing-r-and-rstudio",
    "title": "Welcome to the workshop",
    "section": "Before the workshop: Installing R and RStudio",
    "text": "Before the workshop: Installing R and RStudio\nPlease install R and RStudio on your laptop. If you already have R and Rstudio installed, please make sure they are up-to date. Please install R version 4.3.1 and RStudio version 2023.09.0 Click here for instructions on installing R and RStudio",
    "crumbs": [
      "AWS Instance IPs",
      "<em>1. Reproducible Data Analysis with R</em>",
      "<b>Module 0:</b> Welcome to the workshop"
    ]
  },
  {
    "objectID": "instance_ips.html",
    "href": "instance_ips.html",
    "title": "AWS Instance IP addresses",
    "section": "",
    "text": "ssh into your instance\nOpen your terminal app on macoOS or the Ubuntu WSL app on windows (install that here: https://ubuntu.com/desktop/wsl).\nThen type ssh -Y genomics@&lt;your-ip-address&gt; where you replace  with the correct one next to your name.\nType the password (note it won’t show what your are typing). The password will be shared on the board in the room of the workshop.\n\n\n\n\n\n\n\n\n\nIP Address\nName\n\n\n\n\n54.204.217.195\nAbiodun Ojo\n\n\n3.92.175.203\nAdamu Tayachew Mekonnen\n\n\n3.92.198.110\nAdijat Jimoh\n\n\n54.166.224.247\nAlabi Banjoko\n\n\n54.86.132.187\nArmando Blondel Djiyou Djeuda\n\n\n54.234.30.39\nArthur John Mazhandu\n\n\n52.90.90.197\nAurélie Bernadette Laure Kifack ZetangI\n\n\n44.204.3.77\nBabalwa Nyide\n\n\n44.211.208.240\nBarry Truebody\n\n\n184.72.105.89\nBongani Dlamini\n\n\n3.84.36.3\nBusizwe Sibandze\n\n\n44.202.128.240\nChimwemwe Mhango\n\n\n44.210.128.6\nClyde Mulenga\n\n\n35.172.128.62\nConrad Mogane\n\n\n184.72.204.163\nConstance Himukumbwa\n\n\n44.204.49.215\nDeel-Dylan Ngouajio Kenfack Mezatio\n\n\n34.239.104.73\nDejenie Shiferaw Teklu\n\n\n3.89.140.116\nDelories Sikuku\n\n\n54.234.252.142\nDennis Otieno\n\n\n34.205.78.79\nDorcas Maruapula\n\n\n54.86.113.216\nDrake Byamukama\n\n\n34.205.32.252\nEdwin Magomere\n\n\n54.165.228.106\nEnock Kofi Amoako\n\n\n54.165.159.135\nEnock Mulowa Mumbula\n\n\n44.203.177.20\nEster Adamson\n\n\n54.227.208.165\nFortunate Natwijuka\n\n\n34.201.250.130\nFrancis Gbadamosi\n\n\n54.166.224.247\nGeoffrey Kwenda\n\n\n54.234.132.200\nGlory -\n\n\n3.91.247.50\nGoitseone Lemogang\n\n\n44.211.153.237\nGraceful Mulenga\n\n\n35.171.4.174\nHerman Philipe Nfombouot Njitoyap\n\n\n18.212.51.232\nHlelowenkosi Mlimi\n\n\n44.211.214.77\nJohn Kimotho\n\n\n44.204.193.125\nJohn Lumbala Chitoti\n\n\n54.156.151.90\nJovita Nagawa\n\n\n174.129.171.171\nJoyce Sakala\n\n\n98.81.235.207\nJustice Ohene Amofa\n\n\n54.172.195.76\nKalo Musukuma\n\n\n44.201.83.38\nLeatitia Kampiire-San\n\n\n18.233.156.236\nLeslie Kenou Djionang\n\n\n44.211.155.254\nLimbani Thengo\n\n\n44.203.175.26\nLindiwe Amanda Jhamba\n\n\n35.174.106.173\nLuka Sote\n\n\n52.87.76.31\nMaclay Muzhuzha\n\n\n3.86.24.238\nMahamat Gadji\n\n\n52.91.17.192\nMarea Neo Pema\n\n\n54.86.173.189\nMiteku Andualem Limenih\n\n\n52.91.249.1\nMunsaka Siankuku\n\n\n54.161.129.17\nNatefo Daughter Keakantse\n\n\n184.72.110.180\nNdivhuwo Nemukondeni\n\n\n52.91.239.111\nNelson Sonela\n\n\n54.162.38.93\nNgamlaleu Modeste Romuald\n\n\n3.85.228.28\nNicholas Ekow Thomford\n\n\n44.202.219.145\nNtokozo Mthimunye\n\n\n54.164.16.231\nPatience Motshosi\n\n\n54.174.192.128\nPatricia Kankundiye\n\n\n54.174.252.178\nPatrick Mokgethi\n\n\n44.202.14.123\nRachael Gachogo\n\n\n3.91.61.197\nRobert Opio\n\n\n52.23.249.64\nRufaro Emelda Beryl Makura\n\n\n54.175.111.181\nSabrina Ngubay\n\n\n44.202.138.88\nSamuel Mingle\n\n\n3.87.67.193\nSamuel Chenge\n\n\n54.164.133.104\nSamuel Chima Ugbaja\n\n\n52.90.4.59\nNA\n\n\n3.91.195.136\nSinenhlanhla Mkhize\n\n\n54.166.89.209\nSiphesihle Lukhele\n\n\n52.91.163.102\nSiyethaba Mkhize\n\n\n52.70.86.137\nSomila Nazo\n\n\n54.89.254.93\nSvitsai Chagonda\n\n\n54.159.206.228\nSyrus Semawule\n\n\n52.202.205.103\nTeko Mmapula Matsuru\n\n\n34.230.80.50\nThato Phenyo\n\n\n44.203.55.83\nTimothy Nathaniel Yakubu\n\n\n18.207.248.56\nTrevor -\n\n\n54.210.214.54\nTsholofelo Ratsoma\n\n\n44.201.222.157\nVincent Mekel\n\n\n3.87.251.123\nViolet Ankunda\n\n\n3.87.66.211\nVuyisile Siyaye\n\n\n54.209.53.114\nWenkosi Qulu\n\n\n52.23.159.169\nWinfred Mutwiri\n\n\n54.89.208.70\nYiakon Sein\n\n\n54.173.75.236\nYohana Amos\n\n\n3.84.3.34\nZanele Mtshali\n\n\n44.204.5.131\nZekhethelo Mkhwanazi"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Biology Workshop Series",
    "section": "",
    "text": "Data Science for Biology Workshop Series\n\n\nWelcome to the workshop\nThis is the third workshop in a series for people learning biological data science and bioinformatics.\nLog onto your instance by following these instructions\n\n\n\n\n\n\n\nSchedule\n\n\n\n\n\n\n\nDate/Time\nTopic\nInstructor\n\n\n\n\nMonday, February 24th\n————————————————————–\n————————————————————–\n\n\nAM\nWorkshop Introduction\nJoseph Elsherbini (Ragon Institute)\n\n\n\nViromics Sequencing and Library Preperation\nLindsay Droit (Wash U School of Medicine)\n\n\nPM\nIntro to the UNIX Shell\nMarie HIDJO (AiBST)\n\n\nTuesday, February 25th\n————————————————————–\n————————————————————–\n\n\nAM\n16S Data Preprocessing\nAfrah Khairallah (AHRI)\n\n\nPM\nMapping and Seembly of short read libraries\nJoseph Elsherbini (Ragon Institute)\n\n\nWednesday, February 26th\n————————————————————–\n————————————————————–\n\n\nAM\nTargetted Viromics bioinformatics\nWonderful Choga (University of Botswana)\n\n\nPM\nGenomics Adeventure I (interactive session)\nall instructors\n\n\nEvening\nReception\n\n\n\nThursday, February 27th\n————————————————————–\n————————————————————–\n\n\nAM\nUntargetted Viromics\nLuis Chica (Wash U School of Medicine)\n\n\nPM\nGenomics Adeventure II (interactive session)\nall instructors\n\n\nFriday, February 28th\n————————————————————–\n————————————————————–\n\n\nAM\nCertificate ceremony\n\n\n\n\nsnakemake demonstration\nJoseph Elsherbini (Ragon Institute)\n\n\nPM\nOpen Lab (discussion, work on own data, work on genomics adventure)\nParticipants\n\n\n\nWorkshop Outro\nAfrah Khairallah (AHRI)\n\n\n\n————————————————————–\n————————————————————–"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Series Organizer and Instructor\nDr. Scott Handley\nProfessor\nWashington University School of Medicine\nDepartment of Pathology and Immunology\n\n\n\n\n\n\n\n\n\nSeries Organizer and Instructor\nDr. Joseph Elsherbini\nData Scientist\nRagon Institute of MGH, MIT and Harvard\n\n\n\n\n\n\n\n\n\nInstructor\nDr. Jacques Ravel\nProfessor\nUniversity of Maryland School of Medicine Department of Microbiology and Immunology\n\n\n\n\n\n\n\n\n\nInstructor\nDr. Sinaye Ngcapu\nSenior Scientist\nCAPRISA\nHIV Mucosal Immunology Laboratory\n\n\n\n\n\n\n\n\n\nInstructor\nLindsay Droit\nResearch Laboratory Manager Washington University School of Medicine\n\n\n\n\n\n\n\n\n\nInstructor\nDr. Elizabeth Costello\nResearch Scientist\nStanford University\n\n\n\n\n\n\n\n\n\nInstructor\nDr. Michael France\nBioinformatician\nUniversity of Maryland School of Medicine\n\n\n\n\n\n\n\n\n\nSeries Organizer and Instructor\nChandani Desai\nProject Manager\nRagon Institute of MGH, MIT and Harvard\n\n\n\n\n\n\n\n\n\nInstructor\nAsavela Kama\nBioinformatician\nCentre for the AIDS Programme of Research in South Africa (CAPRISA) - UKZN\n\n\n\n\n\n\n\n\n\nInstructor\nJohnathan Shih\nBioinformatics\nResearch Technician\nRagon Institute of MGH, MIT and Harvard\n\n\n\n\n\n\n\n\n\nSeries Logistics and Management\nSarah Eisa\nProgram Manager\nRagon Institute of MGH, MIT and Harvard\n\n\n\n\n\n\n\n\n\nSeries Logistics and Management\nRenee Schumm\nStaff Assistant II\nRagon Institute of MGH, MIT and Harvard\n\n\n\n\n\n\n\n\n\nTeaching Assistant\nAaron Abai\nResearch Technician\nRagon Institute of MGH, MIT and Harvard\n\n\n\n\n\n\n\n\n\nTeaching Assistant\nDerek Tshiabuila\nPhD Research Fellow\nCentre for Epidemic Response and Innovation (CERI) - Stellenbosch\n\n\n\n\n\n\n\n\n\nTeaching Assistant\nDr. Afrah Khairallah\nPostdoctoral Research Fellow Africa Health Research Institute (AHRI) - UKZN\n\n\n\n\n\n\n\n\n\nTeaching Assistant\nJohan van der Molen\nCentre for the AIDS Programme of Research in South Africa (CAPRISA) - UKZN\n\n\n\n\n\n\n\n\n\nTeaching Assistant\nWasim Abdool Karim\nBioinformatician\nCentre for Epidemic Response and Innovation (CERI) - Stellenbosch"
  },
  {
    "objectID": "about.html#working-with-high-dimensional-data-in-r---teaching-team",
    "href": "about.html#working-with-high-dimensional-data-in-r---teaching-team",
    "title": "About",
    "section": "",
    "text": "Series Organizer and Instructor\nDr. Scott Handley\nProfessor\nWashington University School of Medicine\nDepartment of Pathology and Immunology\n\n\n\n\n\n\n\n\n\nSeries Organizer and Instructor\nDr. Joseph Elsherbini\nData Scientist\nRagon Institute of MGH, MIT and Harvard\n\n\n\n\n\n\n\n\n\nInstructor\nDr. Jacques Ravel\nProfessor\nUniversity of Maryland School of Medicine Department of Microbiology and Immunology\n\n\n\n\n\n\n\n\n\nInstructor\nDr. Sinaye Ngcapu\nSenior Scientist\nCAPRISA\nHIV Mucosal Immunology Laboratory\n\n\n\n\n\n\n\n\n\nInstructor\nLindsay Droit\nResearch Laboratory Manager Washington University School of Medicine\n\n\n\n\n\n\n\n\n\nInstructor\nDr. Elizabeth Costello\nResearch Scientist\nStanford University\n\n\n\n\n\n\n\n\n\nInstructor\nDr. Michael France\nBioinformatician\nUniversity of Maryland School of Medicine\n\n\n\n\n\n\n\n\n\nSeries Organizer and Instructor\nChandani Desai\nProject Manager\nRagon Institute of MGH, MIT and Harvard\n\n\n\n\n\n\n\n\n\nInstructor\nAsavela Kama\nBioinformatician\nCentre for the AIDS Programme of Research in South Africa (CAPRISA) - UKZN\n\n\n\n\n\n\n\n\n\nInstructor\nJohnathan Shih\nBioinformatics\nResearch Technician\nRagon Institute of MGH, MIT and Harvard\n\n\n\n\n\n\n\n\n\nSeries Logistics and Management\nSarah Eisa\nProgram Manager\nRagon Institute of MGH, MIT and Harvard\n\n\n\n\n\n\n\n\n\nSeries Logistics and Management\nRenee Schumm\nStaff Assistant II\nRagon Institute of MGH, MIT and Harvard\n\n\n\n\n\n\n\n\n\nTeaching Assistant\nAaron Abai\nResearch Technician\nRagon Institute of MGH, MIT and Harvard\n\n\n\n\n\n\n\n\n\nTeaching Assistant\nDerek Tshiabuila\nPhD Research Fellow\nCentre for Epidemic Response and Innovation (CERI) - Stellenbosch\n\n\n\n\n\n\n\n\n\nTeaching Assistant\nDr. Afrah Khairallah\nPostdoctoral Research Fellow Africa Health Research Institute (AHRI) - UKZN\n\n\n\n\n\n\n\n\n\nTeaching Assistant\nJohan van der Molen\nCentre for the AIDS Programme of Research in South Africa (CAPRISA) - UKZN\n\n\n\n\n\n\n\n\n\nTeaching Assistant\nWasim Abdool Karim\nBioinformatician\nCentre for Epidemic Response and Innovation (CERI) - Stellenbosch"
  },
  {
    "objectID": "about.html#reproducible-data-analysis-with-r---teaching-team",
    "href": "about.html#reproducible-data-analysis-with-r---teaching-team",
    "title": "About",
    "section": "Reproducible Data Analysis with R - Teaching Team",
    "text": "Reproducible Data Analysis with R - Teaching Team\n\n\n\n\n\n\n\n\n\nKeynote Speaker\nDr. Lenine Liebenberg\nChief Researcher\nCentre for Epidemic Response and Innovation (CERI) - Stellenbosch\nCentre for the AIDS Programme of Research in South Africa (CAPRISA) - UKZN\n\n\n\n\n\n\n\n\n\nWorkshop Instructor\nDr. Laura Symul\nAssistant Professor\nUCLouvain Institute of Statistics, Biostatistics and Actuarial Sciences\n\n\n\n\n\n\n\n\n\nWorkshop Instructor\nMarothi Peter Letsoalo\nSenior Biostatistician\nCentre for the AIDS Programme of Research in South Africa (CAPRISA) - UKZN\n\n\n\n\n\n\n\n\n\nWorkshop Instructor\nSalina Hussain\nResearch Technician\nRagon Institute of MGH, MIT and Harvard\n\n\n\n\n\n\n\n\n\nWorkshop Instructor\nSuuba Demby\nResearch Technician\nRagon Institute of MGH, MIT and Harvard\n\n\n\n\n\n\n\n\n\nSeries Organizer and Instructor\nDr. Scott Handley\nProfessor\nWashington University School of Medicine\nDepartment of Pathology and Immunology\n\n\n\n\n\n\n\n\n\nSeries Organizer and Instructor\nDr. Joseph Elsherbini\nData Scientist\nRagon Institute of MGH, MIT and Harvard\n\n\n\n\n\n\n\n\n\nSeries Organizer and Instructor\nChandani Desai\nProject Manager\nRagon Institute of MGH, MIT and Harvard\n\n\n\n\n\n\n\n\n\nSeries Logistics and Management\nSarah Eisa\nProgram Manager\nRagon Institute of MGH, MIT and Harvard\n\n\n\n\n\n\n\n\n\nTeaching Assistant\nDerek Tshiabuila\nPhD Research Fellow\nCentre for Epidemic Response and Innovation (CERI) - Stellenbosch\n\n\n\n\n\n\n\n\n\nTeaching Assistant\nAsavela Kama\nBioinformatician\nCentre for the AIDS Programme of Research in South Africa (CAPRISA) - UKZN\n\n\n\n\n\n\n\n\n\nTeaching Assistant\nAfrah Khairallah\nPostdoctoral research fellow Africa Health Research Institute (AHRI) - UKZN\n\n\n\n\n\n\n\n\n\nTeaching Assistant\nKwabena Asare\nStatistician\nCentre for the AIDS Programme of Research in South Africa (CAPRISA) - UKZN\n\n\n\n\n\n\n\n\n\nTeaching Assistant\nJohan van der Molen\nCentre for the AIDS Programme of Research in South Africa (CAPRISA) - UKZN\n\n\n\n\n\n\n\n\n\nTeaching Assistant\nWasim Abdool Karim\nBioinformatician Centre for Epidemic Response and Innovation (CERI) - Stellenbosch"
  },
  {
    "objectID": "about.html#about-these-materials",
    "href": "about.html#about-these-materials",
    "title": "About",
    "section": "About these materials",
    "text": "About these materials\nThis website started with the code from Andrew P. Bray’s workshop website and much of the R content has been remixed and adapted from SDS375, a course by Claus O. Wilke."
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Download here"
  },
  {
    "objectID": "datasets.html#icebreaker-survey",
    "href": "datasets.html#icebreaker-survey",
    "title": "Datasets",
    "section": "",
    "text": "Download here"
  },
  {
    "objectID": "datasets.html#download-the-instructional-dataset",
    "href": "datasets.html#download-the-instructional-dataset",
    "title": "Datasets",
    "section": "Download the instructional dataset:",
    "text": "Download the instructional dataset:\nThe main data set for use in lectures is split into 5 tables. The idea of this dataset is that there is a randomized controlled trial of a drug aimed at reducing HIV risk by reducing inflammation. There were 23 participants in the placebo arm and 21 in the treatment arm. There were 3 visits for the trial – baseline (before any treatment occurred), week_1 (1 week after treatment) and week_7 (7 weeks after treatment). At each time point inflammation was measured using luminex (elisa_cytokines table) and immune cells counts were measured from a pap-smear (flow_cytometry table).\n\n00_sample_ids_UKZN_workshop_2023.csv\n\npid – participant id\ntime_point – “baseline”, “week_1”, or “week_7” arm – “treatment” or “placebo”\nsample_id – the “wet-lab” sample id associated with this timepoint\n\n01_participant_metadata_UKZN_workshop_2023.csv\n\npid - participant id\narm - “treatment” or “placebo”\nsmoker - “yes” or “no”\nage – integer age in years\neducation – 4 options (“less than grade 9”, “grade 10-12, not matriculated”, “grade 10-12, matriculated”, “post-secondary”)\nsex – all participants are “F”\n\n02_visit_clinical_measurements_UKZN_workshop_2023.csv\n\npid – particpant id\ntime_point – “baseline”, “week_1”, or “week_7”\narm – “treatment” or “placebo”\nnugent_score – Nugent Score, a number from 0-10. 0-3 is no BV, 4-6 is intermediate BV, and 7-10 is BV . crp_blood – decimal number representing C-reactive protein blood test (CRP) ph – vaginal pH\n\n03_elisa_cytokines_UKZN_workshop_2023.csv\n\nsample_id - the “wet-lab” sample id associated with this timepoint\ncytokine - “IL-1a”, “IL-10”, “IL-1b”, “IL-8”, “IL-6”, “TNFa”, “IP-10”, “MIG”, “IFN-Y”, “MIP-3a”\nconc – decimal number representing concentration\nlimits – either “within limits” or “out of range”\n\n04_flow_cytometry_UKZN_workshop_2023.csv\n\nsample_id - the “wet-lab” sample id associated with this timepoint\nAll other columns – the integer count of this type of cell in this sample\ncd4_t_cells might best be analyzed as a proportion of cd3_t_cells…"
  },
  {
    "objectID": "datasets.html#download-the-yogurt-dataset-odd-number-groups",
    "href": "datasets.html#download-the-yogurt-dataset-odd-number-groups",
    "title": "Datasets",
    "section": "Download the yogurt dataset (Odd number groups)",
    "text": "Download the yogurt dataset (Odd number groups)\n\n00_sample_ids_yogurt.csv\n01_participant_metadata_yogurt.csv\n02_qpcr_results_yogurt.csv\n03_luminex_results_yogurt.csv"
  },
  {
    "objectID": "datasets.html#download-the-birth-control-dataset-even-number-groups",
    "href": "datasets.html#download-the-birth-control-dataset-even-number-groups",
    "title": "Datasets",
    "section": "Download the birth control dataset (Even number groups)",
    "text": "Download the birth control dataset (Even number groups)\n\n00_sample_ids_period.csv\n01_participant_metadata_period.csv\n02_luminex_period.csv\n03_flow_cytometry_period.csv"
  },
  {
    "objectID": "installation-instructions.html",
    "href": "installation-instructions.html",
    "title": "Installing R and RStudio",
    "section": "",
    "text": "adapted from datacarpentry.org\nFor this workshop, we will need R as well RStudio. R and RStudio are both completely free and open source software. While R is the underlying statistical computing environment, RStudio is the graphical IDE (integrated development environment) that makes using R easier, more intuitive, and interactive. Both are separate downloads and installations, and you will need to install R first, followed by RStudio. Once both are installed, you would just open and work in RStudio, which will run R in the background. Then you will install some R packages (collections of functions you can use in your own coding) which we are using for this workshop. Please install R version 4.3.1 or later and RStudio version 2023.09.0 or later."
  },
  {
    "objectID": "installation-instructions.html#step-1---follow-instructions-for-your-operating-system",
    "href": "installation-instructions.html#step-1---follow-instructions-for-your-operating-system",
    "title": "Installing R and RStudio",
    "section": "1 Step 1 - follow instructions for your operating system",
    "text": "1 Step 1 - follow instructions for your operating system\n\n1.0.1 For Windows see Section 3\n\n\n1.0.2 For MacOS see Section 4\n\n\n1.0.3 For Linux see Section 5"
  },
  {
    "objectID": "installation-instructions.html#step-2---install-r-packages",
    "href": "installation-instructions.html#step-2---install-r-packages",
    "title": "Installing R and RStudio",
    "section": "2 Step 2 - install R packages",
    "text": "2 Step 2 - install R packages\nOpen RStudio and Paste the following into your console on the left/bottom-left.\ninstall.packages(c(\"broom\", \"cluster\", \"colorspace\", \"cowplot\", \"distill\", \"gapminder\", \"GGally\", \"gganimate\", \"ggbeeswarm\", \"ggiraph\", \"ggdendro\", \"ggdist\", \"ggforce\",\"ggplot2movies\", \"ggrepel\", \"ggridges\", \"ggthemes\", \"gifski\", \"glue\",\"knitr\", \"learnr\", \"naniar\", \"margins\", \"MASS\", \"Matrix\",\"nycflights13\", \"palmerpenguins\", \"patchwork\", \"quarto\", \"rgdal\", \"rmarkdown\",\"rnaturalearth\", \"sf\", \"shinyjs\", \"tableone\", \"tidyverse\", \"transformr\", \"umap\"))"
  },
  {
    "objectID": "installation-instructions.html#sec-windows",
    "href": "installation-instructions.html#sec-windows",
    "title": "Installing R and RStudio",
    "section": "3 Windows",
    "text": "3 Windows\n\n3.1 If you don’t have R and RStudio installed:\nGo to https://posit.co/download/rstudio-desktop/ and follow this instruction for windows.\n\n\n3.2 If you already have R and RStudio installed, check to see if updates are available.\nRStudio: Open RStudio and click on Help &gt; Check for updates. If a new version is available, quit RStudio, and download the latest version.\nR: Upon starting RStudio, the version of R you are running will appear on the console. You can also type sessionInfo() in the console to display the version of R you are running. The CRAN website will tell you if there is a more recent version available. You can update R using the installr package, by running:\n# installr is for windows only!\nif( !(\"installr\" %in% installed.packages()) ){install.packages(\"installr\")} \n\ninstallr::updateR(TRUE)"
  },
  {
    "objectID": "installation-instructions.html#sec-mac",
    "href": "installation-instructions.html#sec-mac",
    "title": "Installing R and RStudio",
    "section": "4 MacOs",
    "text": "4 MacOs\n\n4.1 Check your processor\nadapted from https://docs.cse.lehigh.edu/determine-mac-architecture/\nMake sure you are downloading and installing the right version of R and R Studio for your laptop’s CPU. Some Macs have an intel chip (also known as x64 or x86_64 architecture), while the newest macs have M1 or M2 chips (also known as ARM architecture).\nTo determine whether a Mac is running an Intel Processor or Apple ARM M1 or M2, click on the  Apple Menu and select ‘About this Mac’:\n\n\n\nclick about this mac\n\n\nFrom the ‘About this Mac’ screen, on the ‘Overview’ tab, look for a line that indicates either ‘Chip’ or ‘Processor’. If the line contains M1 or M2, the machine is running Apple Silicon. Alternatively, the word Intel indicates that the machine is running an Intel-based Core series processor.\n\n\n\nM1 or M2\n\n\n\n\n\nintel\n\n\n\n\n4.2 If you don’t have R and RStudio installed:\nGo to https://posit.co/download/rstudio-desktop/ and follow this instruction for MacOS.\n\n\n4.3 If you already have R and RStudio installed, check to see if updates are available.\nRStudio: Open RStudio and click on Help &gt; Check for updates. If a new version is available, quit RStudio, and download the latest version.\nR: Upon starting RStudio, the version of R you are running will appear on the console. You can also type sessionInfo() to display the version of R you are running. The CRAN website will tell you if there is a more recent version available. For this workshop install version 4.3.1 of R"
  },
  {
    "objectID": "installation-instructions.html#sec-linux",
    "href": "installation-instructions.html#sec-linux",
    "title": "Installing R and RStudio",
    "section": "5 Linux",
    "text": "5 Linux\n\n5.1 If you don’t have R and RStudio installed:\nGo to https://posit.co/download/rstudio-desktop/ and follow this instruction for your Linux OS.\n\n\n5.2 If you already have R and RStudio installed, check to see if updates are available.\nRStudio: Open RStudio and click on Help &gt; Check for updates. If a new version is available, quit RStudio, and download the latest version.\nR: Upon starting RStudio, the version of R you are running will appear on the console. You can also type sessionInfo() to display the version of R you are running. The CRAN website will tell you if there is a more recent version available. For this workshop install version 4.3.1 of R"
  },
  {
    "objectID": "materials/1-workshop1/0-welcome/exercise_1.html#activity",
    "href": "materials/1-workshop1/0-welcome/exercise_1.html#activity",
    "title": "Exploring Data for Patterns",
    "section": "Activity",
    "text": "Activity\n\nPatterns are the essence of data exploration and our eyes’ ability to pick them out is integral to data understanding. Much of the data we work with, however, do not have a natural form and we need to make decisions about how they are to be represented. Try different ways to visualize the datasets so meaningful patterns may be found.\n\n\nGenetic profiles of cancer\nThese datasets contains 10 cancer samples. Table 1 describes the mutational status for a set of genes (A-E) and whether a mutation if absent (0) or present (1). Table 2 summarizes the expression levels of those genes, ranging from no expression (0) to high expression (3).\n\n\n\nTable 1: Mutational status for a set of genes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample -&gt;\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nGene A\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nGene B\n0\n0\n0\n0\n1\n1\n1\n0\n1\n1\n\n\nGene C\n0\n0\n1\n0\n0\n0\n1\n1\n1\n1\n\n\nGene D\n1\n1\n0\n0\n1\n1\n0\n0\n0\n0\n\n\nGene E\n0\n1\n1\n0\n1\n0\n0\n0\n1\n0\n\n\n\n\n\n\n\n\n\nTable 2: Expression levels for a set of genes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample -&gt;\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nGene A\n2\n1\n1\n2\n2\n0\n2\n1\n1\n2\n\n\nGene B\n1\n1\n2\n1\n0\n0\n0\n2\n0\n0\n\n\nGene C\n1\n1\n3\n1\n2\n2\n3\n0\n3\n0\n\n\nGene D\n0\n0\n2\n1\n3\n3\n2\n1\n1\n1\n\n\nGene E\n1\n3\n3\n1\n3\n1\n2\n1\n3\n2\n\n\n\n\n\n\n\n\n          1. Think about the problem on your own for 5 minutes.\n          2. In your groups, discuss and create different visualizations to highlight underlying patterns\n          3. Summarize the group’s approach\n          4. Elect/volunteer a spokesperson to present the solution\n\n\nConsider the following concepts when creating your visualizations\n\n\n\n\nPatterns\nPatterns are the essence of data exploration. What kinds of representation will produce the most meaningful insights?\n   \n\n\nEncodings\nSome visual estimations are easier to make than others. How might you use encodings that are less accurate but otherwise better at conveying overall trends?\n  \n\n\n\n\nColor\nColor is a powerful encoding that presents several challenges. Have you chosen a color scale that is optimal for that data type?\n   \n\n\nSalience and Relevance\nPop-out effects enable quick recognition. Are the most noticeable elements of your visualizations also the most relevant?"
  },
  {
    "objectID": "materials/1-workshop1/0-welcome/slides.html#section",
    "href": "materials/1-workshop1/0-welcome/slides.html#section",
    "title": "Welcome to the workshop",
    "section": "",
    "text": "Goals for this session\n\n\nGet to know your instructors and neighbors\nSet expectations for the week\nGet excited!"
  },
  {
    "objectID": "materials/1-workshop1/0-welcome/slides.html#section-1",
    "href": "materials/1-workshop1/0-welcome/slides.html#section-1",
    "title": "Welcome to the workshop",
    "section": "",
    "text": "Workshop materials are at:\nhttps://elsherbini.github.io/durban-data-science-for-biology/"
  },
  {
    "objectID": "materials/1-workshop1/0-welcome/slides.html#discussions-discord",
    "href": "materials/1-workshop1/0-welcome/slides.html#discussions-discord",
    "title": "Welcome to the workshop",
    "section": "Discussions: discord",
    "text": "Discussions: discord\nAsk questions at #workshop-questions on https://discord.gg/UDAsYTzZE."
  },
  {
    "objectID": "materials/1-workshop1/0-welcome/slides.html#stickies",
    "href": "materials/1-workshop1/0-welcome/slides.html#stickies",
    "title": "Welcome to the workshop",
    "section": "Stickies",
    "text": "Stickies\n\n\n\n\n\n\nDuring an activity, place a yellow sticky on your laptop if you’re good to go and a pink sticky if you want help.\n\n\n\n\nImage by Megan Duffy"
  },
  {
    "objectID": "materials/1-workshop1/0-welcome/slides.html#practicalities",
    "href": "materials/1-workshop1/0-welcome/slides.html#practicalities",
    "title": "Welcome to the workshop",
    "section": "Practicalities",
    "text": "Practicalities\n\nWiFi:\nNetwork: KTB Free Wifi (no password needed)\nNetwork AHRI Password: @hR1W1F1!17\nNetwork CAPRISA-Corp Password: corp@caprisa17\nBathrooms are out the lobby to your left"
  },
  {
    "objectID": "materials/1-workshop1/0-welcome/slides.html#introductions",
    "href": "materials/1-workshop1/0-welcome/slides.html#introductions",
    "title": "Welcome to the workshop",
    "section": "Introductions",
    "text": "Introductions\n\n\n\n−+\n03:00\n\n\n\nTake ~3 minutes to introduce yourself to your neighbors.\nPlease share …\n\nYour name\nWhere you’re from and where you work\nYour current go-to method for analyzing data"
  },
  {
    "objectID": "materials/1-workshop1/0-welcome/slides.html#your-instructors",
    "href": "materials/1-workshop1/0-welcome/slides.html#your-instructors",
    "title": "Welcome to the workshop",
    "section": "Your Instructors",
    "text": "Your Instructors\nWho are we?"
  },
  {
    "objectID": "materials/1-workshop1/0-welcome/slides.html#lets-make-this-workshop-work-for-all",
    "href": "materials/1-workshop1/0-welcome/slides.html#lets-make-this-workshop-work-for-all",
    "title": "Welcome to the workshop",
    "section": "Let’s make this workshop work for all",
    "text": "Let’s make this workshop work for all\n\n\nYou belong here. This workshop is intended for a wide-audience with a focus on beginners. If you feel out of place - it’s our problem, not yours!\nStay committed. This week-long workshop is intended to build each day and leave you with skills you can really use. Commit to stay engaged for best results, for you and your group!\nThis is a challenging but friendly environment. We are here to learn and grow. In order to make the right environment please follow “the 4 social rules” and code-of-conduct."
  },
  {
    "objectID": "materials/1-workshop1/0-welcome/slides.html#flow-of-the-workshop",
    "href": "materials/1-workshop1/0-welcome/slides.html#flow-of-the-workshop",
    "title": "Welcome to the workshop",
    "section": "Flow of the Workshop",
    "text": "Flow of the Workshop"
  },
  {
    "objectID": "materials/1-workshop1/0-welcome/slides.html#the-premise-of-the-workshop",
    "href": "materials/1-workshop1/0-welcome/slides.html#the-premise-of-the-workshop",
    "title": "Welcome to the workshop",
    "section": "The premise of the workshop",
    "text": "The premise of the workshop\nWe’ve created a true-to-life “medium dimensional” dataset that we’ll use for all instruction\n\nabout 40 participants split across two treatment arms\nthree time points (before treatment, after treatment, and longer follow-up)\nseveral measurements per time-point including cytokine concentrations and flow cytometry data (more from Salina and Suuba next!)\n\nWe’ve also created group datasets so you can practice applying what you’ve learned on new data."
  },
  {
    "objectID": "materials/1-workshop1/0-welcome/slides.html#why-this-format",
    "href": "materials/1-workshop1/0-welcome/slides.html#why-this-format",
    "title": "Welcome to the workshop",
    "section": "Why this format",
    "text": "Why this format\nWe’ve taught workshops here before where we packed a lot (too much) into one week.\nBased on feedback, we’re trying to focus on R for the whole week, and to make the example data relevant to a lot of your actual work.\nHowever, many of you in the room know a lot more immunology than us - and even if you know none you can learn the R and generalize it to your research.\nThis is the first time running this new workshop - please give lots of feedback on how to improve it for next time."
  },
  {
    "objectID": "materials/1-workshop1/0-welcome/slides.html#the-content-of-this-workshop",
    "href": "materials/1-workshop1/0-welcome/slides.html#the-content-of-this-workshop",
    "title": "Welcome to the workshop",
    "section": "The content of this workshop",
    "text": "The content of this workshop\n\nWe’ve created 7 modules as well as a group activity for this workshop.\n\nThis is probably too much material to get through in this week!\n\nAs instructors we’re going to be trying to teach at the right pace to keep everyone learning all week.\n\nThe materials will stay on the website forever for you to work through at your own pace."
  },
  {
    "objectID": "materials/1-workshop1/0-welcome/slides.html#lets-take-a-poll",
    "href": "materials/1-workshop1/0-welcome/slides.html#lets-take-a-poll",
    "title": "Welcome to the workshop",
    "section": "Let’s take a poll",
    "text": "Let’s take a poll\nGo to the event on wooclap\n\n\n\n\nback to module"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/images/ex-2.html",
    "href": "materials/1-workshop1/2-intro-to-r/images/ex-2.html",
    "title": "Data Science for Biology Workshop Series",
    "section": "",
    "text": "Spatial Logical Toy Inventor: Erno Rubik Assignee: Politoys Ipari Szovetkezet, Budapest, Hungary The first patent by the inventor is registered in Hungary. Appl. No.: 289,192 Filed: Aug. 3, 1981 Background of the Invention The invention relates to a spatial logical toy having a total of eighteen toy elements which form a regular or an irregular spatial body, preferably an oblong body, in the assembled state. Spatial logical toys are well known, such as that described in the HU-PS No. 170 062 of the same applicant, which relates to a spatial logical toy consisting of twenty-seven solids which form a cube in the assembled state. The toy elements, in the shape of small cubes, may be turned along the spatial axes of the cube by means of connecting elements arranged in the geometric center of the large cube. The surfaces of the small cubes forming each surface of the large cube are colored or carry numbers, figures or any other symbols which can be assembled into the predetermined logical order of sequence by simultaneously rotating the nine toy elements forming the surfaces of the large cube. Summary of the Invention The logical toy according to the present invention represents an improved form of the previously described spatial logical toy. The construction is based on the same principles, however, the internal connection is performed by means of absolutely new and particular solids. The key feature according to the invention, i.e. shape, name, sounds, mode of interconnection and central fixture will be described in detail by means of two preferred embodiments, by the aid of the accompanying drawings, wherein. Component blocks of the spatial logial toy. ex-2-figs.png What is claimed is: 1. In a spatial logical toy assembled from a plurality of toy elements, of which a predetermined number may be rotated in the direction of the spatial axes starting from the geometrical center of the logical toy, the improvement wherein the spatial logical toy is formed by a total of eighteen toy elements. Two sets of eight toy elements each comprise substantially cubiforms with integally formed cam elements and each of the sets comprise eight identical toy elements, and two connecting toy elements, and means for joining the connecting toy elements to coact with the cam elements to form an integrated toy body, the joining means comprising a single screw enclosed by a spring. The spatial logical toy as claimed in claim 1, wherein the toy has the shape of a regular geometrical body in the assembled state. The toy elements thereof belonging to one set comprise eight cubiform homologous elements each having a first cam element connected to one corner thereof. Two confining surfaces of which lie at a unit distance from two surfaces of the cube and are parallel therewith and are cut-off in the form of an ellipsis-quarter, and a third confining surface thereof is coplanar with another surface of the cube and between the two confining faces of the first cam element running parallel with the cube and the cube there is a hollow with a convex spherical surface."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#section",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#section",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "",
    "text": "Workshop materials are at:\nhttps://elsherbini.github.io/durban-data-science-for-biology/"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#section-1",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#section-1",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "",
    "text": "Goals for this session\n\n\nAnswer the question “Why R?”\nLearn how to use Quarto to make notebook reports.\nBegin interacting with data in R."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#discussions-discord",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#discussions-discord",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Discussions: discord",
    "text": "Discussions: discord\nAsk questions at #workshop-questions on https://discord.gg/UDAsYTzZE."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#stickies",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#stickies",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Stickies",
    "text": "Stickies\n\n\n\n\n\n\nDuring an activity, place a blue sticky on your laptop if you’re good to go and a pink sticky if you want help.\n\n\n\n\nImage by Megan Duffy"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#practicalities",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#practicalities",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Practicalities",
    "text": "Practicalities\n\nWiFi:\nNetwork: KTB Free Wifi (no password needed)\nNetwork AHRI Password: @hR1W1F1!17\nNetwork CAPRISA-Corp Password: corp@caprisa17\nBathrooms are out the lobby to your left"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#what-is-r",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#what-is-r",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "What is R?",
    "text": "What is R?\nR is a general purpose programming language that’s really well suited to statistics, manipulating tabular data, and plotting."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#why-r",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#why-r",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Why R?",
    "text": "Why R?"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#why-r-1",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#why-r-1",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Why R?",
    "text": "Why R?\n\nR is completely free and open source\nUsing R connects you with a community around the whole world\nR has a huge amount of packages - code someone else wrote so you don’t have to!"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#obtaining-r",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#obtaining-r",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Obtaining R",
    "text": "Obtaining R\nWindows, Mac or Linux OS: https://www.r-project.org"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#running-r",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#running-r",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Running R",
    "text": "Running R\n\n\n\nRStudio\n\nRStudio: http://www.rstudio.com\nBuilt-in tools for viewing plots, tables, and rendering documents\nThe best way to work if you only use R\n\n\n\nVSCode\n\nhttps://code.visualstudio.com/download\nnot a full IDE, but you can customize it with extensions\nWorks well with not just R, but all major programming languages\nGuide on setting up VSCode for R programming: link and link"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#create-an-r-project",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#create-an-r-project",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Create an R Project",
    "text": "Create an R Project\n\n\n\nFile -&gt; New Project…"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#create-an-r-project-1",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#create-an-r-project-1",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Create an R Project",
    "text": "Create an R Project\n\n\n\nClick on New Directory"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#create-an-r-project-2",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#create-an-r-project-2",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Create an R Project",
    "text": "Create an R Project\n\n\n\nName your directory and click “Create Project”"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#create-an-r-project-3",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#create-an-r-project-3",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Create an R Project",
    "text": "Create an R Project\n\n\n\nYou made a project! This creates a file for you with the .qmd extension"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#create-an-r-project-4",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#create-an-r-project-4",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Create an R Project",
    "text": "Create an R Project\n\n\n\nSwitch from “visual” to “source” to see the plain-text version of this document."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#create-an-r-project-5",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#create-an-r-project-5",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Create an R Project",
    "text": "Create an R Project\n\n\n\nClick on “Render” to ask Quarto to turn this plain-text document into an HTML page"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#create-an-r-project-6",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#create-an-r-project-6",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Create an R Project",
    "text": "Create an R Project\n\n\n\nYour default web-browser will open and show you the rendered document!"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#what-are-the-parts-of-rstudio",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#what-are-the-parts-of-rstudio",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "What are the parts of RStudio?",
    "text": "What are the parts of RStudio?"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#the-text-editor",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#the-text-editor",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "The text editor",
    "text": "The text editor"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#the-console",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#the-console",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "The console",
    "text": "The console"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#the-right-panes",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#the-right-panes",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "The right panes",
    "text": "The right panes"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#lets-take-a-poll",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#lets-take-a-poll",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Let’s take a poll",
    "text": "Let’s take a poll\nGo to the event on wooclap\n\nM2. Match the areas with the right functionality"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#what-is-programming",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#what-is-programming",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "What is programming?",
    "text": "What is programming?\nProgramming is giving the computer instructions using text. The tricky part is learning how to speak to a computer.\n```{r}\n\"a\" == \"A\"\n```\n```{r error=TRUE}\nmax(c(1,2,3,4))\n```\n```{r error=TRUE}\n# how old will I be in 10 years?\nmy_age + 10\n```\n\n\n\n\nComputers are incredibly literal\n\n\n\n[1] FALSE\n\n\n\n\n\nComputers care about punctuation\n\n\n\n[1] 4\n\n\n\n\n\nComputers only know what you tell them\n\n\n\nError in eval(expr, envir, enclos): object 'my_age' not found\n\n\n\n\n\nMost bugs happen because of one of these things."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#what-is-programming-1",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#what-is-programming-1",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "What is programming?",
    "text": "What is programming?\nSo why bother at all? Because if you can tell a computer how to do it once, it is reproducible!\nIf the data changes or you find a mistake, just rerun!\nYou can run the same code on new data\nYou can share your code with others so they can start where you left off"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#lets-use-r-for-math",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#lets-use-r-for-math",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Let’s use R for math",
    "text": "Let’s use R for math\n\n\n\n−+\n08:00\n\n\n\nIn the console, try typing some commands:\n\n# arithmetic\n3 + 5 + 10\n10 * (5 + 1)\n3**2 # what does the ** operator do in R?\n# check inequalities and equalities\n4 &gt;= 1 # what does this mean?\n5 + 4 == 9\n# make some errors\n\"3\" + 5 # why is this an error?\nmy_age + 5  # why is this an error?\n\n# write a math expression to calculate what percentage\n# of your life has been in post-secondary school/training\n# (university, training programs, masters, PhD)"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#say-hello-to-the-text-editor",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#say-hello-to-the-text-editor",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Say hello to the text-editor",
    "text": "Say hello to the text-editor\nWhen you write code in the console, it is gone.\nIt is better to work inside quarto notebooks in order to be able to save and share your code and results."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#articles",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#articles",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Articles",
    "text": "Articles"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#presentations",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#presentations",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Presentations",
    "text": "Presentations"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#websites",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#websites",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Websites",
    "text": "Websites"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#books",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#books",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Books",
    "text": "Books"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#quarto-render",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#quarto-render",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Quarto Render",
    "text": "Quarto Render\nQuarto is integrated into RStudio\nClick  in Editor pane of RStudio.\nRender input file to various document formats.\n\n\n\nInput\n\n*.qmd\n*.ipynb\n*.md\n*.Rmd\n\n\n\nFormat\n\nhtml\npdf\nrevealjs (like these slides!)\ndocx\nppt\nand many more!"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#quartos-code-chunk",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#quartos-code-chunk",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Quarto’s Code Chunk",
    "text": "Quarto’s Code Chunk\n\n\n\n\n```{r}\n#| echo: false\nrnorm(3)\n```\n\n\n\n\n\nThis is a Quarto Code Chunk."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#lets-explore-the-survey-data",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#lets-explore-the-survey-data",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Let’s explore the survey data",
    "text": "Let’s explore the survey data"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#quartos-code-chunk-1",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#quartos-code-chunk-1",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Quarto’s Code Chunk",
    "text": "Quarto’s Code Chunk\n\n\n\n\n```{r}\n#| echo: false\nrnorm(3)\n```\n\n\n\n\nThis is a Quarto Code Chunk.\nMake a new code chunk in three ways:\n\nType it out\ngot to Code -&gt; Insert Chunk on the top menu\nclick in your document and hit the key combination Alt+Ctrl+i\n\nWrite a math expression in a chunk and press the green arrow at the top-right of the chunk."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#execution-options",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#execution-options",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Execution Options",
    "text": "Execution Options\nControl how the code is executed with options.\nOptions are denoted with the “hash-pipe” #|\n\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\neval\nEvaluate the code chunk (if false, just echos the code into the output).\n\n\necho\nInclude the source code in output\n\n\noutput\nInclude the results of executing the code in the output (true, false, or asis to indicate that the output is raw markdown and should not have any of Quarto’s standard enclosing markdown).\n\n\nwarning\nInclude warnings in the output.\n\n\nerror\nInclude errors in the output.\n\n\ninclude\nCatch all for preventing any output (code or results) from being included (e.g. include: false suppresses all output from the code block)."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#example-figures-from-code",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#example-figures-from-code",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Example: Figures from Code",
    "text": "Example: Figures from Code\n\n\n```{r}\nlibrary(palmerpenguins)\nlibrary(ggplot2)\n\nggplot(penguins,\n       aes(x = bill_length_mm,\n           y = bill_depth_mm,\n           col = island)) +\n  geom_point()\n```"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#example-figures-from-code-1",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#example-figures-from-code-1",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Example: Figures from Code",
    "text": "Example: Figures from Code\n\n\n```{r}\n#| fig-width: 5\n#| fig-height: 3\n\nlibrary(palmerpenguins)\nlibrary(ggplot2)\n\nggplot(penguins,\n       aes(x = bill_length_mm,\n           y = bill_depth_mm,\n           col = island)) +\n  geom_point()\n```"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#section-2",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#section-2",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "",
    "text": "Markdown is designed to be easy to write and easy to read:\n\nA Markdown-formatted document should be publishable as-is, as plain text, without looking like it’s been marked up with tags or formatting instructions.\n-John Gruber\n\n\n\n\nQuarto uses extended version of Pandoc markdown.\nPandoc classifies markdown in terms of Inline and Block elements."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#inline-elements-text-formatting",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#inline-elements-text-formatting",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Inline Elements: Text Formatting",
    "text": "Inline Elements: Text Formatting\n\n\nMarkdown\nMarkdown allows you to format text\nwith *emphasis* and **strong emphasis**.\nYou can also add superscripts^2^, \nsubscripts~2~, and display code \n`verbatim`. Little known fact: you can \nalso ~~strikethrough~~ text and present\nit in [small caps]{.smallcaps}.\n\n\nOutput\nMarkdown allows you to format text with emphasis and strong emphasis. You can also add superscripts2, subscripts2, and display code verbatim. Little known fact: you can also strikethrough text and present it in small caps.\n\n\n\n1\n\nEither the * or _ can be used for emphasis and strong."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#inline-elements-links-and-images",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#inline-elements-links-and-images",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Inline Elements: Links and Images",
    "text": "Inline Elements: Links and Images\nMarkdown\nYou can embed [links with names](https://quarto.org/), direct urls\nlike &lt;https://quarto.org/&gt;, and links to \n[other places](#inline-elements-text-formatting) in the document. \nThe syntax is similar for embedding an inline image:\n![render icon](images/render-icon.png).\n\n\n\n\nOutput\nYou can embed links with names, direct urls like https://quarto.org/, and links to other places in the document. The syntax is similar for embedding an inline image: ."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#markdown-can-do-so-much-more",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#markdown-can-do-so-much-more",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Markdown can do so much more",
    "text": "Markdown can do so much more\nTo learn about footnotes, Math, tables, and diagrams, check out the quarto documentation on markdown\n\n\n\nMarkdown\nA short note.^[Fits inline.]\n\n|        |  1   |  2   |\n|--------|------|------|\n| **A**  | 0    | 0    |\n: example table {#tbl-1}\n\n$$\nf(x)={\\sqrt{\\frac{\\tau}{2\\pi}}}\n      e^{-\\tau (x-\\mu )^{2}/2}\n$$\n\n\nOutput\nA short note.1\n\n\n\nTable 1: example table\n\n\n\n\n\n\n1\n2\n\n\n\n\nA\n0\n0\n\n\n\n\n\n\n\\[\nf(x)=\\sqrt{\\frac{\\tau}{2\\pi}}\n    e^{-\\tau (x-\\mu )^{2}/2}\n\\]\n\n\nFits inline."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#metadata-yaml",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#metadata-yaml",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Metadata: YAML",
    "text": "Metadata: YAML\n“Yet Another Markup Language” or “YAML Ain’t Markup Language” is used to provide document level metadata …\n\n\n\n\n[… in key-value pairs,]\n[… that can nest,]\n[… are fussy about indentation,]\n[… and are kept between ---.]\n\n\n\n---\nformat: \n  title: \"Intro to R\"\n  author: \"Yours Truly\"\n  html:\n    toc: true\n    code-fold: true\n---\n\n\nThere are many options for front matter and configuring rendering."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#ok-but-how-do-you-write-code",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#ok-but-how-do-you-write-code",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Ok, but how do you write code?",
    "text": "Ok, but how do you write code?"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#assignment",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#assignment",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Assignment",
    "text": "Assignment\nYou can use &lt;- or = to assign values to variables\na &lt;- 6\nb = 8\nc &lt;- 5.44\nd = TRUE\ne = \"hello world\" \ne &lt;- 'hello world' # same as double quote\nWe will use &lt;- for all examples going forward, but do whichever melts your brain less"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#naming-variables",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#naming-variables",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Naming variables",
    "text": "Naming variables\nA lot of R people use . inside variable names, but in most languages besides R this would be an error. It’s good practice these days to use the _ underscore if you want separation in your variable names.\nr.people.sometimes.put.dots &lt;- TRUE\ndots.are.confusing &lt;- \"maybe\"\njust_use_underscores &lt;- \"please\""
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#functions",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#functions",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Functions",
    "text": "Functions\nFunctions are named bits of code that take parameters as input and return some output\n\nlibrary(tidyverse)\nword_1 &lt;- \"hello\"\nword_2 &lt;- \"world\"\nstr_c(word_1, word_2)\n\n[1] \"helloworld\"\n\n\nstr_c is a function that puts concatenates strings."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#functions-1",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#functions-1",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Functions",
    "text": "Functions\nFunctions are named bits of code that take parameters as input and return some output\n\nlibrary(tidyverse)\nword_1 &lt;- \"hello\"\nword_2 &lt;- \"world\"\nstr_c(word_1, word_2, sep = \" \")\n\n[1] \"hello world\"\n\n\nstr_c is a function that puts concatenates strings.\nfunctions can have named parameters as well as positional parameters."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#functions-2",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#functions-2",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Functions",
    "text": "Functions\nFunctions are named bits of code that take parameters as input and return some output\n\nlibrary(tidyverse)\nword_1 &lt;- \"hello\"\nword_2 &lt;- \"world\"\nstr_c(word_1, word_2, sep = \" \")\n\n[1] \"hello world\"\n\n\nstr_c is a function that puts concatenates strings.\nfunctions can have named parameters as well as positional parameters.\nnamed parameters always take an = sign for assignment."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#getting-help-with-functions",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#getting-help-with-functions",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Getting help with functions",
    "text": "Getting help with functions\nType ?str_c in the console to get a help page. check out this guide on how to read the R help pages.\nalso try googling str_c R tidyverse to get help.\nchatGPT and phind.com are really good at answering specific questions about R functions - not always correct but most of the time."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#everything-is-a-vector",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#everything-is-a-vector",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "“Everything is a vector”",
    "text": "“Everything is a vector”\nThis sounds like nonsense - let’s unpack:\n\nc(1, 2, 3, 4)\n\n[1] 1 2 3 4\n\n\n\nA Vector is a collection of values surrounded by c() and separated with ,"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#everything-is-a-vector-1",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#everything-is-a-vector-1",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "“Everything is a vector”",
    "text": "“Everything is a vector”\nThis sounds like nonsense - let’s unpack:\n\nc(1, 2, 3, 4) * 3\n\n[1]  3  6  9 12\n\n\n\nA vector is a collection of values surrounded by c() and separated with ,\nVectors in R do smart things with most functions and operations."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#everything-is-a-vector-2",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#everything-is-a-vector-2",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "“Everything is a vector”",
    "text": "“Everything is a vector”\nThis sounds like nonsense - let’s unpack:\n\nc(1, 2, 3, \"potato\") * 3\n\nError in c(1, 2, 3, \"potato\") * 3: non-numeric argument to binary operator\n\n\n\nA vector is a collection of values surrounded by c() and separated with ,\nVectors in R do smart things with most functions and operations.\nVectors have only one type of value."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#value-types-in-r",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#value-types-in-r",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Value types in R",
    "text": "Value types in R\nThe type of the value can be\n# numeric\nc(1,2,3,4) \n\n# character\nc(\"a\",\"b\",\"c\",\"d\")\n\n# boolean\nc(TRUE, FALSE)\n\n# factor\nc(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\") %&gt;% as_factor()"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#tibbles-aka-data-frames",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#tibbles-aka-data-frames",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "tibbles (aka data frames)",
    "text": "tibbles (aka data frames)\ntibbles are the big reason R is great for working with tabular data.\nA data frame is a rectangular collection of variables (in the columns) and observations (in the rows).\n\ntable_02\n\n# A tibble: 132 × 6\n   pid    time_point arm     nugent_score crp_blood    ph\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 pid_01 baseline   placebo            8      0.44   5.7\n 2 pid_01 week_1     placebo            7      1.66   5.2\n 3 pid_01 week_7     placebo            7      1.44   5.4\n 4 pid_02 baseline   placebo            7      1.55   5.2\n 5 pid_02 week_1     placebo            7      0.75   4.8\n 6 pid_02 week_7     placebo            4      1.17   4.2\n 7 pid_03 baseline   placebo            6      1.78   4.8\n 8 pid_03 week_1     placebo           10      0.57   5.3\n 9 pid_03 week_7     placebo            7      1.79   5.2\n10 pid_04 baseline   placebo            5      1.76   4.8\n# ℹ 122 more rows"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#exercise",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#exercise",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Exercise",
    "text": "Exercise\nThat’s enough slides for now time to try for yourself! Go to the module and go to the first exercise.\n\n\n\n−+\n30:00"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#how-to-read-in-data",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#how-to-read-in-data",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "How to read in data?",
    "text": "How to read in data?\nData is often in tables, and the easiest way to store tabular data is in csv or tsv format.\ncsv - comma separated values\ntsv - tab separated values\nto read in data stored this way use read_csv(filename) or read_tsv(filename)\ntable_02 &lt;- read_csv(\"02_visit_clinical_measurements_UKZN_workshop_2023.csv\")"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#but-first-the-pipe-operator",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#but-first-the-pipe-operator",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "But first: the pipe operator %>%",
    "text": "But first: the pipe operator %&gt;%"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#but-first-the-pipe-operator-1",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#but-first-the-pipe-operator-1",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "But first: the pipe operator %>%",
    "text": "But first: the pipe operator %&gt;%\n\n%&gt;% is pronounced “and then”"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#the-pipe-feeds-data-into-functions",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#the-pipe-feeds-data-into-functions",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "The pipe %>% feeds data into functions",
    "text": "The pipe %&gt;% feeds data into functions\n\n```{r}\nhead(table_02)\n```\n\n# A tibble: 6 × 6\n  pid    time_point arm     nugent_score crp_blood    ph\n  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 pid_01 baseline   placebo            8      0.44   5.7\n2 pid_01 week_1     placebo            7      1.66   5.2\n3 pid_01 week_7     placebo            7      1.44   5.4\n4 pid_02 baseline   placebo            7      1.55   5.2\n5 pid_02 week_1     placebo            7      0.75   4.8\n6 pid_02 week_7     placebo            4      1.17   4.2"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#the-pipe-feeds-data-into-functions-1",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#the-pipe-feeds-data-into-functions-1",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "The pipe %>% feeds data into functions",
    "text": "The pipe %&gt;% feeds data into functions\n\n```{r}\n# head(table_02)\ntable_02 %&gt;%\n  head()\n```\n\n# A tibble: 6 × 6\n  pid    time_point arm     nugent_score crp_blood    ph\n  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 pid_01 baseline   placebo            8      0.44   5.7\n2 pid_01 week_1     placebo            7      1.66   5.2\n3 pid_01 week_7     placebo            7      1.44   5.4\n4 pid_02 baseline   placebo            7      1.55   5.2\n5 pid_02 week_1     placebo            7      0.75   4.8\n6 pid_02 week_7     placebo            4      1.17   4.2"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#the-pipe-feeds-data-into-functions-2",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#the-pipe-feeds-data-into-functions-2",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "The pipe %>% feeds data into functions",
    "text": "The pipe %&gt;% feeds data into functions\n\n```{r}\nggplot(table_02, aes(crp_blood, ph, color = arm)) + geom_point()\n```"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#the-pipe-feeds-data-into-functions-3",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#the-pipe-feeds-data-into-functions-3",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "The pipe %>% feeds data into functions",
    "text": "The pipe %&gt;% feeds data into functions\n\n```{r}\ntable_02 %&gt;%\n  ggplot(aes(crp_blood, ph, color = arm)) + geom_point()\n```"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#since-r-4.1-native-pipe",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#since-r-4.1-native-pipe",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Since R 4.1: Native pipe |>",
    "text": "Since R 4.1: Native pipe |&gt;\n\n```{r}\ntable_02 |&gt;\n  ggplot(aes(crp_blood, ph, color = arm)) + geom_point()\n```"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#which-to-use-native-pipe-or-old-school-pipe",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#which-to-use-native-pipe-or-old-school-pipe",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Which to use? Native pipe or old-school pipe?",
    "text": "Which to use? Native pipe or old-school pipe?\n\n\n|&gt; is the future. If you can, use it.\n%&gt;% works on older installations. It’s the safe choice for now.\n\nWe use %&gt;% here because many people still run older R versions. Also, we’re old school."
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#pick-rows-from-a-table-filter",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#pick-rows-from-a-table-filter",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Pick rows from a table: filter()",
    "text": "Pick rows from a table: filter()"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#filter-only-placebo",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#filter-only-placebo",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Filter only placebo",
    "text": "Filter only placebo\n\n```{r}\ntable_02 %&gt;%\n  filter(arm == \"placebo\")\n```\n\n# A tibble: 69 × 6\n   pid    time_point arm     nugent_score crp_blood    ph\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 pid_01 baseline   placebo            8      0.44   5.7\n 2 pid_01 week_1     placebo            7      1.66   5.2\n 3 pid_01 week_7     placebo            7      1.44   5.4\n 4 pid_02 baseline   placebo            7      1.55   5.2\n 5 pid_02 week_1     placebo            7      0.75   4.8\n 6 pid_02 week_7     placebo            4      1.17   4.2\n 7 pid_03 baseline   placebo            6      1.78   4.8\n 8 pid_03 week_1     placebo           10      0.57   5.3\n 9 pid_03 week_7     placebo            7      1.79   5.2\n10 pid_04 baseline   placebo            5      1.76   4.8\n# ℹ 59 more rows"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#filter-out-samples-with-ph-4",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#filter-out-samples-with-ph-4",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Filter out samples with ph < 4",
    "text": "Filter out samples with ph &lt; 4\n\n```{r}\ntable_02 %&gt;%\n  filter(ph &lt; 4)\n```\n\n# A tibble: 39 × 6\n   pid    time_point arm       nugent_score crp_blood    ph\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 pid_05 week_1     treatment            3      0.19   3.2\n 2 pid_05 week_7     treatment            2      0.45   3.5\n 3 pid_09 week_1     treatment            3      0.27   3.6\n 4 pid_10 week_1     treatment            0      0.01   3.5\n 5 pid_10 week_7     treatment            1      2.87   2.9\n 6 pid_11 week_1     treatment            1      0.1    3.3\n 7 pid_15 week_1     treatment            3      0.84   3.4\n 8 pid_15 week_7     treatment            3      0.68   3.5\n 9 pid_16 week_1     treatment            0      0.03   3.7\n10 pid_16 week_7     treatment            2      0.5    3.2\n# ℹ 29 more rows"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#pick-columns-from-a-table-select",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#pick-columns-from-a-table-select",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Pick columns from a table: select()",
    "text": "Pick columns from a table: select()"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#pick-columns-pid-ph-and-nugent",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#pick-columns-pid-ph-and-nugent",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Pick columns pid, ph, and nugent",
    "text": "Pick columns pid, ph, and nugent\n\n```{r}\ntable_02 %&gt;%\n  select(pid, ph, nugent_score)\n```\n\n# A tibble: 132 × 3\n   pid       ph nugent_score\n   &lt;chr&gt;  &lt;dbl&gt;        &lt;dbl&gt;\n 1 pid_01   5.7            8\n 2 pid_01   5.2            7\n 3 pid_01   5.4            7\n 4 pid_02   5.2            7\n 5 pid_02   4.8            7\n 6 pid_02   4.2            4\n 7 pid_03   4.8            6\n 8 pid_03   5.3           10\n 9 pid_03   5.2            7\n10 pid_04   4.8            5\n# ℹ 122 more rows"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#rename-columns-and-subset-with-select",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#rename-columns-and-subset-with-select",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Rename columns and subset with select",
    "text": "Rename columns and subset with select\n\n```{r}\ntable_02 %&gt;%\n  select(participant_id = pid, ph, nugent_score)\n```\n\n# A tibble: 132 × 3\n   participant_id    ph nugent_score\n   &lt;chr&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n 1 pid_01           5.7            8\n 2 pid_01           5.2            7\n 3 pid_01           5.4            7\n 4 pid_02           5.2            7\n 5 pid_02           4.8            7\n 6 pid_02           4.2            4\n 7 pid_03           4.8            6\n 8 pid_03           5.3           10\n 9 pid_03           5.2            7\n10 pid_04           4.8            5\n# ℹ 122 more rows"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#sort-the-rows-in-a-table-arrange",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#sort-the-rows-in-a-table-arrange",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Sort the rows in a table: arrange()",
    "text": "Sort the rows in a table: arrange()"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#sort-samples-by-ph-ascending",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#sort-samples-by-ph-ascending",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Sort samples by ph ascending",
    "text": "Sort samples by ph ascending\n\n```{r}\ntable_02 %&gt;%\n  arrange(ph)\n```\n\n# A tibble: 132 × 6\n   pid    time_point arm       nugent_score crp_blood    ph\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 pid_31 week_7     treatment            2      1.36   2.8\n 2 pid_10 week_7     treatment            1      2.87   2.9\n 3 pid_28 baseline   treatment            3      0.67   2.9\n 4 pid_26 week_1     treatment            0      0.11   3  \n 5 pid_23 week_7     placebo              3      3.67   3.1\n 6 pid_40 baseline   treatment            3      1.48   3.1\n 7 pid_40 week_1     treatment            2      0.17   3.1\n 8 pid_05 week_1     treatment            3      0.19   3.2\n 9 pid_16 week_7     treatment            2      0.5    3.2\n10 pid_37 week_7     treatment            2      0.7    3.2\n# ℹ 122 more rows"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#sort-samples-by-ph-descending",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#sort-samples-by-ph-descending",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Sort samples by ph, descending",
    "text": "Sort samples by ph, descending\n\n```{r}\ntable_02 %&gt;%\n  arrange(desc(ph))\n```\n\n# A tibble: 132 × 6\n   pid    time_point arm       nugent_score crp_blood    ph\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 pid_29 baseline   placebo              7      2.39   5.8\n 2 pid_01 baseline   placebo              8      0.44   5.7\n 3 pid_16 baseline   treatment            6      1.91   5.7\n 4 pid_06 week_1     placebo              8      1.72   5.6\n 5 pid_26 baseline   treatment            7      0.94   5.6\n 6 pid_13 week_1     placebo              7      2.57   5.5\n 7 pid_23 week_1     placebo              8      0.8    5.5\n 8 pid_27 baseline   placebo              7      1.17   5.5\n 9 pid_01 week_7     placebo              7      1.44   5.4\n10 pid_04 week_7     placebo              7      5.68   5.4\n# ℹ 122 more rows"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#counting-things",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#counting-things",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Counting things",
    "text": "Counting things\nTo demonstrate counting, let’s switch to table_01\n\n```{r}\ntable_01\n```\n\n# A tibble: 44 × 6\n   pid    arm       smoker       age education                     sex  \n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                         &lt;lgl&gt;\n 1 pid_01 placebo   non-smoker    26 grade 10-12, matriculated     FALSE\n 2 pid_02 placebo   smoker        33 grade 10-12, matriculated     FALSE\n 3 pid_03 placebo   smoker        30 post-secondary                FALSE\n 4 pid_04 placebo   non-smoker    34 grade 10-12, not matriculated FALSE\n 5 pid_05 treatment non-smoker    29 grade 10-12, matriculated     FALSE\n 6 pid_06 placebo   smoker        34 post-secondary                FALSE\n 7 pid_07 placebo   non-smoker    31 grade 10-12, not matriculated FALSE\n 8 pid_08 placebo   smoker        30 grade 10-12, not matriculated FALSE\n 9 pid_09 treatment non-smoker    35 grade 10-12, not matriculated FALSE\n10 pid_10 treatment non-smoker    32 less than grade 9             FALSE\n# ℹ 34 more rows"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#counting-things-1",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#counting-things-1",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Counting things",
    "text": "Counting things\n\n```{r}\ntable_01 %&gt;%\n  count(smoker)\n```\n\n# A tibble: 2 × 2\n  smoker         n\n  &lt;chr&gt;      &lt;int&gt;\n1 non-smoker    27\n2 smoker        17"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#counting-things-2",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#counting-things-2",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Counting things",
    "text": "Counting things\n\n```{r}\ntable_01 %&gt;%\n  count(arm, smoker)\n```\n\n# A tibble: 4 × 3\n  arm       smoker         n\n  &lt;chr&gt;     &lt;chr&gt;      &lt;int&gt;\n1 placebo   non-smoker    12\n2 placebo   smoker        11\n3 treatment non-smoker    15\n4 treatment smoker         6"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#lets-take-a-poll-1",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#lets-take-a-poll-1",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Let’s take a poll",
    "text": "Let’s take a poll\nGo to the event on wooclap\n\nM2. Does filter get rid of rows that match TRUE, or keep rows that match TRUE?"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#use-the-pipe-to-build-analysis-pipelines",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#use-the-pipe-to-build-analysis-pipelines",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Use the pipe to build analysis pipelines",
    "text": "Use the pipe to build analysis pipelines\n\n```{r}\ntable_01 %&gt;%\n  filter(arm == \"placebo\")\n```\n\n# A tibble: 23 × 6\n   pid    arm     smoker       age education                     sex  \n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                         &lt;lgl&gt;\n 1 pid_01 placebo non-smoker    26 grade 10-12, matriculated     FALSE\n 2 pid_02 placebo smoker        33 grade 10-12, matriculated     FALSE\n 3 pid_03 placebo smoker        30 post-secondary                FALSE\n 4 pid_04 placebo non-smoker    34 grade 10-12, not matriculated FALSE\n 5 pid_06 placebo smoker        34 post-secondary                FALSE\n 6 pid_07 placebo non-smoker    31 grade 10-12, not matriculated FALSE\n 7 pid_08 placebo smoker        30 grade 10-12, not matriculated FALSE\n 8 pid_12 placebo non-smoker    31 grade 10-12, matriculated     FALSE\n 9 pid_13 placebo non-smoker    32 post-secondary                FALSE\n10 pid_14 placebo smoker        32 grade 10-12, matriculated     FALSE\n# ℹ 13 more rows"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#use-the-pipe-to-build-analysis-pipelines-1",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#use-the-pipe-to-build-analysis-pipelines-1",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Use the pipe to build analysis pipelines",
    "text": "Use the pipe to build analysis pipelines\n\n```{r}\ntable_01 %&gt;%\n  filter(age &lt; 30) %&gt;%\n  select(pid, arm, smoker)\n```\n\n# A tibble: 12 × 3\n   pid    arm       smoker    \n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;     \n 1 pid_01 placebo   non-smoker\n 2 pid_05 treatment non-smoker\n 3 pid_15 treatment non-smoker\n 4 pid_17 treatment non-smoker\n 5 pid_21 treatment non-smoker\n 6 pid_27 placebo   smoker    \n 7 pid_30 placebo   non-smoker\n 8 pid_31 treatment non-smoker\n 9 pid_35 placebo   non-smoker\n10 pid_36 placebo   non-smoker\n11 pid_41 treatment smoker    \n12 pid_44 treatment non-smoker"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#use-the-pipe-to-build-analysis-pipelines-2",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#use-the-pipe-to-build-analysis-pipelines-2",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Use the pipe to build analysis pipelines",
    "text": "Use the pipe to build analysis pipelines\n\n```{r}\ntable_01 %&gt;%\n  filter(age &lt; 30) %&gt;%\n  select(pid, arm, smoker) %&gt;%\n  count(arm, smoker)\n```\n\n# A tibble: 4 × 3\n  arm       smoker         n\n  &lt;chr&gt;     &lt;chr&gt;      &lt;int&gt;\n1 placebo   non-smoker     4\n2 placebo   smoker         1\n3 treatment non-smoker     6\n4 treatment smoker         1"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#adding-new-columns-to-a-table",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#adding-new-columns-to-a-table",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Adding new columns to a table",
    "text": "Adding new columns to a table"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#make-a-new-table-column-mutate",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#make-a-new-table-column-mutate",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Make a new table column: mutate()",
    "text": "Make a new table column: mutate()"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#example-c-reactive-protein",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#example-c-reactive-protein",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Example: C-reactive protein",
    "text": "Example: C-reactive protein\nThe crp_blood column is in units of mg/L. What if you needed it in ug/ul? What’s the calculation?\n\n```{r}\ntable_02 %&gt;%\n  select(pid, time_point, crp_blood)\n```\n\n# A tibble: 132 × 3\n   pid    time_point crp_blood\n   &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;\n 1 pid_01 baseline        0.44\n 2 pid_01 week_1          1.66\n 3 pid_01 week_7          1.44\n 4 pid_02 baseline        1.55\n 5 pid_02 week_1          0.75\n 6 pid_02 week_7          1.17\n 7 pid_03 baseline        1.78\n 8 pid_03 week_1          0.57\n 9 pid_03 week_7          1.79\n10 pid_04 baseline        1.76\n# ℹ 122 more rows"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#example-c-reactive-protein-1",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#example-c-reactive-protein-1",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Example: C-reactive protein",
    "text": "Example: C-reactive protein\nThe crp_blood column is in units of mg/L. What if you needed it in ug/ul? What’s the calculation?\nTo get ug/L you would multiply by 1000. To get ug/ul you need to then divide by 1000000\n\n```{r}\ntable_02 %&gt;%\n  select(pid, time_point, crp_blood)\n```\n\n# A tibble: 132 × 3\n   pid    time_point crp_blood\n   &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;\n 1 pid_01 baseline        0.44\n 2 pid_01 week_1          1.66\n 3 pid_01 week_7          1.44\n 4 pid_02 baseline        1.55\n 5 pid_02 week_1          0.75\n 6 pid_02 week_7          1.17\n 7 pid_03 baseline        1.78\n 8 pid_03 week_1          0.57\n 9 pid_03 week_7          1.79\n10 pid_04 baseline        1.76\n# ℹ 122 more rows"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#example-c-reactive-protein-2",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#example-c-reactive-protein-2",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Example: C-reactive protein",
    "text": "Example: C-reactive protein\nThe crp_blood column is in units of mg/L. What if you needed it in ug/ul? What’s the calculation?\nTo get ug/L you would multiply by 1000. To get ug/ul you need to then divide by 1000000\n\n```{r}\ntable_02 %&gt;%\n  select(pid, time_point, crp_blood) %&gt;%\n  mutate(crp_blood_ugul = crp_blood / 1000)\n```\n\n# A tibble: 132 × 4\n   pid    time_point crp_blood crp_blood_ugul\n   &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n 1 pid_01 baseline        0.44        0.00044\n 2 pid_01 week_1          1.66        0.00166\n 3 pid_01 week_7          1.44        0.00144\n 4 pid_02 baseline        1.55        0.00155\n 5 pid_02 week_1          0.75        0.00075\n 6 pid_02 week_7          1.17        0.00117\n 7 pid_03 baseline        1.78        0.00178\n 8 pid_03 week_1          0.57        0.00057\n 9 pid_03 week_7          1.79        0.00179\n10 pid_04 baseline        1.76        0.00176\n# ℹ 122 more rows"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#make-multiple-columns-at-once",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#make-multiple-columns-at-once",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Make multiple columns at once",
    "text": "Make multiple columns at once\n\n```{r}\ntable_02 %&gt;%\n  select(pid, time_point, crp_blood) %&gt;%\n  mutate(crp_blood_ugul = crp_blood / 1000,\n         crp_blood_ugl = crp_blood * 1000)\n```\n\n# A tibble: 132 × 5\n   pid    time_point crp_blood crp_blood_ugul crp_blood_ugl\n   &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n 1 pid_01 baseline        0.44        0.00044           440\n 2 pid_01 week_1          1.66        0.00166          1660\n 3 pid_01 week_7          1.44        0.00144          1440\n 4 pid_02 baseline        1.55        0.00155          1550\n 5 pid_02 week_1          0.75        0.00075           750\n 6 pid_02 week_7          1.17        0.00117          1170\n 7 pid_03 baseline        1.78        0.00178          1780\n 8 pid_03 week_1          0.57        0.00057           570\n 9 pid_03 week_7          1.79        0.00179          1790\n10 pid_04 baseline        1.76        0.00176          1760\n# ℹ 122 more rows"
  },
  {
    "objectID": "materials/1-workshop1/2-intro-to-r/slides.html#exercise-1",
    "href": "materials/1-workshop1/2-intro-to-r/slides.html#exercise-1",
    "title": "Intro to R,R Studio, and Quarto",
    "section": "Exercise",
    "text": "Exercise\nThat’s enough slides for now time to try for yourself! Go to the module and go to the second exercise.\n\n\n\n−+\n30:00\n\n\n\n\n\n\nback to module"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#section",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#section",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "",
    "text": "Workshop materials are at:\nhttps://elsherbini.github.io/durban-data-science-for-biology/"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#section-1",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#section-1",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "",
    "text": "Goals for this session\n\n\nGet the big picture of data visualization\nLearn how to wrangle data and make plots with the tidyverse\n\n\n\n\ndata wrangling (n.) - the art of taking data in one format and filtering, reshaping, and deriving values to make the data format you need."
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#discussions-discord",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#discussions-discord",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Discussions: discord",
    "text": "Discussions: discord\nAsk questions at #workshop-questions on https://discord.gg/UDAsYTzZE."
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#stickies",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#stickies",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Stickies",
    "text": "Stickies\n\n\n\n\n\n\nDuring an activity, place a yellow sticky on your laptop if you’re good to go and a pink sticky if you want help.\n\n\n\n\nImage by Megan Duffy"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#practicalities",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#practicalities",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Practicalities",
    "text": "Practicalities\n\nWiFi:\nNetwork: KTB Free Wifi (no password needed)\nNetwork AHRI Password: @hR1W1F1!17\nNetwork CAPRISA-Corp Password: corp@caprisa17\nBathrooms are out the lobby to your left"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#group-pen-and-paper-exercise",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#group-pen-and-paper-exercise",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Group Pen and Paper exercise",
    "text": "Group Pen and Paper exercise\n\n\n\n−+\n10:00\n\n\n\n\n−+\n30:00\n\n\n\nGet with your group. Go to the activity\n\nFor the first 10 minutes think on your own\nFor 30 minutes discuss with your group and produce at least one plot\nSomeone post a picture on the  #pen-and-paper-activity channel.\nDecide on one member of your group to present your plot (3 minute limit per group)"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#presentation",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#presentation",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Presentation",
    "text": "Presentation\nHave one member from your group present the plot to everyone! 3 minute limit!\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#plots-map-data-onto-graphical-elements.",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#plots-map-data-onto-graphical-elements.",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Plots map data onto graphical elements.",
    "text": "Plots map data onto graphical elements.\n\n\n\n\nTable 1: 02_visit_clinical_measurements_UKZN_workshop_2023.csv\n\n\n\n\n\n\npid\ntime_point\narm\nnugent_score\ncrp_blood\nph\n\n\n\n\npid_01\nbaseline\nplacebo\n8\n0.44\n5.7\n\n\npid_01\nweek_1\nplacebo\n7\n1.66\n5.2\n\n\npid_01\nweek_7\nplacebo\n7\n1.44\n5.4\n\n\npid_02\nbaseline\nplacebo\n7\n1.55\n5.2\n\n\npid_02\nweek_1\nplacebo\n7\n0.75\n4.8\n\n\npid_02\nweek_7\nplacebo\n4\n1.17\n4.2"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#ph-mapped-to-y-position",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#ph-mapped-to-y-position",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "pH mapped to y position",
    "text": "pH mapped to y position"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#ph-mapped-to-color",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#ph-mapped-to-color",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "pH mapped to color",
    "text": "pH mapped to color"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#commonly-used-aesthetics",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#commonly-used-aesthetics",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Commonly used aesthetics",
    "text": "Commonly used aesthetics\n\nFigure from  Claus O. Wilke. Fundamentals of Data Visualization. O’Reilly, 2019"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#the-same-data-values-can-be-mapped-to-different-aesthetics",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#the-same-data-values-can-be-mapped-to-different-aesthetics",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "The same data values can be mapped to different aesthetics",
    "text": "The same data values can be mapped to different aesthetics\n\nFigure from  Claus O. Wilke. Fundamentals of Data Visualization. O’Reilly, 2019"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#we-can-use-many-different-aesthetics-at-once",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#we-can-use-many-different-aesthetics-at-once",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "We can use many different aesthetics at once",
    "text": "We can use many different aesthetics at once"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#we-define-the-mapping-with-aes",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#we-define-the-mapping-with-aes",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "We define the mapping with aes()",
    "text": "We define the mapping with aes()\n\n```{r}\ntable_02 %&gt;%\n  ggplot(mapping = aes(x = time_point, y = ph, color = ph)) +\n  geom_jitter()\n```"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#we-frequently-omit-argument-names",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#we-frequently-omit-argument-names",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "We frequently omit argument names",
    "text": "We frequently omit argument names\nLong form, all arguments are named:\n\n```{r}\n#| eval: false\n\nggplot(\n  data= table_02,\n  mapping = aes(x = time_point, y = ph, color = ph)\n) +\n  geom_jitter()\n```"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#we-frequently-omit-argument-names-1",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#we-frequently-omit-argument-names-1",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "We frequently omit argument names",
    "text": "We frequently omit argument names\nAbbreviated form, common arguments remain unnamed:\n\n```{r}\n#| eval: false\n\nggplot(table_02, aes(x = time_point, y = ph, color = ph)) +\n  geom_jitter()\n```"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#the-geom-determines-how-the-data-is-shown",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#the-geom-determines-how-the-data-is-shown",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "The geom determines how the data is shown",
    "text": "The geom determines how the data is shown\n\n```{r}\nggplot(table_02, aes(x = time_point, y = ph, color = ph)) +\n  geom_point()\n```"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#the-geom-determines-how-the-data-is-shown-1",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#the-geom-determines-how-the-data-is-shown-1",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "The geom determines how the data is shown",
    "text": "The geom determines how the data is shown\n\n```{r}\nggplot(table_02, aes(x = time_point, y = ph, color = ph)) +\n  geom_boxplot()\n```"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#the-geom-determines-how-the-data-is-shown-2",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#the-geom-determines-how-the-data-is-shown-2",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "The geom determines how the data is shown",
    "text": "The geom determines how the data is shown\n\n```{r}\nggplot(table_02, aes(x = time_point, y = ph, color = ph)) +\n  geom_jitter()\n```"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#different-geoms-have-parameters-for-control",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#different-geoms-have-parameters-for-control",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Different geoms have parameters for control",
    "text": "Different geoms have parameters for control\n\n```{r}\nggplot(table_02, aes(x = time_point, y = ph, color = ph)) +\n  geom_jitter(size=3)\n```"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#different-geoms-have-parameters-for-control-1",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#different-geoms-have-parameters-for-control-1",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Different geoms have parameters for control",
    "text": "Different geoms have parameters for control\n\n```{r}\nggplot(table_02, aes(x = time_point, y = ph, color = ph)) +\n  geom_jitter(size=3, width = 0.2)\n```"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#important-color-and-fill-apply-to-different-elements",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#important-color-and-fill-apply-to-different-elements",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Important: color and fill apply to different elements",
    "text": "Important: color and fill apply to different elements\ncolor Applies color to points, lines, text, borders\nfill Applies color to any filled areas"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#many-geoms-have-both-color-and-fill-aesthetics",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#many-geoms-have-both-color-and-fill-aesthetics",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Many geoms have both color and fill aesthetics",
    "text": "Many geoms have both color and fill aesthetics\n\n\n```{r}\n#| output-location: column\nggplot(\n  data = table_02,\n  mapping = aes(\n    x = time_point,\n    y = ph,\n    color = time_point\n  )\n) + geom_boxplot()\n```"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#many-geoms-have-both-color-and-fill-aesthetics-1",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#many-geoms-have-both-color-and-fill-aesthetics-1",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Many geoms have both color and fill aesthetics",
    "text": "Many geoms have both color and fill aesthetics\n\n\n```{r}\n#| output-location: column\nggplot(\n  data = table_02,\n  mapping = aes(\n    x = time_point,\n    y = ph,\n    fill = time_point\n  )\n) + geom_boxplot()\n```"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#many-geoms-have-both-color-and-fill-aesthetics-2",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#many-geoms-have-both-color-and-fill-aesthetics-2",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Many geoms have both color and fill aesthetics",
    "text": "Many geoms have both color and fill aesthetics\n\n\n```{r}\n#| output-location: column\nggplot(\n  data = table_02,\n  mapping = aes(\n    x = time_point,\n    y = ph,\n    fill = time_point,\n    color = time_point\n  )\n) + geom_boxplot()\n```"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#aesthetics-can-also-be-used-as-parameters-in-geoms",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#aesthetics-can-also-be-used-as-parameters-in-geoms",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Aesthetics can also be used as parameters in geoms",
    "text": "Aesthetics can also be used as parameters in geoms\n\n\n```{r}\n#| output-location: column\nggplot(\n  data = table_02,\n  mapping = aes(\n    x = time_point,\n    y = ph\n  )\n) + geom_boxplot()\n```"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#aesthetics-can-also-be-used-as-parameters-in-geoms-1",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#aesthetics-can-also-be-used-as-parameters-in-geoms-1",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Aesthetics can also be used as parameters in geoms",
    "text": "Aesthetics can also be used as parameters in geoms\n\n\n```{r}\n#| output-location: column\nggplot(\n  data = table_02,\n  mapping = aes(\n    x = time_point,\n    y = ph\n  )\n) + geom_boxplot(fill=\"orange\")\n```"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#exercise",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#exercise",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Exercise",
    "text": "Exercise\n\n\n\n−+\n30:00\n\n\n\nTime to try it yourself. Go to the first coding exercise.\n\nDuring an activity, place a blue sticky on your laptop if you’re good to go and a pink sticky if you want help."
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#we-often-encounter-datasets-containing-simple-amounts",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#we-often-encounter-datasets-containing-simple-amounts",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "We often encounter datasets containing simple amounts",
    "text": "We often encounter datasets containing simple amounts\nExample: Highest grossing movies 2023 to date\n\n\n\n\n\n\nrank\ntitle\namount\n\n\n\n\n1\nBarbie\n1437.8\n\n\n2\nThe Super Mario Bros Movie\n1361.9\n\n\n3\nOppenheimer\n939.3\n\n\n4\nGuardians of the Galaxy 3\n845.5\n\n\n5\nThe Little Mermaid\n569.6\n\n\n\n\n\n\n\n\nMillions USD. Data source: Box Office Mojo"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#we-can-visualize-amounts-with-bar-plots",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#we-can-visualize-amounts-with-bar-plots",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "We can visualize amounts with bar plots",
    "text": "We can visualize amounts with bar plots"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#bars-can-also-run-horizontally",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#bars-can-also-run-horizontally",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Bars can also run horizontally",
    "text": "Bars can also run horizontally"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#avoid-rotated-axis-labels",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#avoid-rotated-axis-labels",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Avoid rotated axis labels",
    "text": "Avoid rotated axis labels"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#avoid-rotated-axis-labels---flip-the-axes",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#avoid-rotated-axis-labels---flip-the-axes",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Avoid rotated axis labels - flip the axes!",
    "text": "Avoid rotated axis labels - flip the axes!"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#pay-attention-to-the-order-of-the-bars",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#pay-attention-to-the-order-of-the-bars",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Pay attention to the order of the bars",
    "text": "Pay attention to the order of the bars"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#pay-attention-to-the-order-of-the-bars-1",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#pay-attention-to-the-order-of-the-bars-1",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Pay attention to the order of the bars",
    "text": "Pay attention to the order of the bars"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#we-can-use-dots-instead-of-bars",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#we-can-use-dots-instead-of-bars",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "We can use dots instead of bars",
    "text": "We can use dots instead of bars"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#dots-are-preferable-if-we-want-to-truncate-the-axes",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#dots-are-preferable-if-we-want-to-truncate-the-axes",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Dots are preferable if we want to truncate the axes",
    "text": "Dots are preferable if we want to truncate the axes"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#dots-are-preferable-if-we-want-to-truncate-the-axes-1",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#dots-are-preferable-if-we-want-to-truncate-the-axes-1",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Dots are preferable if we want to truncate the axes",
    "text": "Dots are preferable if we want to truncate the axes\n\nbar lengths do not accurately represent the data values"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#dots-are-preferable-if-we-want-to-truncate-the-axes-2",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#dots-are-preferable-if-we-want-to-truncate-the-axes-2",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Dots are preferable if we want to truncate the axes",
    "text": "Dots are preferable if we want to truncate the axes\n\nkey features of the data are obscured"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#dots-are-preferable-if-we-want-to-truncate-the-axes-3",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#dots-are-preferable-if-we-want-to-truncate-the-axes-3",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Dots are preferable if we want to truncate the axes",
    "text": "Dots are preferable if we want to truncate the axes"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#lets-take-a-poll",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#lets-take-a-poll",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Let’s take a poll",
    "text": "Let’s take a poll\nGo to the event on wooclap\n\nM3. Do you think it makes sense to truncate the axes for the life expectancy data?"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#we-use-grouped-bars-for-higher-dimensional-datasets",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#we-use-grouped-bars-for-higher-dimensional-datasets",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "We use grouped bars for higher-dimensional datasets",
    "text": "We use grouped bars for higher-dimensional datasets"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#we-are-free-to-choose-by-which-variable-to-group",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#we-are-free-to-choose-by-which-variable-to-group",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "We are free to choose by which variable to group",
    "text": "We are free to choose by which variable to group"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#we-can-also-use-multiple-plot-panels-facets",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#we-can-also-use-multiple-plot-panels-facets",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "We can also use multiple plot panels (facets)",
    "text": "We can also use multiple plot panels (facets)"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#the-simple-dataset",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#the-simple-dataset",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "The simple dataset",
    "text": "The simple dataset\n\n# Data from Box Office Mojo for 2023. \nboxoffice &lt;- tibble(\n  rank = 1:5,\n  title = c(\"Barbie\", \"The Super Mario Bros Movie\", \"Oppenheimer\", \"Guardians of the Galaxy 3\", \"The Little Mermaid\"),\n  amount = c(1437.8, 1361.9, 939.3, 845.5, 569.6) # million USD\n)\n\n\n\n\n\n\n\nrank\ntitle\namount\n\n\n\n\n1\nBarbie\n1437.8\n\n\n2\nThe Super Mario Bros Movie\n1361.9\n\n\n3\nOppenheimer\n939.3\n\n\n4\nGuardians of the Galaxy 3\n845.5\n\n\n5\nThe Little Mermaid\n569.6"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#visualize-as-a-bar-plot",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#visualize-as-a-bar-plot",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Visualize as a bar plot",
    "text": "Visualize as a bar plot\n\nggplot(boxoffice, aes(title, amount)) +\n  geom_col()  # \"col\" stands for column"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#order-by-data-value",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#order-by-data-value",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Order by data value",
    "text": "Order by data value\n\nggplot(boxoffice, aes(fct_reorder(title, amount), amount)) +\n  geom_col()"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#order-by-data-value-descending",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#order-by-data-value-descending",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Order by data value, descending",
    "text": "Order by data value, descending\n\nggplot(boxoffice, aes(fct_reorder(title, -amount), amount)) +\n  geom_col() + \n  xlab(NULL) # remove x axis label"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#flip-x-and-y-set-custom-x-axis-label",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#flip-x-and-y-set-custom-x-axis-label",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Flip x and y, set custom x axis label",
    "text": "Flip x and y, set custom x axis label\n\nggplot(boxoffice, aes(amount, fct_reorder(title, amount))) +\n  geom_col() +\n  xlab(\"amount (in million USD)\") +\n  ylab(NULL)"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#sometimes-we-need-to-count-before-visualization",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#sometimes-we-need-to-count-before-visualization",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Sometimes we need to count before visualization",
    "text": "Sometimes we need to count before visualization\n\nlibrary(here)\nlibrary(tidyverse)\n\ntable_02 &lt;- read_csv(here(\"datasets/instructional_dataset/02_visit_clinical_measurements_UKZN_workshop_2023.csv\")) %&gt;%\n  mutate(nugent_score = as_factor(nugent_score))\n\n\n\n\n\n\n\npid\ntime_point\narm\nnugent_score\ncrp_blood\nph\n\n\n\n\npid_01\nbaseline\nplacebo\n8\n0.44\n5.7\n\n\npid_01\nweek_1\nplacebo\n7\n1.66\n5.2\n\n\npid_01\nweek_7\nplacebo\n7\n1.44\n5.4\n\n\npid_02\nbaseline\nplacebo\n7\n1.55\n5.2\n\n\npid_02\nweek_1\nplacebo\n7\n0.75\n4.8\n\n\npid_02\nweek_7\nplacebo\n4\n1.17\n4.2\n\n\npid_03\nbaseline\nplacebo\n6\n1.78\n4.8\n\n\npid_03\nweek_1\nplacebo\n10\n0.57\n5.3\n\n\npid_03\nweek_7\nplacebo\n7\n1.79\n5.2\n\n\npid_04\nbaseline\nplacebo\n5\n1.76\n4.8\n\n\npid_04\nweek_1\nplacebo\n9\n2.58\n5.1\n\n\npid_04\nweek_7\nplacebo\n7\n5.68\n5.4\n\n\npid_05\nbaseline\ntreatment\n8\n0.95\n4.9\n\n\npid_05\nweek_1\ntreatment\n3\n0.19\n3.2\n\n\npid_05\nweek_7\ntreatment\n2\n0.45\n3.5\n\n\npid_06\nbaseline\nplacebo\n10\n4.03\n5.3\n\n\npid_06\nweek_1\nplacebo\n8\n1.72\n5.6\n\n\npid_06\nweek_7\nplacebo\n8\n3.19\n5.0\n\n\npid_07\nbaseline\nplacebo\n7\n0.10\n5.2\n\n\npid_07\nweek_1\nplacebo\n7\n1.36\n4.9\n\n\npid_07\nweek_7\nplacebo\n5\n0.38\n5.1\n\n\npid_08\nbaseline\nplacebo\n9\n3.18\n5.4\n\n\npid_08\nweek_1\nplacebo\n5\n1.55\n4.8\n\n\npid_08\nweek_7\nplacebo\n7\n1.77\n5.0\n\n\npid_09\nbaseline\ntreatment\n5\n2.13\n4.9\n\n\npid_09\nweek_1\ntreatment\n3\n0.27\n3.6\n\n\npid_09\nweek_7\ntreatment\n4\n1.04\n4.2\n\n\npid_10\nbaseline\ntreatment\n8\n0.98\n4.9\n\n\npid_10\nweek_1\ntreatment\n0\n0.01\n3.5\n\n\npid_10\nweek_7\ntreatment\n1\n2.87\n2.9\n\n\npid_11\nbaseline\ntreatment\n7\n0.31\n5.0\n\n\npid_11\nweek_1\ntreatment\n1\n0.10\n3.3\n\n\npid_11\nweek_7\ntreatment\n4\n1.15\n5.1\n\n\npid_12\nbaseline\nplacebo\n8\n2.42\n5.0\n\n\npid_12\nweek_1\nplacebo\n6\n0.64\n4.5\n\n\npid_12\nweek_7\nplacebo\n9\n4.36\n5.2\n\n\npid_13\nbaseline\nplacebo\n8\n2.69\n5.1\n\n\npid_13\nweek_1\nplacebo\n7\n2.57\n5.5\n\n\npid_13\nweek_7\nplacebo\n8\n1.98\n4.8\n\n\npid_14\nbaseline\nplacebo\n7\n0.34\n5.3\n\n\npid_14\nweek_1\nplacebo\n5\n2.07\n4.2\n\n\npid_14\nweek_7\nplacebo\n7\n5.06\n5.1\n\n\npid_15\nbaseline\ntreatment\n7\n0.29\n4.8\n\n\npid_15\nweek_1\ntreatment\n3\n0.84\n3.4\n\n\npid_15\nweek_7\ntreatment\n3\n0.68\n3.5\n\n\npid_16\nbaseline\ntreatment\n6\n1.91\n5.7\n\n\npid_16\nweek_1\ntreatment\n0\n0.03\n3.7\n\n\npid_16\nweek_7\ntreatment\n2\n0.50\n3.2\n\n\npid_17\nbaseline\ntreatment\n5\n1.39\n4.8\n\n\npid_17\nweek_1\ntreatment\n2\n0.00\n3.3\n\n\npid_17\nweek_7\ntreatment\n3\n0.90\n3.7\n\n\npid_18\nbaseline\ntreatment\n6\n0.45\n4.3\n\n\npid_18\nweek_1\ntreatment\n1\n1.81\n3.6\n\n\npid_18\nweek_7\ntreatment\n6\n0.41\n3.9\n\n\npid_19\nbaseline\nplacebo\n7\n1.34\n5.3\n\n\npid_19\nweek_1\nplacebo\n5\n2.91\n4.3\n\n\npid_19\nweek_7\nplacebo\n5\n1.27\n4.5\n\n\npid_20\nbaseline\nplacebo\n4\n0.86\n4.3\n\n\npid_20\nweek_1\nplacebo\n8\n1.45\n5.2\n\n\npid_20\nweek_7\nplacebo\n5\n3.95\n4.9\n\n\npid_21\nbaseline\ntreatment\n5\n0.50\n4.6\n\n\npid_21\nweek_1\ntreatment\n1\n1.60\n3.4\n\n\npid_21\nweek_7\ntreatment\n4\n1.23\n4.8\n\n\npid_22\nbaseline\ntreatment\n6\n1.10\n4.0\n\n\npid_22\nweek_1\ntreatment\n3\n0.58\n4.2\n\n\npid_22\nweek_7\ntreatment\n6\n1.67\n5.1\n\n\npid_23\nbaseline\nplacebo\n8\n0.99\n5.4\n\n\npid_23\nweek_1\nplacebo\n8\n0.80\n5.5\n\n\npid_23\nweek_7\nplacebo\n3\n3.67\n3.1\n\n\npid_24\nbaseline\nplacebo\n5\n4.91\n3.8\n\n\npid_24\nweek_1\nplacebo\n7\n0.94\n5.1\n\n\npid_24\nweek_7\nplacebo\n4\n1.03\n4.5\n\n\npid_25\nbaseline\ntreatment\n3\n2.84\n3.9\n\n\npid_25\nweek_1\ntreatment\n4\n3.52\n4.7\n\n\npid_25\nweek_7\ntreatment\n2\n0.49\n3.7\n\n\npid_26\nbaseline\ntreatment\n7\n0.94\n5.6\n\n\npid_26\nweek_1\ntreatment\n0\n0.11\n3.0\n\n\npid_26\nweek_7\ntreatment\n4\n0.29\n4.8\n\n\npid_27\nbaseline\nplacebo\n7\n1.17\n5.5\n\n\npid_27\nweek_1\nplacebo\n5\n1.62\n4.7\n\n\npid_27\nweek_7\nplacebo\n8\n0.76\n4.7\n\n\npid_28\nbaseline\ntreatment\n3\n0.67\n2.9\n\n\npid_28\nweek_1\ntreatment\n1\n0.05\n3.3\n\n\npid_28\nweek_7\ntreatment\n1\n0.22\n3.5\n\n\npid_29\nbaseline\nplacebo\n7\n2.39\n5.8\n\n\npid_29\nweek_1\nplacebo\n4\n4.09\n4.5\n\n\npid_29\nweek_7\nplacebo\n3\n3.13\n3.5\n\n\npid_30\nbaseline\nplacebo\n7\n0.85\n4.8\n\n\npid_30\nweek_1\nplacebo\n8\n2.56\n5.1\n\n\npid_30\nweek_7\nplacebo\n7\n1.62\n5.2\n\n\npid_31\nbaseline\ntreatment\n6\n1.78\n4.4\n\n\npid_31\nweek_1\ntreatment\n2\n0.41\n3.5\n\n\npid_31\nweek_7\ntreatment\n2\n1.36\n2.8\n\n\npid_32\nbaseline\ntreatment\n5\n4.83\n4.9\n\n\npid_32\nweek_1\ntreatment\n1\n0.03\n3.3\n\n\npid_32\nweek_7\ntreatment\n3\n0.21\n3.8\n\n\npid_33\nbaseline\ntreatment\n6\n5.26\n4.6\n\n\npid_33\nweek_1\ntreatment\n1\n0.07\n3.6\n\n\npid_33\nweek_7\ntreatment\n2\n1.92\n3.3\n\n\npid_34\nbaseline\nplacebo\n8\n3.16\n5.4\n\n\npid_34\nweek_1\nplacebo\n4\n1.12\n4.7\n\n\npid_34\nweek_7\nplacebo\n7\n2.34\n5.3\n\n\npid_35\nbaseline\nplacebo\n8\n0.74\n5.3\n\n\npid_35\nweek_1\nplacebo\n5\n0.16\n4.4\n\n\npid_35\nweek_7\nplacebo\n3\n1.97\n3.9\n\n\npid_36\nbaseline\nplacebo\n8\n1.21\n5.1\n\n\npid_36\nweek_1\nplacebo\n5\n2.28\n4.3\n\n\npid_36\nweek_7\nplacebo\n8\n1.10\n4.8\n\n\npid_37\nbaseline\ntreatment\n5\n1.16\n4.8\n\n\npid_37\nweek_1\ntreatment\n1\n0.07\n3.6\n\n\npid_37\nweek_7\ntreatment\n2\n0.70\n3.2\n\n\npid_38\nbaseline\nplacebo\n8\n0.41\n5.1\n\n\npid_38\nweek_1\nplacebo\n5\n1.55\n4.8\n\n\npid_38\nweek_7\nplacebo\n4\n3.22\n4.5\n\n\npid_39\nbaseline\ntreatment\n6\n1.61\n4.6\n\n\npid_39\nweek_1\ntreatment\n2\n0.09\n3.6\n\n\npid_39\nweek_7\ntreatment\n5\n0.77\n4.7\n\n\npid_40\nbaseline\ntreatment\n3\n1.48\n3.1\n\n\npid_40\nweek_1\ntreatment\n2\n0.17\n3.1\n\n\npid_40\nweek_7\ntreatment\n6\n0.21\n4.5\n\n\npid_41\nbaseline\ntreatment\n4\n1.51\n4.3\n\n\npid_41\nweek_1\ntreatment\n2\n0.64\n3.4\n\n\npid_41\nweek_7\ntreatment\n4\n0.78\n4.4\n\n\npid_42\nbaseline\nplacebo\n6\n0.91\n4.7\n\n\npid_42\nweek_1\nplacebo\n5\n0.88\n4.3\n\n\npid_42\nweek_7\nplacebo\n7\n3.06\n5.3\n\n\npid_43\nbaseline\nplacebo\n6\n1.08\n4.7\n\n\npid_43\nweek_1\nplacebo\n6\n0.94\n4.1\n\n\npid_43\nweek_7\nplacebo\n6\n1.79\n4.1\n\n\npid_44\nbaseline\ntreatment\n6\n0.48\n4.4\n\n\npid_44\nweek_1\ntreatment\n1\n1.67\n3.5\n\n\npid_44\nweek_7\ntreatment\n3\n0.60\n3.4"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#goal-visualize-number-of-people-with-different-nugent-scores",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#goal-visualize-number-of-people-with-different-nugent-scores",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Goal: Visualize number of people with different nugent scores",
    "text": "Goal: Visualize number of people with different nugent scores"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#use-geom_bar-to-count-before-plotting",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#use-geom_bar-to-count-before-plotting",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Use geom_bar() to count before plotting",
    "text": "Use geom_bar() to count before plotting\n\ntable_02 %&gt;%\n  ggplot(aes(y=nugent_score))+\n  geom_bar()"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#getting-the-bars-into-the-right-order",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#getting-the-bars-into-the-right-order",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Getting the bars into the right order",
    "text": "Getting the bars into the right order\n\ntable_01 %&gt;%\n  ggplot(aes(y=education))+\n  geom_bar()"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#getting-the-bars-into-the-right-order-1",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#getting-the-bars-into-the-right-order-1",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Getting the bars into the right order",
    "text": "Getting the bars into the right order\n\neducation_order &lt;- c(\"less than grade 9\",\"grade 10-12, not matriculated\",\"grade 10-12, matriculated\",\"post-secondary\")\ntable_01 %&gt;%\n  mutate(education = fct_relevel(education, education_order)) %&gt;%\n  ggplot(aes(y=education))+\n  geom_bar()"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#display-counts-by-smoking-and-education",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#display-counts-by-smoking-and-education",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Display counts by smoking and education",
    "text": "Display counts by smoking and education\n\ntable_01 %&gt;%\n  mutate(education = fct_relevel(education, education_order)) %&gt;%\n  ggplot(aes(y=education, fill=smoker))+\n  geom_bar()"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#positions-define-how-subgroups-are-shown",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#positions-define-how-subgroups-are-shown",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Positions define how subgroups are shown",
    "text": "Positions define how subgroups are shown\nposition = \"dodge\": Place bars for subgroups side-by-side\n\ntable_01 %&gt;%\n  mutate(education = fct_relevel(education, education_order)) %&gt;%\n  ggplot(aes(y=education, fill=smoker))+\n  geom_bar(position = \"dodge\")"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#positions-define-how-subgroups-are-shown-1",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#positions-define-how-subgroups-are-shown-1",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Positions define how subgroups are shown",
    "text": "Positions define how subgroups are shown\nposition = \"stack\": Place bars for subgroups on top of each other\n\ntable_01 %&gt;%\n  mutate(education = fct_relevel(education, education_order)) %&gt;%\n  ggplot(aes(y=education, fill=smoker))+\n  geom_bar(position = \"stack\")"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#positions-define-how-subgroups-are-shown-2",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#positions-define-how-subgroups-are-shown-2",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Positions define how subgroups are shown",
    "text": "Positions define how subgroups are shown\nposition = \"fill\": Like \"stack\", but scale to 100%\n\ntable_01 %&gt;%\n  mutate(education = fct_relevel(education, education_order)) %&gt;%\n  ggplot(aes(y=education, fill=smoker))+\n  geom_bar(position = \"fill\")"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#lets-take-a-poll-1",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#lets-take-a-poll-1",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Let’s take a poll",
    "text": "Let’s take a poll\nGo to the event on wooclap\n\n2 questions: M3. What’s the difference between geom_col and geom_bar? and M3. What patterns did you see in the smoker CRP data (slide 49)?"
  },
  {
    "objectID": "materials/1-workshop1/3-tidyverse-101/slides.html#exercise-1",
    "href": "materials/1-workshop1/3-tidyverse-101/slides.html#exercise-1",
    "title": "Intro to data visualization and data wrangling with the tidyverse",
    "section": "Exercise",
    "text": "Exercise\n\n\n\n−+\n30:00\n\n\n\nTime to try it yourself. Go to back to the module.\n\nDuring an activity, place a yellow sticky on your laptop if you’re good to go and a pink sticky if you want help.\n\n\n\nback to module"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#section",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#section",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "",
    "text": "Workshop materials are at:\nhttps://elsherbini.github.io/durban-data-science-for-biology/"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#section-1",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#section-1",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "",
    "text": "Goals for this session\n\n\nLearn more advanced table commands\nLearn about plotting distributions with the tidyverse\n\n\n\n\ndata wrangling (n.) - the art of taking data in one format and filtering, reshaping, and deriving values to make the data format you need."
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#discussions-discord",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#discussions-discord",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Discussions: discord",
    "text": "Discussions: discord\nAsk questions at #workshop-questions on https://discord.gg/UDAsYTzZE."
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#stickies",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#stickies",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Stickies",
    "text": "Stickies\n\n\n\n\n\n\nDuring an activity, place a yellow sticky on your laptop if you’re good to go and a pink sticky if you want help.\n\n\n\n\nImage by Megan Duffy"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#practicalities",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#practicalities",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Practicalities",
    "text": "Practicalities\n\nWiFi:\nNetwork: KTB Free Wifi (no password needed)\nNetwork AHRI Password: @hR1W1F1!17\nNetwork CAPRISA-Corp Password: corp@caprisa17\nBathrooms are out the lobby to your left"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#elementary-data-manipulations",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#elementary-data-manipulations",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Elementary data manipulations",
    "text": "Elementary data manipulations\n\n\nYesterday:\n\nPick rows: filter()\nPick columns: select()\nSort rows: arrange()\nCount things: count()\nMake new columns: mutate()\n\n\nToday:\n\nAnalyze subsets:group_by() and summarize()\nReshape:pivot_wider(), pivot_longer()\nCombine datasets:left_join(), inner_join(), ..."
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#analyze-subsets-group_by-and-summarize",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#analyze-subsets-group_by-and-summarize",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Analyze subsets: group_by() and summarize()",
    "text": "Analyze subsets: group_by() and summarize()"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#example-application-of-grouping-counting",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#example-application-of-grouping-counting",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Example application of grouping: Counting",
    "text": "Example application of grouping: Counting\nPreviously, we counted like so:\n\n```{r}\ntable_01 %&gt;%\n  count(smoker)\n```\n\n# A tibble: 2 × 2\n  smoker         n\n  &lt;chr&gt;      &lt;int&gt;\n1 non-smoker    27\n2 smoker        17\n\n\n\nNow let’s do it the hard way"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#example-application-of-grouping-counting-1",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#example-application-of-grouping-counting-1",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Example application of grouping: Counting",
    "text": "Example application of grouping: Counting\nLet’s go back to the original table\n\n```{r}\ntable_01\n```\n\n# A tibble: 44 × 6\n   pid    arm       smoker       age education                     sex  \n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                         &lt;lgl&gt;\n 1 pid_01 placebo   non-smoker    26 grade 10-12, matriculated     FALSE\n 2 pid_02 placebo   smoker        33 grade 10-12, matriculated     FALSE\n 3 pid_03 placebo   smoker        30 post-secondary                FALSE\n 4 pid_04 placebo   non-smoker    34 grade 10-12, not matriculated FALSE\n 5 pid_05 treatment non-smoker    29 grade 10-12, matriculated     FALSE\n 6 pid_06 placebo   smoker        34 post-secondary                FALSE\n 7 pid_07 placebo   non-smoker    31 grade 10-12, not matriculated FALSE\n 8 pid_08 placebo   smoker        30 grade 10-12, not matriculated FALSE\n 9 pid_09 treatment non-smoker    35 grade 10-12, not matriculated FALSE\n10 pid_10 treatment non-smoker    32 less than grade 9             FALSE\n# ℹ 34 more rows"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#example-application-of-grouping-counting-2",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#example-application-of-grouping-counting-2",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Example application of grouping: Counting",
    "text": "Example application of grouping: Counting\nThen we group the data\n\n```{r}\ntable_01 %&gt;%\n  group_by(smoker)\n```\n\n# A tibble: 44 × 6\n# Groups:   smoker [2]\n   pid    arm       smoker       age education                     sex  \n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                         &lt;lgl&gt;\n 1 pid_01 placebo   non-smoker    26 grade 10-12, matriculated     FALSE\n 2 pid_02 placebo   smoker        33 grade 10-12, matriculated     FALSE\n 3 pid_03 placebo   smoker        30 post-secondary                FALSE\n 4 pid_04 placebo   non-smoker    34 grade 10-12, not matriculated FALSE\n 5 pid_05 treatment non-smoker    29 grade 10-12, matriculated     FALSE\n 6 pid_06 placebo   smoker        34 post-secondary                FALSE\n 7 pid_07 placebo   non-smoker    31 grade 10-12, not matriculated FALSE\n 8 pid_08 placebo   smoker        30 grade 10-12, not matriculated FALSE\n 9 pid_09 treatment non-smoker    35 grade 10-12, not matriculated FALSE\n10 pid_10 treatment non-smoker    32 less than grade 9             FALSE\n# ℹ 34 more rows"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#example-application-of-grouping-counting-3",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#example-application-of-grouping-counting-3",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Example application of grouping: Counting",
    "text": "Example application of grouping: Counting\nThen we group the data, and then summarise\n\n```{r}\ntable_01 %&gt;%\n  group_by(smoker) %&gt;%\n  summarise(\n    n = n() # n() returns the number of observations per group\n    )\n```\n\n# A tibble: 2 × 2\n  smoker         n\n  &lt;chr&gt;      &lt;int&gt;\n1 non-smoker    27\n2 smoker        17"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#example-application-of-grouping-counting-4",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#example-application-of-grouping-counting-4",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Example application of grouping: Counting",
    "text": "Example application of grouping: Counting\nNow let’s group by multiple variables\n\n```{r}\ntable_01 %&gt;%\n  group_by(smoker, arm)\n```\n\n# A tibble: 44 × 6\n# Groups:   smoker, arm [4]\n   pid    arm       smoker       age education                     sex  \n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                         &lt;lgl&gt;\n 1 pid_01 placebo   non-smoker    26 grade 10-12, matriculated     FALSE\n 2 pid_02 placebo   smoker        33 grade 10-12, matriculated     FALSE\n 3 pid_03 placebo   smoker        30 post-secondary                FALSE\n 4 pid_04 placebo   non-smoker    34 grade 10-12, not matriculated FALSE\n 5 pid_05 treatment non-smoker    29 grade 10-12, matriculated     FALSE\n 6 pid_06 placebo   smoker        34 post-secondary                FALSE\n 7 pid_07 placebo   non-smoker    31 grade 10-12, not matriculated FALSE\n 8 pid_08 placebo   smoker        30 grade 10-12, not matriculated FALSE\n 9 pid_09 treatment non-smoker    35 grade 10-12, not matriculated FALSE\n10 pid_10 treatment non-smoker    32 less than grade 9             FALSE\n# ℹ 34 more rows"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#example-application-of-grouping-counting-5",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#example-application-of-grouping-counting-5",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Example application of grouping: Counting",
    "text": "Example application of grouping: Counting\nNow let’s group by multiple variables, and summarise\n\n```{r}\ntable_01 %&gt;%\n  group_by(smoker, arm) %&gt;%\n    summarise(\n    n = n() # n() returns the number of observations per group\n    )\n```\n\n# A tibble: 4 × 3\n# Groups:   smoker [2]\n  smoker     arm           n\n  &lt;chr&gt;      &lt;chr&gt;     &lt;int&gt;\n1 non-smoker placebo      12\n2 non-smoker treatment    15\n3 smoker     placebo      11\n4 smoker     treatment     6"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#example-application-of-grouping-counting-6",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#example-application-of-grouping-counting-6",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Example application of grouping: Counting",
    "text": "Example application of grouping: Counting\ncount(...) is a short-cut for group_by(...) %&gt;% summarize(n = n())\n\n```{r}\ntable_01 %&gt;%\n  count(smoker, arm)\n```\n\n# A tibble: 4 × 3\n  smoker     arm           n\n  &lt;chr&gt;      &lt;chr&gt;     &lt;int&gt;\n1 non-smoker placebo      12\n2 non-smoker treatment    15\n3 smoker     placebo      11\n4 smoker     treatment     6"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#group_by-and-summariseis-the-general-method",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#group_by-and-summariseis-the-general-method",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "group_by() and summarise()is the general method",
    "text": "group_by() and summarise()is the general method\n\n```{r}\ntable_01 %&gt;%\n  group_by(smoker, arm) %&gt;%\n  summarise(median_age = median(age))\n```\n\n# A tibble: 4 × 3\n# Groups:   smoker [2]\n  smoker     arm       median_age\n  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;\n1 non-smoker placebo         31  \n2 non-smoker treatment       30  \n3 smoker     placebo         33  \n4 smoker     treatment       33.5"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#you-can-make-multiple-summarise-at-once",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#you-can-make-multiple-summarise-at-once",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "You can make multiple summarise at once",
    "text": "You can make multiple summarise at once\n\n```{r}\ntable_01 %&gt;%\n  group_by(smoker, arm) %&gt;%\n  summarise(\n    n = n(),\n    median_age = median(age)\n    )\n```\n\n# A tibble: 4 × 4\n# Groups:   smoker [2]\n  smoker     arm           n median_age\n  &lt;chr&gt;      &lt;chr&gt;     &lt;int&gt;      &lt;dbl&gt;\n1 non-smoker placebo      12       31  \n2 non-smoker treatment    15       30  \n3 smoker     placebo      11       33  \n4 smoker     treatment     6       33.5"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#lets-take-a-poll",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#lets-take-a-poll",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Let’s take a poll",
    "text": "Let’s take a poll\nGo to the event on wooclap\n\nWhat 4 columns do you expect in the output of this code?\n\ntable_01 %&gt;%\n  group_by(education_level, smoker) %&gt;%\n  summarise(n = n(), average_age = mean(age))"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#reshape-pivot_wider-and-pivot_longer",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#reshape-pivot_wider-and-pivot_longer",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Reshape: pivot_wider() and pivot_longer()",
    "text": "Reshape: pivot_wider() and pivot_longer()"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#reshaping-example-making-a-wide-summary-table",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#reshaping-example-making-a-wide-summary-table",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Reshaping example: Making a wide summary table",
    "text": "Reshaping example: Making a wide summary table\n\n```{r}\ntable_01 %&gt;%\n  count(education, arm)\n```\n\n# A tibble: 8 × 3\n  education                     arm           n\n  &lt;chr&gt;                         &lt;chr&gt;     &lt;int&gt;\n1 grade 10-12, matriculated     placebo       7\n2 grade 10-12, matriculated     treatment     9\n3 grade 10-12, not matriculated placebo      11\n4 grade 10-12, not matriculated treatment     7\n5 less than grade 9             placebo       2\n6 less than grade 9             treatment     4\n7 post-secondary                placebo       3\n8 post-secondary                treatment     1"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#reshaping-example-making-a-wide-summary-table-1",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#reshaping-example-making-a-wide-summary-table-1",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Reshaping example: Making a wide summary table",
    "text": "Reshaping example: Making a wide summary table\n\n```{r}\ntable_01 %&gt;%\n  count(education, arm) %&gt;%\n  pivot_wider(names_from = arm, values_from = n)\n```\n\n# A tibble: 4 × 3\n  education                     placebo treatment\n  &lt;chr&gt;                           &lt;int&gt;     &lt;int&gt;\n1 grade 10-12, matriculated           7         9\n2 grade 10-12, not matriculated      11         7\n3 less than grade 9                   2         4\n4 post-secondary                      3         1"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#reshaping-example-making-a-wide-summary-table-2",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#reshaping-example-making-a-wide-summary-table-2",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Reshaping example: Making a wide summary table",
    "text": "Reshaping example: Making a wide summary table\n\n```{r}\neducation_wide &lt;- table_01 %&gt;%\n  count(education, arm) %&gt;%\n  pivot_wider(names_from = arm, values_from = n)\n\neducation_wide %&gt;%\n  pivot_longer(-education, names_to = \"arm\", values_to = \"n\")\n```\n\n# A tibble: 8 × 3\n  education                     arm           n\n  &lt;chr&gt;                         &lt;chr&gt;     &lt;int&gt;\n1 grade 10-12, matriculated     placebo       7\n2 grade 10-12, matriculated     treatment     9\n3 grade 10-12, not matriculated placebo      11\n4 grade 10-12, not matriculated treatment     7\n5 less than grade 9             placebo       2\n6 less than grade 9             treatment     4\n7 post-secondary                placebo       3\n8 post-secondary                treatment     1"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#we-use-joins-to-add-columns-from-one-table-into-another",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#we-use-joins-to-add-columns-from-one-table-into-another",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "We use joins to add columns from one table into another",
    "text": "We use joins to add columns from one table into another"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#joins-turn-two-tables-into-one",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#joins-turn-two-tables-into-one",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Joins turn two tables into one",
    "text": "Joins turn two tables into one"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#there-are-different-types-of-joins",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#there-are-different-types-of-joins",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "There are different types of joins",
    "text": "There are different types of joins\nThe differences are all about how to handle when the two tables have different key values\n\n\n\nleft_join() - the resulting table always has the same key_values as the “left” table\nright_join() - the resulting table always has the same key_values as the “right” table\ninner_join() - the resulting table always only keeps the key_values that are in both tables\nfull_join() - the resulting table always has all key_values found in both tables"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#left-join",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#left-join",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Left Join",
    "text": "Left Join\nleft_join() - the resulting table always has the same key_values as the “left” table\n\n\ntable_a %&gt;% left_join(table_b)"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#right-join",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#right-join",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Right Join",
    "text": "Right Join\nright_join() - the resulting table always has the same key_values as the “right” table\n\n\ntable_a %&gt;% right_join(table_b)"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#inner_join",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#inner_join",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "inner_join",
    "text": "inner_join\ninner_join() - the resulting table always only keeps the key_values that are in both tables\n\n\ntable_a %&gt;% inner_join(table_b)"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#full-join",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#full-join",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Full join",
    "text": "Full join\nfull_join() - the resulting table always has all key_values found in both tables\n\n\ntable_a %&gt;% full_join(table_b)\n\n\nBut what are those NAs?"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#aside-na-is-how-r-denotes-missing-data",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#aside-na-is-how-r-denotes-missing-data",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Aside: NA is how R denotes missing data",
    "text": "Aside: NA is how R denotes missing data\n\nCheck out the naniar package for help seeing the missing data in your datasets\nhttps://naniar.njtierney.com/index.html"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#in-case-of-doubt-use-left_join",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#in-case-of-doubt-use-left_join",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "In case of doubt, use left_join()",
    "text": "In case of doubt, use left_join()"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#note-merging-tables-vertically-is-bind_rows-not-a-join",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#note-merging-tables-vertically-is-bind_rows-not-a-join",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Note, merging tables vertically is bind_rows(), not a join",
    "text": "Note, merging tables vertically is bind_rows(), not a join\n\n\ntable_a %&gt;% bind_rows(table_b)"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#by-default-joins-will-match-all-column-names-in-common",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#by-default-joins-will-match-all-column-names-in-common",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "by default, joins will match all column names in common",
    "text": "by default, joins will match all column names in common\n\n```{r}\n#| message: true\ntable_01 %&gt;% left_join(table_02)\n```\n\nJoining with `by = join_by(pid, arm)`\n\n\n# A tibble: 132 × 10\n   pid    arm     smoker   age education sex   time_point nugent_score crp_blood\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;lgl&gt; &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;\n 1 pid_01 placebo non-s…    26 grade 10… FALSE baseline              8      0.44\n 2 pid_01 placebo non-s…    26 grade 10… FALSE week_1                7      1.66\n 3 pid_01 placebo non-s…    26 grade 10… FALSE week_7                7      1.44\n 4 pid_02 placebo smoker    33 grade 10… FALSE baseline              7      1.55\n 5 pid_02 placebo smoker    33 grade 10… FALSE week_1                7      0.75\n 6 pid_02 placebo smoker    33 grade 10… FALSE week_7                4      1.17\n 7 pid_03 placebo smoker    30 post-sec… FALSE baseline              6      1.78\n 8 pid_03 placebo smoker    30 post-sec… FALSE week_1               10      0.57\n 9 pid_03 placebo smoker    30 post-sec… FALSE week_7                7      1.79\n10 pid_04 placebo non-s…    34 grade 10… FALSE baseline              5      1.76\n# ℹ 122 more rows\n# ℹ 1 more variable: ph &lt;dbl&gt;"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#exercise",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#exercise",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Exercise",
    "text": "Exercise\nThat’s enough slides for now time to try for yourself! Go to the module and go to the first exercise.\n\n\n\n−+\n30:00"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#histograms-and-density-plots",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#histograms-and-density-plots",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Histograms and density plots",
    "text": "Histograms and density plots\n\n\n\n\n\n\n\nage\nsex\nclass\nsurvived\n\n\n\n\n0.17\nfemale\n3rd\nsurvived\n\n\n0.33\nmale\n3rd\ndied\n\n\n0.80\nmale\n2nd\nsurvived\n\n\n0.83\nmale\n2nd\nsurvived\n\n\n0.83\nmale\n3rd\nsurvived\n\n\n0.92\nmale\n1st\nsurvived\n\n\n1.00\nfemale\n2nd\nsurvived\n\n\n1.00\nfemale\n3rd\nsurvived\n\n\n1.00\nmale\n2nd\nsurvived\n\n\n1.00\nmale\n2nd\nsurvived\n\n\n1.00\nmale\n3rd\nsurvived\n\n\n1.50\nfemale\n3rd\ndied\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\nclass\nsurvived\n\n\n\n\n1.5\nfemale\n3rd\ndied\n\n\n2.0\nfemale\n1st\ndied\n\n\n2.0\nfemale\n2nd\nsurvived\n\n\n2.0\nfemale\n3rd\ndied\n\n\n2.0\nfemale\n3rd\ndied\n\n\n2.0\nmale\n2nd\nsurvived\n\n\n2.0\nmale\n2nd\nsurvived\n\n\n2.0\nmale\n2nd\nsurvived\n\n\n3.0\nfemale\n2nd\nsurvived\n\n\n3.0\nfemale\n3rd\nsurvived\n\n\n3.0\nmale\n2nd\nsurvived\n\n\n3.0\nmale\n2nd\nsurvived\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\nclass\nsurvived\n\n\n\n\n3\nmale\n3rd\nsurvived\n\n\n3\nmale\n3rd\nsurvived\n\n\n4\nfemale\n2nd\nsurvived\n\n\n4\nfemale\n2nd\nsurvived\n\n\n4\nfemale\n3rd\nsurvived\n\n\n4\nfemale\n3rd\nsurvived\n\n\n4\nmale\n1st\nsurvived\n\n\n4\nmale\n3rd\ndied\n\n\n4\nmale\n3rd\nsurvived\n\n\n5\nfemale\n3rd\nsurvived\n\n\n5\nfemale\n3rd\nsurvived\n\n\n5\nmale\n3rd\ndied"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#define-bins-and-count-classes",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#define-bins-and-count-classes",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Define bins and count classes",
    "text": "Define bins and count classes\n\n\n\n\n\n\n\n\nage range\ncount\n\n\n\n\n0–5\n36\n\n\n6–10\n19\n\n\n11–15\n18\n\n\n16–20\n99\n\n\n21–25\n139\n\n\n26–30\n121\n\n\n31–35\n76\n\n\n36–40\n74\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage range\ncount\n\n\n\n\n41–45\n54\n\n\n46–50\n50\n\n\n51–55\n26\n\n\n56–60\n22\n\n\n61–65\n16\n\n\n66–70\n3\n\n\n71–75\n3\n\n\n76–80\n0"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#define-bins-and-count-classes-1",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#define-bins-and-count-classes-1",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Define bins and count classes",
    "text": "Define bins and count classes\n\n\n\n\n\n\n\n\nage range\ncount\n\n\n\n\n0–5\n36\n\n\n6–10\n19\n\n\n11–15\n18\n\n\n16–20\n99\n\n\n21–25\n139\n\n\n26–30\n121\n\n\n31–35\n76\n\n\n36–40\n74\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage range\ncount\n\n\n\n\n41–45\n54\n\n\n46–50\n50\n\n\n51–55\n26\n\n\n56–60\n22\n\n\n61–65\n16\n\n\n66–70\n3\n\n\n71–75\n3\n\n\n76–80\n0"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#histograms-depend-on-the-chosen-bin-width",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#histograms-depend-on-the-chosen-bin-width",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Histograms depend on the chosen bin width",
    "text": "Histograms depend on the chosen bin width"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#alternative-to-histogram-kernel-density-estimate-kde",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#alternative-to-histogram-kernel-density-estimate-kde",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Alternative to histogram: Kernel density estimate (KDE)",
    "text": "Alternative to histogram: Kernel density estimate (KDE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHistograms show raw counts, KDEs show proportions. (Total area = 1)"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#kdes-also-depend-on-parameter-settings",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#kdes-also-depend-on-parameter-settings",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "KDEs also depend on parameter settings",
    "text": "KDEs also depend on parameter settings"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#careful-kdes-can-show-non-sensical-data",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#careful-kdes-can-show-non-sensical-data",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Careful: KDEs can show non-sensical data",
    "text": "Careful: KDEs can show non-sensical data"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#careful-are-bars-stacked-or-overlapping",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#careful-are-bars-stacked-or-overlapping",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Careful, are bars stacked or overlapping?",
    "text": "Careful, are bars stacked or overlapping?"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#alternatively-age-pyramid",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#alternatively-age-pyramid",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Alternatively: Age pyramid",
    "text": "Alternatively: Age pyramid"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#alternatively-kdes-showing-proportions-of-total",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#alternatively-kdes-showing-proportions-of-total",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Alternatively: KDEs showing proportions of total",
    "text": "Alternatively: KDEs showing proportions of total"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#making-histograms-with-ggplot-geom_histogram",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#making-histograms-with-ggplot-geom_histogram",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Making histograms with ggplot: geom_histogram()",
    "text": "Making histograms with ggplot: geom_histogram()\n\n```{r}\nggplot(titanic, aes(age)) +\n  geom_histogram()\n```"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#setting-the-bin-width",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#setting-the-bin-width",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Setting the bin width",
    "text": "Setting the bin width\n\n```{r}\nggplot(titanic, aes(age)) +\n  geom_histogram(binwidth = 5)\n```\n\n\n\nDo you like where there bins are? What does the first bin say?"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#always-set-the-center-as-well-to-half-the-bin_width",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#always-set-the-center-as-well-to-half-the-bin_width",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Always set the center as well, to half the bin_width",
    "text": "Always set the center as well, to half the bin_width\n\n```{r}\nggplot(titanic, aes(age)) +\n  geom_histogram(binwidth = 5, center=2.5)\n```\n\n\n\nSetting center 2.5 makes the bars start 0-5, 5-10, etc. instead of 2.5-7.5, etc. You could instead use the argument boundary=5 to accomplish the same behavior."
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#making-density-plots-with-ggplot-geom_density-auto-animatetrue",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#making-density-plots-with-ggplot-geom_density-auto-animatetrue",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Making density plots with ggplot: geom_density() {auto-animate:true}",
    "text": "Making density plots with ggplot: geom_density() {auto-animate:true}\n\n```{r}\nggplot(titanic, aes(age)) +\n  geom_density(fill = \"skyblue\")\n```"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#making-density-plots-with-ggplot-geom_density-auto-animatetrue-1",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#making-density-plots-with-ggplot-geom_density-auto-animatetrue-1",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Making density plots with ggplot: geom_density() {auto-animate:true}",
    "text": "Making density plots with ggplot: geom_density() {auto-animate:true}\n\n```{r}\nggplot(titanic, aes(age)) +\n  geom_density()\n```\n\n\nwithout fill"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#modifying-bandwidth-bw-and-kernel-parameters-auto-animatetrue",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#modifying-bandwidth-bw-and-kernel-parameters-auto-animatetrue",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Modifying bandwidth (bw) and kernel parameters {auto-animate:true}",
    "text": "Modifying bandwidth (bw) and kernel parameters {auto-animate:true}\n\n```{r}\nggplot(titanic, aes(age)) +\n  geom_density(\n    fill = \"skyblue\",\n    bw = 0.5,               # a small bandwidth\n    kernel = \"gaussian\"     # Gaussian kernel (the default)\n  )\n```"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#modifying-bandwidth-bw-and-kernel-parameters-auto-animatetrue-1",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#modifying-bandwidth-bw-and-kernel-parameters-auto-animatetrue-1",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Modifying bandwidth (bw) and kernel parameters {auto-animate:true}",
    "text": "Modifying bandwidth (bw) and kernel parameters {auto-animate:true}\n\n```{r}\nggplot(titanic, aes(age)) +\n  geom_density(\n    fill = \"skyblue\",\n    bw = 2,                 # a moderate bandwidth\n    kernel = \"rectangular\"  # rectangular kernel\n  )\n```"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#density-estimates-visualize-distributions",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#density-estimates-visualize-distributions",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Density estimates visualize distributions",
    "text": "Density estimates visualize distributions\nMean temperatures in Lincoln, NE, in January 2016:\n\n\n\n\n\n\n\ndate\nmean temp\n\n\n\n\n2016-01-01\n-4\n\n\n2016-01-02\n-5\n\n\n2016-01-03\n-5\n\n\n2016-01-04\n-8\n\n\n2016-01-05\n-2\n\n\n2016-01-06\n1\n\n\n2016-01-07\n-1\n\n\n2016-01-08\n-4\n\n\n2016-01-09\n-13\n\n\n2016-01-10\n-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can we compare distributions across months?"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#a-bad-idea-many-overlapping-density-plots",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#a-bad-idea-many-overlapping-density-plots",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "A bad idea: Many overlapping density plots",
    "text": "A bad idea: Many overlapping density plots"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#another-bad-idea-stacked-density-plots",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#another-bad-idea-stacked-density-plots",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Another bad idea: Stacked density plots",
    "text": "Another bad idea: Stacked density plots"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#somewhat-better-small-multiples",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#somewhat-better-small-multiples",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Somewhat better: Small multiples",
    "text": "Somewhat better: Small multiples"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#instead-show-values-along-y-conditions-along-x",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#instead-show-values-along-y-conditions-along-x",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Instead: Show values along y, conditions along x",
    "text": "Instead: Show values along y, conditions along x\n\nA boxplot is a crude way of visualizing a distribution."
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#how-to-read-a-boxplot",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#how-to-read-a-boxplot",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "How to read a boxplot",
    "text": "How to read a boxplot"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#if-you-like-density-plots-consider-violins",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#if-you-like-density-plots-consider-violins",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "If you like density plots, consider violins",
    "text": "If you like density plots, consider violins\n\n\nA violin plot is a density plot rotated 90 degrees and then mirrored."
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#how-to-read-a-violin-plot",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#how-to-read-a-violin-plot",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "How to read a violin plot",
    "text": "How to read a violin plot"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#for-small-datasets-you-can-also-use-a-strip-chart",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#for-small-datasets-you-can-also-use-a-strip-chart",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "For small datasets, you can also use a strip chart",
    "text": "For small datasets, you can also use a strip chart\nAdvantage: Can see raw data points instead of abstract representation.\n\n\nHorizontal jittering may be necessary to avoid overlapping points."
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#another-option-is-a-scatter-density-plot",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#another-option-is-a-scatter-density-plot",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Another option is a scatter-density plot",
    "text": "Another option is a scatter-density plot\nAdvantage: Best of both worlds for violin and jitter plot, see the raw data but also see the shape of the density"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#advice---always-show-the-finest-granularity-of-data-that-is-practical.",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#advice---always-show-the-finest-granularity-of-data-that-is-practical.",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Advice - always show the finest granularity of data that is practical.",
    "text": "Advice - always show the finest granularity of data that is practical.\nIf you don’t have too many points, show them! It makes it much easier to interpret the data. Especially when you are exploring new datasets.\nFavor showing distributions over just a mean with error bars."
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#making-boxplots-violins-etc.-in-ggplot2-1",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#making-boxplots-violins-etc.-in-ggplot2-1",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Making boxplots, violins, etc. in ggplot2",
    "text": "Making boxplots, violins, etc. in ggplot2\n\n\n\n\n\n\n\n\nPlot type\nGeom\nNotes\n\n\n\n\nboxplot\ngeom_boxplot()\n\n\n\nviolin plot\ngeom_violin()\n\n\n\nstrip chart\ngeom_point()\nJittering requires position_jitter()\n\n\nsina plot\ngeom_sina()\nFrom package ggforce\n\n\nscatter-density plot\ngeom_quasirandom()\nFrom package ggbeeswarm\n\n\nridgeline\ngeom_density_ridges()\nFrom package ggridges"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#examples-boxplot-auto-animate-true",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#examples-boxplot-auto-animate-true",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Examples: Boxplot {auto-animate: true}",
    "text": "Examples: Boxplot {auto-animate: true}\n\n```{r}\nggplot(lincoln_temps, aes(x = month, y = mean_temp)) +\n  geom_boxplot(fill = \"skyblue\") \n```"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#examples-violins-auto-animate-true",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#examples-violins-auto-animate-true",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Examples: Violins {auto-animate: true}",
    "text": "Examples: Violins {auto-animate: true}\n\n```{r}\nggplot(lincoln_temps, aes(x = month, y = mean_temp)) +\n  geom_violin(fill = \"skyblue\") \n```"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#examples-strip-chart-no-jitter-auto-animate-true",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#examples-strip-chart-no-jitter-auto-animate-true",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Examples: Strip chart (no jitter) {auto-animate: true}",
    "text": "Examples: Strip chart (no jitter) {auto-animate: true}\n\n```{r}\nggplot(lincoln_temps, aes(x = month, y = mean_temp)) +\n  geom_point(color = \"skyblue\") \n```"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#examples-strip-chart-w-jitter-auto-animate-true",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#examples-strip-chart-w-jitter-auto-animate-true",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Examples: Strip chart (w/ jitter) {auto-animate: true}",
    "text": "Examples: Strip chart (w/ jitter) {auto-animate: true}\n\n```{r}\nggplot(lincoln_temps, aes(x = month, y = mean_temp)) +\n  geom_jitter(color = \"skyblue\") \n```"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#examples-scatter-density-plot-auto-animate-true",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#examples-scatter-density-plot-auto-animate-true",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Examples: Scatter density plot {auto-animate: true}",
    "text": "Examples: Scatter density plot {auto-animate: true}\n\n```{r}\nggplot(lincoln_temps, aes(x = month, y = mean_temp)) +\n  geom_quasirandom(color = \"skyblue\") \n```"
  },
  {
    "objectID": "materials/1-workshop1/4-tidyverse-201/slides.html#exercise-1",
    "href": "materials/1-workshop1/4-tidyverse-201/slides.html#exercise-1",
    "title": "More data wrangling and data visualization with the tidyverse",
    "section": "Exercise",
    "text": "Exercise\nTry exploring different continuous variables in table 01, table 02, and table_03 using these density visualization strategies.\n\n\n\nback to module"
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/slides.html#learning-outcomes",
    "href": "materials/1-workshop1/5-tableone/slides.html#learning-outcomes",
    "title": "tableone and its Basic Statistics",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\n\nRefresh on what baseline characteristics are and why they are important.\n\n\n\n\nKnow which descriptive statistics and statistical tests are used to evaluate baseline characteristic.\n\n\n\n\nKnow what is tableone package and how it works.\n\n\n\n\nKnow how to use tableone package with a real data set."
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/slides.html#learning-outcomes-1",
    "href": "materials/1-workshop1/5-tableone/slides.html#learning-outcomes-1",
    "title": "tableone and its Basic Statistics",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\n\nVariablePlaceboTreatmentOverallPn232144Smoker = smoker (%)11 (47.8)6 (28.6)17 (38.6)0.317Age (years); median (IQR)31.0 (30.0, 33.0)31.0 (29.0, 34.0)31.0 (29.0, 33.2)0.897Education (%)0.437   grade 10-12, matriculated7 (30.4)9 (42.9)16 (36.4)   grade 10-12, not matriculated11 (47.8)7 (33.3)18 (40.9)   less than grade 92 (8.7)4 (19.0)6 (13.6)   post-secondary3 (13.0)1 (4.8)4 (9.1)Sex = Female (%)23 (100.0)21 (100.0)44 (100.0)-Nugent Score; median (IQR)7.0 (6.5, 8.0)6.0 (5.0, 6.0)6.5 (5.0, 8.0)0.001CRP Blood; median (IQR)1.2 (0.9, 2.4)1.2 (0.7, 1.8)1.2 (0.8, 2.0)0.724pH; median (IQR)5.2 (4.8, 5.3)4.6 (4.3, 4.9)4.9 (4.6, 5.3)0.002IFN-Y; median (IQR)0.3 (0.3, 0.8)0.3 (0.3, 0.3)0.3 (0.3, 0.5)0.159IL-10; median (IQR)0.8 (0.8, 2.0)0.8 (0.8, 1.8)0.8 (0.8, 1.9)0.820"
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/slides.html#introduction",
    "href": "materials/1-workshop1/5-tableone/slides.html#introduction",
    "title": "tableone and its Basic Statistics",
    "section": "Introduction",
    "text": "Introduction\nWhat are baseline characteristics and why are they important?\n\n\nBaseline characteristics describe the participants at the start of a study, for example\n\n\n\nage, sex, disease severity, etc.*\n\n\n\nThey help readers assess the validity and applicability of the study results (Schulz, Altman, and Moher 2010; Altman 1985).\n\n\n\n\nThey allow researchers to explore the treatment effect across different subgroups (Matthews 2006).\n\n\n\n\nExplain what validity and applicability mean in the context of research.\nGive examples of subgroups and how they can influence the treatment effect."
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/slides.html#methods",
    "href": "materials/1-workshop1/5-tableone/slides.html#methods",
    "title": "tableone and its Basic Statistics",
    "section": "Methods",
    "text": "Methods\nWhich descriptive statistics and statistical tests?\n\n\nDescriptive statistics summarize the distribution and characteristics of a variable, such as\n\n\n\nmeans (standard deviations), medians (interquartile range), count (percentage)\n\n\n\nStatistical tests evaluate whether there is a significant difference or association between groups or variables, such as\n\n\n\nt-tests, anova, rank sum tests, chi-squared tests\n\n\n\nStandardized mean difference (SMD) is a measure of the effect size that can compare different variables or combine results from different studies (Schulz, Altman, and Moher 2010).\n\n\n\n\nExplain what distribution and characteristics mean in the context of statistics.\nGive examples of when to use different statistical tests and how to interpret them.\nExplain what effect size means and why it is important."
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/slides.html#r-package-tableone",
    "href": "materials/1-workshop1/5-tableone/slides.html#r-package-tableone",
    "title": "tableone and its Basic Statistics",
    "section": "R package tableone",
    "text": "R package tableone\nWhat is tableone package and how does it work?\n\n\ntableone is a package that simplifies the creation of “Table 1: Baseline demographics and clinical characteristics” (Yoshida and Bartel 2022).\n\n\n\n\nIt can handle both continuous and categorical variables, and provide descriptive statistics, statistical tests, and SMDs.\n\n\n\n\nIt can handle weighted data using the survey package, which allows researchers to account for complex sampling designs and adjust for confounding factors.\n\n\n\n\nIt has a simple and flexible syntax, and can produce nice-looking tables using the print or kableone function (together with flextable you get nice tables).\n\n\n\n\nExplain what Table 1 is and why it is important in research reports.\nExplain what continuous and categorical variables are and how to handle them differently.\nExplain what weighted data, complex sampling designs, and confounding factors are and how they affect the analysis.\nShow an example of the syntax and output of tableone."
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/slides.html#load-tableone-package",
    "href": "materials/1-workshop1/5-tableone/slides.html#load-tableone-package",
    "title": "tableone and its Basic Statistics",
    "section": "Load tableone package",
    "text": "Load tableone package\ntableone Github Site\n\nlibrary(tableone) # Loading/Attaching and Listing of Packages\n\n\nCreateTableOne function\nsvyCreateTableOne function (not in the scope)\n\n\n?CreateTableOne"
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/slides.html#createtableone-function",
    "href": "materials/1-workshop1/5-tableone/slides.html#createtableone-function",
    "title": "tableone and its Basic Statistics",
    "section": "CreateTableOne Function",
    "text": "CreateTableOne Function\n\nDataTesting…category vars…continuous varsTotal\n\n\n\n\ndata: A data frame in which these variables exist. All variables (both vars and strata) must be in this data frame.\nvars: Variables to be summarized given as a character vector. Factors are handled as categorical variables, whereas numeric variables are handled as continuous variables. If empty, all variables in the data frame specified in the data argument are used.\nstrata: Stratifying (grouping) variable name(s) given as a character vector. If omitted, the overall results are returned.\nfactorVars: Numerically coded variables that should be handled as categorical variables given as a character vector. Do not include factors, unless you need to relevel them by removing empty levels. If omitted, only factors are considered categorical variables. The variables specified here must also be specified in the vars argument.\nincludeNA = FALSE: If TRUE, NA is handled as a regular factor level rather than missing. NA is shown as the last factor level in the table. Only effective for categorical variables.\n\n\n\n\n\n\ntest = TRUE: If TRUE, as in the default and there are more than two groups, groupwise comparisons are performed.\n\n\n\n\n\n\ntestApprox = chisq.test: A function used to perform the large sample approximation based tests. The default is chisq.test. This is not recommended when some of the cell have small counts like fewer than 5.\nargsApprox = list(correct = TRUE): A named list of arguments passed to the function specified in testApprox. The default is list (correct = TRUE), which turns on the continuity correction for chisq.test.\ntestExact = fisher.test: A function used to perform the exact tests. The default is fisher.test. If the cells have large numbers, it will fail because of memory limitation. In this situation, the large sample approximation based should suffice.\n\n\n\n\n\n\ntestNormal = oneway.test: A function used to perform the normal assumption based tests. The default is oneway.test. This is equivalent of the t-test when there are only two groups.\nargsNormal = list(var.equal = TRUE): A named list of arguments passed to the function specified in testNormal.\ntestNonNormal = kruskal.test: A function used to perform non-normal assumption based tests.\nargsNonNormal = list(NULL): A named list of arguments passed to the function specified in testNonNormal.\nsmd = TRUE: If set to TRUE, standardized mean differences are calculated.\n\n\n\n\n\n\naddOverall = FALSE: If set to TRUE, an overall column is added to the table."
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/slides.html#print-function-for-createtableone-object",
    "href": "materials/1-workshop1/5-tableone/slides.html#print-function-for-createtableone-object",
    "title": "tableone and its Basic Statistics",
    "section": "print function for CreateTableOne object",
    "text": "print function for CreateTableOne object\n\nObjectSimple lookDecimalsVariablesTest\n\n\n\n\nx: object that you want to print.\n\n\n\n\n\n\nprintToggle = TRUE: If set to TRUE, the function will print the table.\nquote = FALSE: If set to FALSE, the function will not quote character strings.\nvarLabels = FALSE: If set to TRUE, variable labels (if available) are used instead of variable names.\nexplain = TRUE: If set to TRUE, explanations for the statistics are printed.\nnoSpaces = FALSE: If set to TRUE, spaces are removed from variable names.\npadColnames = FALSE: If set to TRUE, column names are padded with spaces for alignment.\ndropEqual = FALSE: If set to TRUE, the equal sign is dropped from p-values.\nshowAllLevels = FALSE: If set to TRUE, all levels of factors are shown even if some levels have zero count.\n\n\n\n\n\n\ncatDigits = 1: The number of digits after the decimal point for categorical variables.\ncontDigits = 2: The number of digits after the decimal point for continuous variables.\npDigits = 3: The number of digits after the decimal point for p-values.\nformatOptions = list(scientific = FALSE): A list of options for formatting numbers.\n\n\n\n\n\n\nmissing = FALSE: If set to TRUE, missing values are included in the table.\nminMax = FALSE: If set to TRUE, minimum and maximum values are included in the table for continuous variables.\nformat = c(\"fp\", \"f\", \"p\", \"pf\")[1]: The format of the table. Options include “fp” (frequency and percentage), “f” (frequency only), “p” (percentage only), and “pf” (percentage and frequency).\nnonnormal = NULL: A character vector of non-normal variables. For these variables, median and IQR are reported instead of mean and SD.\ncramVars = NULL: A character vector of variables for which Cramér’s V is calculated.\n\n\n\n\n\n\ntest = TRUE: If set to TRUE, tests are performed for differences across strata.\nexact = NULL: A character vector of variables for which exact tests are performed instead of chi-squared tests.\nsmd = FALSE: If set to TRUE, standardized mean differences are calculated."
  },
  {
    "objectID": "materials/1-workshop1/5-tableone/slides.html#references",
    "href": "materials/1-workshop1/5-tableone/slides.html#references",
    "title": "tableone and its Basic Statistics",
    "section": "References",
    "text": "References\n\n\n\nback to module\n\n\n\n\nAltman, Douglas G. 1985. “Comparability of Randomised Groups.” The Statistician 34 (1): 125. https://doi.org/10.2307/2987510.\n\n\nMatthews, John N. S. 2006. Introduction to Randomized Controlled Clinical Trials. Chapman; Hall/CRC. https://doi.org/10.1201/9781420011302.\n\n\nSchulz, K. F, D. G Altman, and D. Moher. 2010. “CONSORT 2010 Statement: Updated Guidelines for Reporting Parallel Group Randomised Trials.” BMJ 340 (mar23 1): c332–32. https://doi.org/10.1136/bmj.c332.\n\n\nYoshida, Kazuki, and Alexander Bartel. 2022. “Tableone: Create ’Table 1’ to Describe Baseline Characteristics with or Without Propensity Score Weights.” https://CRAN.R-project.org/package=tableone."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/joining_table_demo.html",
    "href": "materials/1-workshop1/6-hypothesis-testing/joining_table_demo.html",
    "title": "joining table",
    "section": "",
    "text": "In this document, we’ll explore with very simple and small tables how to use the different join functions to merge tables together.\n\nPackages\nFirst, as usual, we need to load the tidyverse package.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWe also load the tidylog package which is very useful for beginners as, for most dplyr and tidyr functions, it adds a message to the console explaining in plain English what the function did.\n\nlibrary(tidylog)\n\n\nAttaching package: 'tidylog'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    add_count, add_tally, anti_join, count, distinct, distinct_all,\n    distinct_at, distinct_if, filter, filter_all, filter_at, filter_if,\n    full_join, group_by, group_by_all, group_by_at, group_by_if,\n    inner_join, left_join, mutate, mutate_all, mutate_at, mutate_if,\n    relocate, rename, rename_all, rename_at, rename_if, rename_with,\n    right_join, sample_frac, sample_n, select, select_all, select_at,\n    select_if, semi_join, slice, slice_head, slice_max, slice_min,\n    slice_sample, slice_tail, summarise, summarise_all, summarise_at,\n    summarise_if, summarize, summarize_all, summarize_at, summarize_if,\n    tally, top_frac, top_n, transmute, transmute_all, transmute_at,\n    transmute_if, ungroup\n\n\nThe following objects are masked from 'package:tidyr':\n\n    drop_na, fill, gather, pivot_longer, pivot_wider, replace_na,\n    spread, uncount\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nWe also load the pander package to print the tables in a pretty way:\n\nlibrary(pander)\n\n\n\nJoining two tables\nWe create two simple tables that contain two columns: a sample column providing the sample ID, and a value column that contains the corresponding values.\n\nA &lt;- tibble(sample = c(\"a\",\"b\",\"c\"), value_A = c(1,2,3))\nA |&gt; pander()\n\n\n\n\n\n\n\n\nsample\nvalue_A\n\n\n\n\na\n1\n\n\nb\n2\n\n\nc\n3\n\n\n\n\nB &lt;- tibble(sample = c(\"a\",\"b\"), value_B = c(4,5))\nB |&gt; pander()\n\n\n\n\n\n\n\n\nsample\nvalue_B\n\n\n\n\na\n4\n\n\nb\n5\n\n\n\n\n\n\nA |&gt; left_join(B) |&gt; pander()\n\nJoining with `by = join_by(sample)`\nleft_join: added one column (value_B)\n&gt; rows only in x 1\n&gt; rows only in y (0)\n&gt; matched rows 2\n&gt; ===\n&gt; rows total 3\n\n\n\n\n\n\n\n\n\n\nsample\nvalue_A\nvalue_B\n\n\n\n\na\n1\n4\n\n\nb\n2\n5\n\n\nc\n3\nNA\n\n\n\n\nB |&gt; left_join(A) |&gt; pander()\n\nJoining with `by = join_by(sample)`\nleft_join: added one column (value_A)\n&gt; rows only in x 0\n&gt; rows only in y (1)\n&gt; matched rows 2\n&gt; ===\n&gt; rows total 2\n\n\n\n\n\n\n\n\n\n\nsample\nvalue_B\nvalue_A\n\n\n\n\na\n4\n1\n\n\nb\n5\n2\n\n\n\n\n\n\nA |&gt; full_join(B) |&gt; pander()\n\nJoining with `by = join_by(sample)`\nfull_join: added one column (value_B)\n&gt; rows only in x 1\n&gt; rows only in y 0\n&gt; matched rows 2\n&gt; ===\n&gt; rows total 3\n\n\n\n\n\n\n\n\n\n\nsample\nvalue_A\nvalue_B\n\n\n\n\na\n1\n4\n\n\nb\n2\n5\n\n\nc\n3\nNA\n\n\n\n\n\nWe see that, automatically, left_join and full_join identify the column that exist in both tables and join using that column.\n\n\nJoining when no common column\nIf we do not have any column in common, we get an error. Let’s try introducing another small table:\n\nC &lt;- tibble(sid = c(\"a\",\"b\",\"c\"), value_C = c(7,8,9))\nC |&gt; pander()\n\n\n\n\n\n\n\n\nsid\nvalue_C\n\n\n\n\na\n7\n\n\nb\n8\n\n\nc\n9\n\n\n\n\n\n\nA |&gt; left_join(C) |&gt; pander()\n\nError in `.fun()`:\n! `by` must be supplied when `x` and `y` have no common variables.\nℹ Use `cross_join()` to perform a cross-join.\n\n\nError in if (tail(stdout, 1) == \"\") {: argument is of length zero\n\n\nIf we know that these two tables are, in fact, reporting values for the same samples, we can rename the sid column so that it matches the name in table A\n\nC2 &lt;- C |&gt; rename(sample = sid)\n\nrename: renamed one variable (sample)\n\nC2  |&gt; pander()\n\n\n\n\n\n\n\n\nsample\nvalue_C\n\n\n\n\na\n7\n\n\nb\n8\n\n\nc\n9\n\n\n\n\nA |&gt; left_join(C2) |&gt; pander()\n\nJoining with `by = join_by(sample)`\n\n\nleft_join: added one column (value_C)\n\n\n           &gt; rows only in x   0\n\n\n           &gt; rows only in y  (0)\n\n\n           &gt; matched rows     3\n\n\n           &gt;                 ===\n\n\n           &gt; rows total       3\n\n\n\n\n\n\n\n\n\n\nsample\nvalue_A\nvalue_C\n\n\n\n\na\n1\n7\n\n\nb\n2\n8\n\n\nc\n3\n9\n\n\n\n\n\n\n\nJoining when no samples in common\nNow, let’s try to join with a table that does not have any sample in common with A:\n\nD &lt;- tibble(sample = c(\"d\",\"e\",\"f\"), value_D = c(1, 11, 111))\nD |&gt; pander()\n\n\n\n\n\n\n\n\nsample\nvalue_D\n\n\n\n\nd\n1\n\n\ne\n11\n\n\nf\n111\n\n\n\n\n\n\nA |&gt; left_join(D) |&gt; pander()\n\nJoining with `by = join_by(sample)`\nleft_join: added one column (value_D)\n&gt; rows only in x 3\n&gt; rows only in y (3)\n&gt; matched rows 0\n&gt; ===\n&gt; rows total 3\n\n\n\n\n\n\n\n\n\n\nsample\nvalue_A\nvalue_D\n\n\n\n\na\n1\nNA\n\n\nb\n2\nNA\n\n\nc\n3\nNA\n\n\n\n\nA |&gt; full_join(D) |&gt; pander()\n\nJoining with `by = join_by(sample)`\nfull_join: added one column (value_D)\n&gt; rows only in x 3\n&gt; rows only in y 3\n&gt; matched rows 0\n&gt; ===\n&gt; rows total 6\n\n\n\n\n\n\n\n\n\n\nsample\nvalue_A\nvalue_D\n\n\n\n\na\n1\nNA\n\n\nb\n2\nNA\n\n\nc\n3\nNA\n\n\nd\nNA\n1\n\n\ne\nNA\n11\n\n\nf\nNA\n111\n\n\n\n\nA |&gt; inner_join(D) |&gt; pander()\n\nJoining with `by = join_by(sample)`\ninner_join: added one column (value_D)\n&gt; rows only in x (3)\n&gt; rows only in y (3)\n&gt; matched rows 0\n&gt; ===\n&gt; rows total 0\n\n\n\n\n\n\n\n\n\n\nsample\nvalue_A\nvalue_D\n\n\n\n\n\n\n\nHow do we reorder the columns from the joined table?\nFor example, if we want to see the value_B column first after joining A and B, we can do:\n\nA |&gt;\n  left_join(B) |&gt; \n  select(value_B, everything()) |&gt; pander()\n\nJoining with `by = join_by(sample)`\nleft_join: added one column (value_B)\n&gt; rows only in x 1\n&gt; rows only in y (0)\n&gt; matched rows 2\n&gt; ===\n&gt; rows total 3\nselect: columns reordered (value_B, sample, value_A)\n\n\n\n\n\n\n\n\n\n\nvalue_B\nsample\nvalue_A\n\n\n\n\n4\na\n1\n\n\n5\nb\n2\n\n\nNA\nc\n3\n\n\n\n\n\nWhat if there are several rows for each sample in a table we want to join with?\n\nG &lt;- tibble(sample = rep(c(\"a\",\"b\",\"c\"), each = 2), timepoint = rep(c(\"1\",\"2\"),3), value_G = 1:6)\nG |&gt; pander()\n\n\n\n\n\n\n\n\n\nsample\ntimepoint\nvalue_G\n\n\n\n\na\n1\n1\n\n\na\n2\n2\n\n\nb\n1\n3\n\n\nb\n2\n4\n\n\nc\n1\n5\n\n\nc\n2\n6\n\n\n\n\n\n\nA |&gt; left_join(G) |&gt; pander()\n\nJoining with `by = join_by(sample)`\nleft_join: added 2 columns (timepoint, value_G)\n&gt; rows only in x 0\n&gt; rows only in y (0)\n&gt; matched rows 6 (includes duplicates)\n&gt; ===\n&gt; rows total 6\n\n\n\n\n\n\n\n\n\n\n\nsample\nvalue_A\ntimepoint\nvalue_G\n\n\n\n\na\n1\n1\n1\n\n\na\n1\n2\n2\n\n\nb\n2\n1\n3\n\n\nb\n2\n2\n4\n\n\nc\n3\n1\n5\n\n\nc\n3\n2\n6\n\n\n\n\nG |&gt; left_join(A) |&gt; pander()\n\nJoining with `by = join_by(sample)`\nleft_join: added one column (value_A)\n&gt; rows only in x 0\n&gt; rows only in y (0)\n&gt; matched rows 6\n&gt; ===\n&gt; rows total 6\n\n\n\n\n\n\n\n\n\n\n\nsample\ntimepoint\nvalue_G\nvalue_A\n\n\n\n\na\n1\n1\n1\n\n\na\n2\n2\n1\n\n\nb\n1\n3\n2\n\n\nb\n2\n4\n2\n\n\nc\n1\n5\n3\n\n\nc\n2\n6\n3\n\n\n\n\nA |&gt; full_join(G) |&gt; pander()\n\nJoining with `by = join_by(sample)`\nfull_join: added 2 columns (timepoint, value_G)\n&gt; rows only in x 0\n&gt; rows only in y 0\n&gt; matched rows 6 (includes duplicates)\n&gt; ===\n&gt; rows total 6\n\n\n\n\n\n\n\n\n\n\n\nsample\nvalue_A\ntimepoint\nvalue_G\n\n\n\n\na\n1\n1\n1\n\n\na\n1\n2\n2\n\n\nb\n2\n1\n3\n\n\nb\n2\n2\n4\n\n\nc\n3\n1\n5\n\n\nc\n3\n2\n6\n\n\n\n\nA |&gt; inner_join(G) |&gt; pander()\n\nJoining with `by = join_by(sample)`\ninner_join: added 2 columns (timepoint, value_G)\n&gt; rows only in x (0)\n&gt; rows only in y (0)\n&gt; matched rows 6 (includes duplicates)\n&gt; ===\n&gt; rows total 6\n\n\n\n\n\n\n\n\n\n\n\nsample\nvalue_A\ntimepoint\nvalue_G\n\n\n\n\na\n1\n1\n1\n\n\na\n1\n2\n2\n\n\nb\n2\n1\n3\n\n\nb\n2\n2\n4\n\n\nc\n3\n1\n5\n\n\nc\n3\n2\n6\n\n\n\n\n\n\n\nPivots\nHow do we transform table G so that we have only one row per sample and two columns: one for each time-point?\n\nG_wide &lt;- \nG |&gt; \n  pivot_wider(\n   id_cols = sample,\n   names_from = timepoint,\n   names_prefix = \"time_point\",\n   values_from = value_G\n  ) \n\npivot_wider: reorganized (timepoint, value_G) into (time_point1, time_point2) [was 6x3, now 3x3]\n\nG_wide |&gt; pander()\n\n\n\n\n\n\n\n\n\nsample\ntime_point1\ntime_point2\n\n\n\n\na\n1\n2\n\n\nb\n3\n4\n\n\nc\n5\n6\n\n\n\n\n\nAnd we can go back to the table in long format by calling the pivot_longer function:\n\nG_wide |&gt; \n  pivot_longer(\n    cols = starts_with(\"time_point\"),\n    names_to = \"timepoint\",\n    names_pattern = \"time_point(.)\",\n    values_to = \"x_g\"\n  ) |&gt; pander()\n\npivot_longer: reorganized (time_point1, time_point2) into (timepoint, x_g) [was 3x3, now 6x3]\n\n\n\n\n\n\n\n\n\n\nsample\ntimepoint\nx_g\n\n\n\n\na\n1\n1\n\n\na\n2\n2\n\n\nb\n1\n3\n\n\nb\n2\n4\n\n\nc\n1\n5\n\n\nc\n2\n6"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#section",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#section",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "",
    "text": "Workshop materials are at:\nhttps://elsherbini.github.io/durban-data-science-for-biology/"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#hello",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#hello",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Hello! 👋👋",
    "text": "Hello! 👋👋\nMy name is Laura Symul.\n🎓 I am an assistant professor in non-clinical biostatistics at the University of Louvain in Belgium.\n💙 I love statistics AND climbing, hiking, and painting."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#section-1",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#section-1",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "",
    "text": "🎯 Goals for this module\n\n\nUnderstand what hypothesis testing is\n\nLearn about parametric and non-parametric tests\nLearn how to perform (t-)tests in R\n\nUnderstand when and why to transform data\nUnderstand what is “multiple testing” & how to adjust for it.\nLearn how to display longitudinal data\nUnderstand the limitations inherent to compositional data"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#discussions-discord",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#discussions-discord",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "💬 Discussions: discord",
    "text": "💬 Discussions: discord\nAsk questions at #workshop-questions on https://discord.gg/UDAsYTzZE."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#wooclap",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#wooclap",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Wooclap",
    "text": "Wooclap\nGo to the event on wooclap"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#malva-pudding",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#malva-pudding",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Malva pudding",
    "text": "Malva pudding\n\n\nMy friend told me that Malva puddings in Durban are usually much sweeter than those sold in Cape Town.\n\n\nIs this true?"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#is-my-friend-right",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#is-my-friend-right",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Is my friend right?",
    "text": "Is my friend right?\nAre Malva puddings in Durban sweeter than those in Cape Town?\n\nHow do we find out?"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#malva-pudding-and-hypothesis-testing",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#malva-pudding-and-hypothesis-testing",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Malva pudding and hypothesis testing",
    "text": "Malva pudding and hypothesis testing\nWe have a claim:\n“Malva puddings in Durban are sweeter than those in Cape Town”.\n\nWe need to design and plan an experiment to collect data to test this claim.\n\n\nStatistically, this is hypothesis testing."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#hypothesis-testing",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#hypothesis-testing",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Hypothesis testing 🤔",
    "text": "Hypothesis testing 🤔\nWhat is hypothesis testing? 🤔\n\n\nHypothesis testing is a method for making decisions about the value of a population parameter.\n\n\n\n\nA population is not necessarily a group of humans. In statistics, a population is a group of individuals, objects, or measurements that are of interest to us. For example: the population of Malva puddings in Durban.\n\n\n\n\nA population parameter is a numerical value that describes a population. For example: the average sugar content of Malva puddings in Durban."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#hypothesis-testing-is-a-critical-step-of-the-scientific-method",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#hypothesis-testing-is-a-critical-step-of-the-scientific-method",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Hypothesis testing is a critical step of the scientific method",
    "text": "Hypothesis testing is a critical step of the scientific method\n\nState a claim that we want to verify (or disprove) 🤓\n\n\n\nDesign an experiment to test the claim 📝\n\n\n\n\nPlan and execute the experiment (= collect the data) 🔎🔎\n\n\n\n\nExploratory analysis of the data and visualization 📈📈\n\n\n\n\nHypothesis testing\n\n\n\n\nInterpretation (and predictions) 💁"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#how-many-samples-do-we-need",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#how-many-samples-do-we-need",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "How many samples do we need?",
    "text": "How many samples do we need?\nDo we need to measure the sugar content of ALL Malva puddings in Durban and Cape Town?\n🥮🥮🥮🥮🥮🥮🥮🥮🥮🥮🥮🥮🥮🥮🥮🥮🥮🥮🥮🥮🥮\n\n❌ No, we can to collect a representative sample from both populations.\n\n\n💡 The number of samples we need to collect depends on the variability of sugar content in both populations and the minimum difference in sugar we want to detect.\n\n\n🤓 Formally, we would need to do a power analysis to estimate the number of samples needed."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#malva-pudding-data",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#malva-pudding-data",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Malva pudding data",
    "text": "Malva pudding data\nLet’s pretend we have done a power analysis. 🤫\n\n→ We need to collect 20 samples from each “population”\n\n\nWe go to the lab and measure the sugar content of all 40 puddings. Let’s display this data."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#malva-puddings-conclusions",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#malva-puddings-conclusions",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Malva puddings: conclusions?",
    "text": "Malva puddings: conclusions?\nLet’s compute the mean and standard deviation for both groups.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntown\nmean sugar (%)\nsd\n\n\n\n\nDurban\n35.85\n3.92\n\n\nCape Town\n30.35\n4.78\n\n\n\n\n\n\n\n\nDo we think my friend was right?\n\n\n🤔 Could it not be just by chance that the average sugar content in Durban is higher than in Cape Town?"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#the-null-and-alternative-hypotheses.",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#the-null-and-alternative-hypotheses.",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "The Null and Alternative hypotheses.",
    "text": "The Null and Alternative hypotheses.\nFormally, in statistics, to do a test, we need to define a pair of hypotheses: the Null and the Alternative hypotheses.\n\n⚖️ The Null hypothesis (\\(H_0\\)) is the “status quo” claim: usually, it assumes no effect, or no differences between groups.\nFor example, the average sugar content in Malva puddings is the same in both towns:\n\\[H_0: \\mu_{C} = \\mu_{D}\\]"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#the-null-and-alternative-hypotheses.-1",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#the-null-and-alternative-hypotheses.-1",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "The Null and Alternative hypotheses.",
    "text": "The Null and Alternative hypotheses.\nThe alternative hypothesis (\\(H_a\\)) is the complement of the Null hypothesis. It is usually the hypothesis we want to prove.\n\nHere:\n\\[H_a: \\mu_C \\neq \\mu_D\\]"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#the-null-and-alternative-hypotheses.-2",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#the-null-and-alternative-hypotheses.-2",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "The Null and Alternative hypotheses.",
    "text": "The Null and Alternative hypotheses.\nWe can do “two-sided” tests, like above, or “one-sided” tests.\n\nIn “one-sided” tests, the Null and Alternative hypotheses are of the form:\n\\[H_0: \\mu_{C} \\geq \\mu_{D}\\] and\n\\[H_a: \\mu_C &lt; \\mu_D\\]\n\n\n(or the reverse)."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#the-null-and-alternative-hypothesis-for-malva-puddings",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#the-null-and-alternative-hypothesis-for-malva-puddings",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "The Null and alternative hypothesis for Malva puddings",
    "text": "The Null and alternative hypothesis for Malva puddings\n\n\\[H_0: \\mu_{D} \\leq \\mu_{C}\\] and\n\\[H_a: \\mu_D &gt; \\mu_C\\]\nThe alternative is my friend’s claim (Malva puddings in Durban are sweeter than those in Cape Town)."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#is-it-enough-to-just-compute-the-means",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#is-it-enough-to-just-compute-the-means",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Is it enough to just compute the means?",
    "text": "Is it enough to just compute the means?\n\n\n\n\n\ntown\nmean sugar (%)\nsd\n\n\n\n\nDurban\n35.85\n3.92\n\n\nCape Town\n30.35\n4.78\n\n\n\n\n\nWould it be possible to observe these values if the Null hypothesis was true?"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#the-p-value",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#the-p-value",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "The \\(p\\)-value",
    "text": "The \\(p\\)-value\n✨The \\(p\\)-value is the probability of observing a test statistic as extreme as the one we observed, under the Null hypothesis.\n\nThe \\(p\\)-value is a number between 0 and 1.\n\n\n0 = impossible / never\n1 = always"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#hypothesis-testing-the-test-statistic",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#hypothesis-testing-the-test-statistic",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Hypothesis testing: the test statistic",
    "text": "Hypothesis testing: the test statistic\nRemember:\nThe \\(p\\)-value is the probability of observing a test statistic as extreme as the one we observed, under the Null hypothesis.\n\n🤔 What is a test statistics?\nA test statistic is a numerical value that we compute from our data, and that we will use to make a decision about the Null hypothesis."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#hypothesis-testing-the-test-statistic-1",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#hypothesis-testing-the-test-statistic-1",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Hypothesis testing: the test statistic",
    "text": "Hypothesis testing: the test statistic\nA naive test statistic for comparing means of two populations could be simply to compute the difference in means.\n\nUnder the Null (= assuming the Null is true and \\(\\mu_D \\leq \\mu_C\\)), this difference should be small or below zero.\n\n\nSo a large positive value would be quite unlikely under the Null and have a small \\(p\\)-value."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#a-less-naive-test-statistic",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#a-less-naive-test-statistic",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "A less naive test statistic",
    "text": "A less naive test statistic\nA more sophisticated test statistic is the \\(t\\)-statistic which takes into account the variability in the data.\n\n\\[😱😱😱\\ \\ \\ \\  T  = \\frac{\\bar{x}_D - \\bar{x}_C}{\\sqrt{\\frac{S^2_D}{n_D} + \\frac{S^2_C}{n_C}}} \\ \\ \\ \\ 😱😱😱\\]\n\n\nIF\n\nthe two populations follow a normal distribution OR\nthe number of samples is larger than ~40 in each group\n\nTHEN this \\(T\\) test statistics follows a \\(t\\) distribution."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#back-to-the-pudding-data",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#back-to-the-pudding-data",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Back to the pudding data",
    "text": "Back to the pudding data\nLet’s compute the \\(t\\)-statistic for the Malva pudding data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntown\nmean sugar (%)\nsd\n\n\n\n\nDurban\n35.85\n3.92\n\n\nCape Town\n30.35\n4.78\n\n\n\n\n\n\n\nDifference in means: 5.5\n\\(t\\)-statistics: 3.976"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#the-t-distribution-under-the-null",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#the-t-distribution-under-the-null",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "The \\(t\\)-distribution under the Null",
    "text": "The \\(t\\)-distribution under the Null\nUnder the Null, the distribution of \\(t\\)-statistics is the \\(t\\) distribution (looks like a “narrower” normal distribution):\n\n\nIt is VERY unlikely to observe such a large \\(t\\) statistics assuming that the two average sugar contents are the same between the two towns."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#the-p-value-1",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#the-p-value-1",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "The \\(p\\)-value",
    "text": "The \\(p\\)-value\nRemember: The \\(p\\)-value is the probability of observing a test statistic as extreme as the one we observed, under the Null hypothesis."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#the-t-test",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#the-t-test",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "The \\(t\\)-test",
    "text": "The \\(t\\)-test\nWhat we just did is called a \\(t\\)-test.\n\nThe assumptions for a \\(t\\)-test are:\n\nthe two populations follow a normal distribution OR\nthe sample sizes are larger than ~40 in each group\n\n\n\n⚠️ Were we allowed to do a \\(t\\)-test for the pudding data?"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#normal-malva",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#normal-malva",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Normal Malva?",
    "text": "Normal Malva?\nDid we have more than 40 samples in each group?\n. . . Were our data normally distributed?"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#qq-what",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#qq-what",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "QQ-what?",
    "text": "QQ-what?\n\nA more formal way to check that data are normally distributed is to make a QQ-plot.\n\n\n\n\n\n\n\n\n\n\n\nIf the dots are close to the line, that indicates that the data is compatible with a normal distribution.\n\\(\\rightarrow\\) We can use the \\(t\\)-test ✅"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#the-t-test-in-r",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#the-t-test-in-r",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "The \\(t\\)-test in R",
    "text": "The \\(t\\)-test in R\nIn R, we can do a \\(t\\)-test using the function t.test.\n\nt.test(sugar ~ town, data = malva, alternative = \"greater\")\n\n\n    Welch Two Sample t-test\n\ndata:  sugar by town\nt = 3.9761, df = 36.604, p-value = 0.0001584\nalternative hypothesis: true difference in means between group Durban and group Cape Town is greater than 0\n95 percent confidence interval:\n 3.165654      Inf\nsample estimates:\n   mean in group Durban mean in group Cape Town \n                  35.85                   30.35"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#hands-on-exercises",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#hands-on-exercises",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "🫵 Hands-on exercises",
    "text": "🫵 Hands-on exercises\nRead the Module 6 Quarto document and execute all chunks until the “data transformation” section."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#enough-pudding-lets-science",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#enough-pudding-lets-science",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "🙅 Enough Pudding! Let’s science 🧪",
    "text": "🙅 Enough Pudding! Let’s science 🧪\nOur 1st scientific question is:\n“Does IL-1\\(\\beta\\)” (a cytokine) have a different concentration in samples from individuals diagnosed with BV (bacterial vaginosis) or not?”\n\nWhat is the Null and Alternative hypothesis?\n\n\n\\[H_0: \\mu_{BV} = \\mu_{H} \\]\n\\[H_a: \\mu_{BV} \\neq \\mu_{H}\\]"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#lets-display-the-data",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#lets-display-the-data",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Let’s display the data",
    "text": "Let’s display the data\n\nDoes this look normal?\n\nWhat should we do?"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#data-transformation",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#data-transformation",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Data transformation",
    "text": "Data transformation\n\nIL1b &lt;- IL1b |&gt; mutate(logconc = log10(conc))\n\n\nDoes this look normal?"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#qq-plot",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#qq-plot",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "QQ-plot",
    "text": "QQ-plot\n\nNot perfect, but not too bad."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#il-1beta-by-bv-status",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#il-1beta-by-bv-status",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "IL-1\\(\\beta\\) by BV status",
    "text": "IL-1\\(\\beta\\) by BV status"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#t-test",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#t-test",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "\\(t\\)-test?",
    "text": "\\(t\\)-test?\n\nIL1b |&gt; count(BV) |&gt; pivot_wider(names_from = BV, values_from = n)\n\n# A tibble: 1 × 2\n     BV Healthy\n  &lt;int&gt;   &lt;int&gt;\n1    46      86\n\n\n\nThe data roughly follow a normal distribution AND we have more than 40 samples in each group, so we can use a \\(t\\)-test:\n\n\n\nt.test(logconc ~ BV, data = IL1b)\n\n\n    Welch Two Sample t-test\n\ndata:  logconc by BV\nt = 6.5449, df = 116.4, p-value = 1.672e-09\nalternative hypothesis: true difference in means between group BV and group Healthy is not equal to 0\n95 percent confidence interval:\n 0.5332052 0.9959381\nsample estimates:\n     mean in group BV mean in group Healthy \n            1.1245273             0.3599557"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#hands-on-exercises-1",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#hands-on-exercises-1",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "🫵 Hands-on exercises",
    "text": "🫵 Hands-on exercises\nContinue on the Module 6 Quarto document and execute all chunks until the “Non-parametric tests” section."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#what-if",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#what-if",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "What if?",
    "text": "What if?\n😬 What if the \\(t\\)-test assumptions weren’t met?\n\n✅ We can use another test such as the Wilcoxon rank sum test"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#parametric-vs-non-parametric-tests",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#parametric-vs-non-parametric-tests",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Parametric vs non-parametric tests",
    "text": "Parametric vs non-parametric tests\nThe \\(t\\)-test is a parametric test.\n\n🤔 What does it mean?\n\n\nIt assumes that the populations can be described by a distribution characterized by a few parameters (e.g., mean and standard deviation).\nFor example: the Normal distribution: \\(N(\\mu, \\sigma^2)\\)"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#non-parametric-tests",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#non-parametric-tests",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Non-parametric tests",
    "text": "Non-parametric tests\nNon-parametric tests do not make any assumption about the distribution of the data.\n\nThe “equivalent” of the \\(t\\)-test is the Mann-Whitney U test or the Wilcoxon rank sum test.\n\n\nIn R, we can use the wilcox.test function to run this test."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#why-not-always-use-non-parametric-tests",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#why-not-always-use-non-parametric-tests",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Why not always use non-parametric tests?",
    "text": "Why not always use non-parametric tests?\nBecause these tests have less power than parametric tests.\n\nThe power of a test is the probability of rejecting the Null hypothesis when it is false.\n\n\nIt is the ability of a test to detect small but real effects."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#hands-on-exercises-2",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#hands-on-exercises-2",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "🫵 Hands-on exercises",
    "text": "🫵 Hands-on exercises\nContinue on the Module 6 Quarto document and execute all chunks until the “Multiple testing” section."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#break",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#break",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "☕️ BREAK",
    "text": "☕️ BREAK"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#hypothesis-testing-summary",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#hypothesis-testing-summary",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Hypothesis testing: summary",
    "text": "Hypothesis testing: summary\n\n\n☝️ The first and most important step is to have a clear and testable scientific question.\n⚖️ Then, one can define the null and alternative hypotheses.\n❓Depending on the question and the hypotheses, one must pick an appropriate statistical test or model.\n\n🙋 For complex questions/models, it might be necessary to consult a statistician.\n\n🧑‍💻 Most common tests have been implemented in R. Read their documentation carefully and check their assumptions."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#what-about-other-cytokines",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#what-about-other-cytokines",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "What about other cytokines?",
    "text": "What about other cytokines?\n🤔 Can we repeat the same test we did for IL-1\\(\\beta\\) for all the other cytokines?\n\n🚨 Yes, but be careful about multiple testing!"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#multiple-testing",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#multiple-testing",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Multiple testing",
    "text": "Multiple testing\nRemember the \\(p\\)-value definition:\nThe \\(p\\)-value is the probability of observing a test statistic as extreme as the one we observed, under the Null hypothesis.\n\nIf this probability is smaller than, let’s say 1/20 (0.05), we reject the Null hypothesis. But we still have 1 chance / 20 to be wrong!\n\n\nSo, as we perform many tests, we are more likely to obtain small \\(p\\)-values by chance."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#hands-on-exercises-3",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#hands-on-exercises-3",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "🫵 Hands-on exercises",
    "text": "🫵 Hands-on exercises\nExplore the effects of multiple testing using simulations and check the p.adjust function and the p.adjust.methods.\nContinue on the Module 6 Quarto document and execute all chunks until the “Displaying longitudinal data” section."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#longitudinal-design",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#longitudinal-design",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "➡️➡️➡️ Longitudinal design",
    "text": "➡️➡️➡️ Longitudinal design\nRemember the study design of the data we are analyzing:\n\n\nShould we try to display the data in a way that highlights the “trajectories” of participants along the study?"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#remember-left_join",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#remember-left_join",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "🧠 Remember: left_join",
    "text": "🧠 Remember: left_join\nFirst, we need to join the cytokine data with the sample data so we have the study arm and the time-point for each sample:\n\nelisa &lt;- \n  elisa |&gt; \n  mutate(logconc = log10(conc)) |&gt; \n  left_join(sample_info, by = join_by(sample_id))\n\nelisa |&gt; head() \n\n# A tibble: 6 × 8\n  sample_id cytokine    conc limits        logconc pid    time_point arm    \n  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;  \n1 SAMP094   IL-1a    174.    within limits   2.24  pid_01 baseline   placebo\n2 SAMP094   IL-10      0.767 out of range   -0.115 pid_01 baseline   placebo\n3 SAMP094   IL-1b      5.39  within limits   0.732 pid_01 baseline   placebo\n4 SAMP094   IL-8      48.3   within limits   1.68  pid_01 baseline   placebo\n5 SAMP094   IL-6       5.07  within limits   0.705 pid_01 baseline   placebo\n6 SAMP094   TNFa       0.471 out of range   -0.327 pid_01 baseline   placebo"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#bonus-character-string-manipulation",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#bonus-character-string-manipulation",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "🎁 Bonus: character string manipulation",
    "text": "🎁 Bonus: character string manipulation\nSince we’ll want to display the visit data, we can “polish” the labels of the time-point column so that they read nicely.\nRight now, the visit labels are:\n\n\n[1] \"baseline\" \"week_1\"   \"week_7\"  \n\n\n\nWe can use the str_replace function from the stringr package to replace the underscores with spaces:\n\nelisa &lt;- \n  elisa |&gt; \n  mutate(time_point = time_point |&gt; str_replace(\"_\", \" \"))\n\nelisa$time_point |&gt; unique()\n\n[1] \"baseline\" \"week 1\"   \"week 7\""
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#remember-factor",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#remember-factor",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "🧠 Remember: factor",
    "text": "🧠 Remember: factor\n\n\n# A tibble: 6 × 8\n  sample_id cytokine    conc limits        logconc pid    time_point arm    \n  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;  \n1 SAMP094   IL-1a    174.    within limits   2.24  pid_01 baseline   placebo\n2 SAMP094   IL-10      0.767 out of range   -0.115 pid_01 baseline   placebo\n3 SAMP094   IL-1b      5.39  within limits   0.732 pid_01 baseline   placebo\n4 SAMP094   IL-8      48.3   within limits   1.68  pid_01 baseline   placebo\n5 SAMP094   IL-6       5.07  within limits   0.705 pid_01 baseline   placebo\n6 SAMP094   TNFa       0.471 out of range   -0.327 pid_01 baseline   placebo\n\n\nWhich columns should be converted to factors?\n\n\nelisa &lt;- \n  elisa |&gt; \n  mutate(\n    sample_id = sample_id |&gt; factor(),\n    cytokine = cytokine |&gt; factor(),\n    pid = pid |&gt; factor(),\n    time_point =time_point |&gt; factor(),\n    arm = arm |&gt; factor(),\n    limits = limits |&gt; factor()\n    )"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#il-1beta-longitudinal-patterns",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#il-1beta-longitudinal-patterns",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "IL-1\\(\\beta\\) longitudinal patterns",
    "text": "IL-1\\(\\beta\\) longitudinal patterns\n\nelisa |&gt; \n  filter(cytokine == \"IL-1b\") |&gt; \n  ggplot(aes(x = time_point, y = logconc, color = arm)) +\n  geom_line(aes(group = pid), alpha = 0.75) +\n  geom_point(alpha = 0.5) +\n  xlab(\"Visits\") +\n  ylab(\"log10(IL-1b concentration)\") +\n  scale_color_manual(\"Arm\", values = c(\"deeppink\",\"steelblue1\"))"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#hands-on-exercises-4",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#hands-on-exercises-4",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "🫵 Hands-on exercises",
    "text": "🫵 Hands-on exercises\nContinue on the Module 6 Quarto document and execute all chunks until the “Compositional data” section.\n🎁 Bonus: there is a section on “paired tests” that you can read if you want to learn more about the paired \\(t\\)-test and the Wilcoxon signed rank test."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#compositional-data",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#compositional-data",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "☯ Compositional data",
    "text": "☯ Compositional data\nWhat are compositional data?\n\nMultivariate data are compositional when the variables are expressed as proportions of a whole.\n\n\nRemember the flow cytometry data?\nSometimes, data are expressed as “proportions of cells X among all cells”."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#whats-the-problem-with-compositional-data",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#whats-the-problem-with-compositional-data",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "⚠️ What’s the problem with compositional data?",
    "text": "⚠️ What’s the problem with compositional data?\n\nThe proportions are not independent.\n\n\nIf we have 3 cell types (A, B, and C) and in one condition, only cells A increase in number, when expressed “compositionally”, it may look like cells B and C are decreasing.\n\n\n\ncondition_1 &lt;- c(\"A\" = 102, \"B\" = 239, \"C\" = 163)\ncondition_2 &lt;- condition_1\ncondition_2[\"A\"] &lt;- condition_2[\"A\"] + 78 # we only increase A\n\n(condition_1/sum(condition_1)) |&gt; round(digits = 2)\n\n   A    B    C \n0.20 0.47 0.32 \n\n(condition_2/sum(condition_2)) |&gt; round(digits = 2) # but the % of B and C decrease!\n\n   A    B    C \n0.31 0.41 0.28"
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#potential-solutions",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#potential-solutions",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Potential solutions",
    "text": "Potential solutions\nThere is no “one-size-fits-all” for compositional data.\n\nIn some cases, some smart solutions have been proposed.\n\n\nFor example, for RNA-seq data, the DESeq2 package uses a “regularized log” transformation that allows to use standard statistical tests.\nThat transformation assumes that most genes are not differentially expressed but that only works for datasets with a large number of features."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#see-you-at-the-next-workshop",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#see-you-at-the-next-workshop",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "📆 See you at the next workshop!",
    "text": "📆 See you at the next workshop!\nIf you are interested in analysing compositional data such as gene expression data or microbiome data, register for the next workshop!\n\n🫵 In the meanwhile, there are some examples and exercises in the Module 6 Quarto document that you can try."
  },
  {
    "objectID": "materials/1-workshop1/6-hypothesis-testing/slides.html#module-6-summary",
    "href": "materials/1-workshop1/6-hypothesis-testing/slides.html#module-6-summary",
    "title": "Hypothesis testing, data transformation, longitudinal displays",
    "section": "Module 6 Summary",
    "text": "Module 6 Summary\n☝️ Be clear with the scientific questions you have\n\n📆 Plan your experiments and collect your data so you can answer your questions\n\n\n☝️ Get to know your data\n\n\n📊 Make your \"table 1\", check for confounders\n\n\n📈 Do exploratory visualizations\n\n\n🤓 Answer your questions\n\n\n📈 Display your data in a way that highlights the answer to your questions\n\n\n📊 Use the appropriate statistical tests (and consult a statistician *before* and after collecting your data if your question requires something more complex than a two-sample test)\n\n\n\nback to module"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#uses-of-color-in-data-visualization",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#uses-of-color-in-data-visualization",
    "title": "Color Scales",
    "section": "Uses of color in data visualization",
    "text": "Uses of color in data visualization\n\nDistinguish categories (qualitative)"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#qualitative-scale-example",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#qualitative-scale-example",
    "title": "Color Scales",
    "section": "Qualitative scale example",
    "text": "Qualitative scale example\n\nHumans can only distinguish 7 or 8 colors."
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#qualitative-scale-example-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#qualitative-scale-example-1",
    "title": "Color Scales",
    "section": "Qualitative scale example",
    "text": "Qualitative scale example\n\nPalette name: Okabe-Ito"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#qualitative-scale-example-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#qualitative-scale-example-2",
    "title": "Color Scales",
    "section": "Qualitative scale example",
    "text": "Qualitative scale example\n\nPalette name: ColorBrewer Set1"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#qualitative-scale-example-3",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#qualitative-scale-example-3",
    "title": "Color Scales",
    "section": "Qualitative scale example",
    "text": "Qualitative scale example\n\nPalette name: ColorBrewer Set3"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#uses-of-color-in-data-visualization-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#uses-of-color-in-data-visualization-1",
    "title": "Color Scales",
    "section": "Uses of color in data visualization",
    "text": "Uses of color in data visualization\n\n\nDistinguish categories (qualitative)\n\n\n\n\n\n\n\nRepresent numeric values (sequential)"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#sequential-scale-example",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#sequential-scale-example",
    "title": "Color Scales",
    "section": "Sequential scale example",
    "text": "Sequential scale example\n\nPalette name: Viridis"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#sequential-scale-example-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#sequential-scale-example-1",
    "title": "Color Scales",
    "section": "Sequential scale example",
    "text": "Sequential scale example\n\nPalette name: Inferno"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#sequential-scale-example-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#sequential-scale-example-2",
    "title": "Color Scales",
    "section": "Sequential scale example",
    "text": "Sequential scale example\n\nPalette name: Cividis"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#uses-of-color-in-data-visualization-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#uses-of-color-in-data-visualization-2",
    "title": "Color Scales",
    "section": "Uses of color in data visualization",
    "text": "Uses of color in data visualization\n\n\nDistinguish categories (qualitative)\n\n\n\n\n\n\n\nRepresent numeric values (sequential)\n\n\n\n\n\n\n\nRepresent numeric values (diverging)"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#diverging-scale-example",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#diverging-scale-example",
    "title": "Color Scales",
    "section": "Diverging scale example",
    "text": "Diverging scale example\n\nPalette name: ColorBrewer PiYG"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#diverging-scale-example-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#diverging-scale-example-1",
    "title": "Color Scales",
    "section": "Diverging scale example",
    "text": "Diverging scale example\n\nPalette name: Carto Earth"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#diverging-scale-example-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#diverging-scale-example-2",
    "title": "Color Scales",
    "section": "Diverging scale example",
    "text": "Diverging scale example\n\nPalette name: Blue-Red"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#uses-of-color-in-data-visualization-3",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#uses-of-color-in-data-visualization-3",
    "title": "Color Scales",
    "section": "Uses of color in data visualization",
    "text": "Uses of color in data visualization\n\n\nDistinguish categories (qualitative)\n\n\n\n\n\n\n\nRepresent numeric values (sequential)\n\n\n\n\n\n\n\nRepresent numeric values (diverging)\n\n\n\n\n\n\n\nHighlight"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#highlight-example",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#highlight-example",
    "title": "Color Scales",
    "section": "Highlight example",
    "text": "Highlight example\n\nPalette name: Grays with accents"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#highlight-example-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#highlight-example-1",
    "title": "Color Scales",
    "section": "Highlight example",
    "text": "Highlight example\n\nPalette name: Okabe-Ito accent"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#highlight-example-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#highlight-example-2",
    "title": "Color Scales",
    "section": "Highlight example",
    "text": "Highlight example\n\nPalette name: ColorBrewer accent"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#uses-of-color-in-data-visualization-4",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#uses-of-color-in-data-visualization-4",
    "title": "Color Scales",
    "section": "Uses of color in data visualization",
    "text": "Uses of color in data visualization\n\n\nDistinguish categories (qualitative)\n\n\n\n\n\n\n\nRepresent numeric values (sequential)\n\n\n\n\n\n\n\nRepresent numeric values (diverging)\n\n\n\n\n\n\n\nHighlight"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#ggplot2-color-scale-functions-are-a-bit-of-a-mess",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#ggplot2-color-scale-functions-are-a-bit-of-a-mess",
    "title": "Color Scales",
    "section": "ggplot2 color scale functions are a bit of a mess",
    "text": "ggplot2 color scale functions are a bit of a mess\n\n\n\n\n\n\n\n\n\nScale function\nAesthetic    \nData type\nPalette type\n\n\n\n\nscale_color_hue()\ncolor\ndiscrete\nqualitative"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#ggplot2-color-scale-functions-are-a-bit-of-a-mess-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#ggplot2-color-scale-functions-are-a-bit-of-a-mess-1",
    "title": "Color Scales",
    "section": "ggplot2 color scale functions are a bit of a mess",
    "text": "ggplot2 color scale functions are a bit of a mess\n\n\n\n\n\n\n\n\n\nScale function\nAesthetic    \nData type\nPalette type\n\n\n\n\nscale_color_hue()\ncolor\ndiscrete\nqualitative\n\n\nscale_fill_hue()\nfill\ndiscrete\nqualitative"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#ggplot2-color-scale-functions-are-a-bit-of-a-mess-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#ggplot2-color-scale-functions-are-a-bit-of-a-mess-2",
    "title": "Color Scales",
    "section": "ggplot2 color scale functions are a bit of a mess",
    "text": "ggplot2 color scale functions are a bit of a mess\n\n\n\n\n\n\n\n\n\nScale function\nAesthetic    \nData type\nPalette type\n\n\n\n\nscale_color_hue()\ncolor\ndiscrete\nqualitative\n\n\nscale_fill_hue()\nfill\ndiscrete\nqualitative\n\n\nscale_color_gradient()\ncolor\ncontinuous\nsequential"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#ggplot2-color-scale-functions-are-a-bit-of-a-mess-3",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#ggplot2-color-scale-functions-are-a-bit-of-a-mess-3",
    "title": "Color Scales",
    "section": "ggplot2 color scale functions are a bit of a mess",
    "text": "ggplot2 color scale functions are a bit of a mess\n\n\n\n\n\n\n\n\n\nScale function\nAesthetic    \nData type\nPalette type\n\n\n\n\nscale_color_hue()\ncolor\ndiscrete\nqualitative\n\n\nscale_fill_hue()\nfill\ndiscrete\nqualitative\n\n\nscale_color_gradient()\ncolor\ncontinuous\nsequential\n\n\nscale_color_gradient2()\ncolor\ncontinuous\ndiverging"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#ggplot2-color-scale-functions-are-a-bit-of-a-mess-4",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#ggplot2-color-scale-functions-are-a-bit-of-a-mess-4",
    "title": "Color Scales",
    "section": "ggplot2 color scale functions are a bit of a mess",
    "text": "ggplot2 color scale functions are a bit of a mess\n\n\n\n\n\n\n\n\n\nScale function\nAesthetic    \nData type\nPalette type\n\n\n\n\nscale_color_hue()\ncolor\ndiscrete\nqualitative\n\n\nscale_fill_hue()\nfill\ndiscrete\nqualitative\n\n\nscale_color_gradient()\ncolor\ncontinuous\nsequential\n\n\nscale_color_gradient2()\ncolor\ncontinuous\ndiverging\n\n\nscale_fill_viridis_c()\ncolor\ncontinuous\nsequential\n\n\nscale_fill_viridis_d()\nfill\ndiscrete\nsequential\n\n\nscale_color_brewer()\ncolor\ndiscrete\nqualitative, diverging, sequential\n\n\nscale_fill_brewer()\nfill\ndiscrete\nqualitative, diverging, sequential\n\n\nscale_color_distiller()\ncolor\ncontinuous\nqualitative, diverging, sequential\n\n\n\n… and there are many many more"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#examples",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#examples",
    "title": "Color Scales",
    "section": "Examples",
    "text": "Examples\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile(width = 0.95, height = 0.95) + \n  coord_fixed(expand = FALSE) +\n  theme_classic()\n\n  # no fill scale defined, default is scale_fill_gradient()"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#examples-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#examples-1",
    "title": "Color Scales",
    "section": "Examples",
    "text": "Examples\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile(width = 0.95, height = 0.95) + \n  coord_fixed(expand = FALSE) +\n  theme_classic() +\n  scale_fill_gradient()"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#examples-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#examples-2",
    "title": "Color Scales",
    "section": "Examples",
    "text": "Examples\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile(width = 0.95, height = 0.95) + \n  coord_fixed(expand = FALSE) +\n  theme_classic() +\n  scale_fill_viridis_c()"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#examples-3",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#examples-3",
    "title": "Color Scales",
    "section": "Examples",
    "text": "Examples\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile(width = 0.95, height = 0.95) + \n  coord_fixed(expand = FALSE) +\n  theme_classic() +\n  scale_fill_viridis_c(option = \"B\", begin = 0.15)"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#examples-4",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#examples-4",
    "title": "Color Scales",
    "section": "Examples",
    "text": "Examples\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile(width = 0.95, height = 0.95) + \n  coord_fixed(expand = FALSE) +\n  theme_classic() +\n  scale_fill_distiller(palette = \"YlGnBu\")"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#the-colorspace-package-creates-some-order",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#the-colorspace-package-creates-some-order",
    "title": "Color Scales",
    "section": "The colorspace package creates some order",
    "text": "The colorspace package creates some order\nScale name: scale_&lt;aesthetic&gt;_&lt;datatype&gt;_&lt;colorscale&gt;()\n\n&lt;aesthetic&gt;: name of the aesthetic (fill, color, colour)\n&lt;datatype&gt;: type of variable plotted (discrete, continuous, binned)\n&lt;colorscale&gt;: type of the color scale (qualitative, sequential, diverging, divergingx)\n\n\n\n\n\n\n\n\n\n\nScale function\nAesthetic    \nData type\nPalette type    \n\n\n\n\nscale_color_discrete_qualitative()\ncolor\ndiscrete\nqualitative\n\n\nscale_fill_continuous_sequential()\nfill\ncontinuous\nsequential\n\n\nscale_colour_continous_divergingx()\ncolour\ncontinuous\ndiverging"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#examples-5",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#examples-5",
    "title": "Color Scales",
    "section": "Examples",
    "text": "Examples\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile(width = 0.95, height = 0.95) + \n  coord_fixed(expand = FALSE) +\n  theme_classic() +\n  scale_fill_continuous_sequential(palette = \"YlGnBu\", rev = FALSE)"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#examples-6",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#examples-6",
    "title": "Color Scales",
    "section": "Examples",
    "text": "Examples\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile(width = 0.95, height = 0.95) + \n  coord_fixed(expand = FALSE) +\n  theme_classic() +\n  scale_fill_continuous_sequential(palette = \"Viridis\", rev = FALSE)"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#examples-7",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#examples-7",
    "title": "Color Scales",
    "section": "Examples",
    "text": "Examples\n\nggplot(temps_months, aes(x = month, y = location, fill = mean)) + \n  geom_tile(width = 0.95, height = 0.95) + \n  coord_fixed(expand = FALSE) +\n  theme_classic() +\n  scale_fill_continuous_sequential(palette = \"Inferno\", begin = 0.15, rev = FALSE)"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#section",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#section",
    "title": "Color Scales",
    "section": "",
    "text": "colorspace::hcl_palettes(type = \"sequential\", plot = TRUE) # all sequential palettes"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#section-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#section-1",
    "title": "Color Scales",
    "section": "",
    "text": "colorspace::hcl_palettes(type = \"diverging\", plot = TRUE, n = 9) # all diverging palettes"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#section-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#section-2",
    "title": "Color Scales",
    "section": "",
    "text": "colorspace::divergingx_palettes(plot = TRUE, n = 9) # all divergingx palettes"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#discrete-qualitative-scales-are-best-set-manually",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#discrete-qualitative-scales-are-best-set-manually",
    "title": "Color Scales",
    "section": "Discrete, qualitative scales are best set manually",
    "text": "Discrete, qualitative scales are best set manually\n\nggplot(popgrowth, aes(x = pop2000, y = popgrowth, color = region)) +\n  geom_point() +\n  scale_x_log10()\n\n  # no color scale defined, default is scale_color_hue()"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#discrete-qualitative-scales-are-best-set-manually-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#discrete-qualitative-scales-are-best-set-manually-1",
    "title": "Color Scales",
    "section": "Discrete, qualitative scales are best set manually",
    "text": "Discrete, qualitative scales are best set manually\n\nggplot(popgrowth, aes(x = pop2000, y = popgrowth, color = region)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_color_hue()"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#discrete-qualitative-scales-are-best-set-manually-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#discrete-qualitative-scales-are-best-set-manually-2",
    "title": "Color Scales",
    "section": "Discrete, qualitative scales are best set manually",
    "text": "Discrete, qualitative scales are best set manually\n\nlibrary(ggthemes)  # for scale_color_colorblind()\n\nggplot(popgrowth, aes(x = pop2000, y = popgrowth, color = region)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_color_colorblind()  # uses Okabe-Ito colors"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#discrete-qualitative-scales-are-best-set-manually-3",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#discrete-qualitative-scales-are-best-set-manually-3",
    "title": "Color Scales",
    "section": "Discrete, qualitative scales are best set manually",
    "text": "Discrete, qualitative scales are best set manually\n\nggplot(popgrowth, aes(x = pop2000, y = popgrowth, color = region)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_color_manual(\n    values = c(West = \"#E69F00\", South = \"#56B4E9\", Midwest = \"#009E73\", Northeast = \"#F0E442\")\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#okabe-ito-rgb-codes",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#okabe-ito-rgb-codes",
    "title": "Color Scales",
    "section": "Okabe-Ito RGB codes",
    "text": "Okabe-Ito RGB codes\n\n\n\n\nName\nHex code   \nR, G, B (0-255)\n\n\n\n\norange\n#E69F00\n230, 159, 0\n\n\nsky blue\n#56B4E9\n86, 180, 233\n\n\nbluish green\n#009E73\n0, 158, 115\n\n\nyellow\n#F0E442\n240, 228, 66\n\n\nblue\n#0072B2\n0, 114, 178\n\n\nvermilion\n#D55E00\n213, 94, 0\n\n\nreddish purple\n#CC79A7\n204, 121, 167\n\n\nblack\n#000000\n0, 0, 0"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#be-aware-of-color-vision-deficiency",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#be-aware-of-color-vision-deficiency",
    "title": "Color Scales",
    "section": "Be aware of color-vision deficiency",
    "text": "Be aware of color-vision deficiency\n5%–8% of men are color blind!\n\nRed-green color-vision deficiency is the most common"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#be-aware-of-color-vision-deficiency-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#be-aware-of-color-vision-deficiency-1",
    "title": "Color Scales",
    "section": "Be aware of color-vision deficiency",
    "text": "Be aware of color-vision deficiency\n5%–8% of men are color blind!\n\nBlue-green color-vision deficiency is rare but does occur"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#be-aware-of-color-vision-deficiency-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#be-aware-of-color-vision-deficiency-2",
    "title": "Color Scales",
    "section": "Be aware of color-vision deficiency",
    "text": "Be aware of color-vision deficiency\nChoose colors that can be distinguished with CVD"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#consider-using-the-okabe-ito-scale-as-your-default",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#consider-using-the-okabe-ito-scale-as-your-default",
    "title": "Color Scales",
    "section": "Consider using the Okabe-Ito scale as your default",
    "text": "Consider using the Okabe-Ito scale as your default\n\n\n\n\nName\nHex code   \nR, G, B (0-255)\n\n\n\n\norange\n#E69F00\n230, 159, 0\n\n\nsky blue\n#56B4E9\n86, 180, 233\n\n\nbluish green\n#009E73\n0, 158, 115\n\n\nyellow\n#F0E442\n240, 228, 66\n\n\nblue\n#0072B2\n0, 114, 178\n\n\nvermilion\n#D55E00\n213, 94, 0\n\n\nreddish purple\n#CC79A7\n204, 121, 167\n\n\nblack\n#000000\n0, 0, 0"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#cvd-is-worse-for-thin-lines-and-tiny-dots",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#cvd-is-worse-for-thin-lines-and-tiny-dots",
    "title": "Color Scales",
    "section": "CVD is worse for thin lines and tiny dots",
    "text": "CVD is worse for thin lines and tiny dots"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#when-in-doubt-run-cvd-simulations",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#when-in-doubt-run-cvd-simulations",
    "title": "Color Scales",
    "section": "When in doubt, run CVD simulations",
    "text": "When in doubt, run CVD simulations"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#when-in-doubt-run-cvd-simulations-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/color_scales_slides.html#when-in-doubt-run-cvd-simulations-1",
    "title": "Color Scales",
    "section": "When in doubt, run CVD simulations",
    "text": "When in doubt, run CVD simulations\n\n\n\n\n\n\n\n\n\n\n\nthe colorspace\npackage can\nhelp"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#compound-figures",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#compound-figures",
    "title": "Compound Figures",
    "section": "Compound figures",
    "text": "Compound figures\nTwo common scenarios:"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#compound-figures-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#compound-figures-1",
    "title": "Compound Figures",
    "section": "Compound figures",
    "text": "Compound figures\nTwo common scenarios:\n\nThe same type of plot is replicated many times (small multiples)"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#compound-figures-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#compound-figures-2",
    "title": "Compound Figures",
    "section": "Compound figures",
    "text": "Compound figures\nTwo common scenarios:\n\nThe same type of plot is replicated many times (small multiples)\nSeveral disparate plots are combined into one display"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#small-multiples-facets",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#small-multiples-facets",
    "title": "Compound Figures",
    "section": "1. Small multiples (facets)",
    "text": "1. Small multiples (facets)"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#small-multiples-facets-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#small-multiples-facets-1",
    "title": "Compound Figures",
    "section": "1. Small multiples (facets)",
    "text": "1. Small multiples (facets)"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#small-multiples-facets-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#small-multiples-facets-2",
    "title": "Compound Figures",
    "section": "1. Small multiples (facets)",
    "text": "1. Small multiples (facets)\n\nAvoid bars or other elements that are floating in space"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#small-multiples-facets-3",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#small-multiples-facets-3",
    "title": "Compound Figures",
    "section": "1. Small multiples (facets)",
    "text": "1. Small multiples (facets)"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#y-axis-ranges-should-be-consistent-among-panels",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#y-axis-ranges-should-be-consistent-among-panels",
    "title": "Compound Figures",
    "section": "y-axis ranges should be consistent among panels",
    "text": "y-axis ranges should be consistent among panels"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#y-axis-ranges-should-be-consistent-among-panels-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#y-axis-ranges-should-be-consistent-among-panels-1",
    "title": "Compound Figures",
    "section": "y-axis ranges should be consistent among panels",
    "text": "y-axis ranges should be consistent among panels"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#combining-disparate-figures-into-one-display",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#combining-disparate-figures-into-one-display",
    "title": "Compound Figures",
    "section": "2. Combining disparate figures into one display",
    "text": "2. Combining disparate figures into one display"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#dont-use-overly-large-or-otherwise-prominent-labels",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#dont-use-overly-large-or-otherwise-prominent-labels",
    "title": "Compound Figures",
    "section": "Don’t use overly large or otherwise prominent labels",
    "text": "Don’t use overly large or otherwise prominent labels"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#use-a-consistent-color-language-among-sub-plots",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#use-a-consistent-color-language-among-sub-plots",
    "title": "Compound Figures",
    "section": "Use a consistent color language among sub-plots",
    "text": "Use a consistent color language among sub-plots"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#use-a-consistent-color-language-among-sub-plots-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#use-a-consistent-color-language-among-sub-plots-1",
    "title": "Compound Figures",
    "section": "Use a consistent color language among sub-plots",
    "text": "Use a consistent color language among sub-plots"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#pay-attention-to-sub-plot-alignment",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#pay-attention-to-sub-plot-alignment",
    "title": "Compound Figures",
    "section": "Pay attention to sub-plot alignment",
    "text": "Pay attention to sub-plot alignment"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#pay-attention-to-sub-plot-alignment-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#pay-attention-to-sub-plot-alignment-1",
    "title": "Compound Figures",
    "section": "Pay attention to sub-plot alignment",
    "text": "Pay attention to sub-plot alignment"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#combine-plots-of-different-types",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#combine-plots-of-different-types",
    "title": "Compound Figures",
    "section": "Combine plots of different types",
    "text": "Combine plots of different types\n\nThis helps your readers to distinguish different parts of the analysis"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#combine-plots-of-different-types-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#combine-plots-of-different-types-1",
    "title": "Compound Figures",
    "section": "Combine plots of different types",
    "text": "Combine plots of different types\n\nThis helps your readers to distinguish different parts of the analysis"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#combine-plots-of-different-types-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#combine-plots-of-different-types-2",
    "title": "Compound Figures",
    "section": "Combine plots of different types",
    "text": "Combine plots of different types\n\nThis helps your readers to distinguish different parts of the analysis"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#distinguish-infographics-from-figures-in-articlebook",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#distinguish-infographics-from-figures-in-articlebook",
    "title": "Compound Figures",
    "section": "Distinguish infographics from figures in article/book",
    "text": "Distinguish infographics from figures in article/book\n\nThere are two distinct use cases:"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#distinguish-infographics-from-figures-in-articlebook-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#distinguish-infographics-from-figures-in-articlebook-1",
    "title": "Compound Figures",
    "section": "Distinguish infographics from figures in article/book",
    "text": "Distinguish infographics from figures in article/book\n\nThere are two distinct use cases:\n\nInfographic: Standalone, has title/subtitle/caption"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#distinguish-infographics-from-figures-in-articlebook-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#distinguish-infographics-from-figures-in-articlebook-2",
    "title": "Compound Figures",
    "section": "Distinguish infographics from figures in article/book",
    "text": "Distinguish infographics from figures in article/book\n\nThere are two distinct use cases:\n\nInfographic: Standalone, has title/subtitle/caption\n\nFigure in article/book: Caption contains title, not part of figure"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#section-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#section-1",
    "title": "Compound Figures",
    "section": "",
    "text": "Example of infographic"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#section-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#section-2",
    "title": "Compound Figures",
    "section": "",
    "text": "Figure 1. Corruption and human development. The most developed countries experience the least corruption. Inspired by a posting in The Economist online (2011). Data sources: Transparency International & UN Human Development Report.\n\n\n\n\nExample of figure in article or book"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#the-patchwork-package",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#the-patchwork-package",
    "title": "Compound Figures",
    "section": "The patchwork package",
    "text": "The patchwork package\n\n\nlibrary(patchwork)\n\n# make first plot\np1 &lt;- ggplot(mtcars) + \n  geom_point(aes(mpg, disp))\n\np1"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#the-patchwork-package-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#the-patchwork-package-1",
    "title": "Compound Figures",
    "section": "The patchwork package",
    "text": "The patchwork package\n\n\nlibrary(patchwork)\n\n# make first plot\np1 &lt;- ggplot(mtcars) + \n  geom_point(aes(mpg, disp))\n\n# make second plot\np2 &lt;- ggplot(mtcars) + \n  aes(gear, disp, group = gear) +\n  geom_boxplot()\n\np2"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#the-patchwork-package-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#the-patchwork-package-2",
    "title": "Compound Figures",
    "section": "The patchwork package",
    "text": "The patchwork package\n\n\nlibrary(patchwork)\n\n# make first plot\np1 &lt;- ggplot(mtcars) + \n  geom_point(aes(mpg, disp))\n\n# make second plot\np2 &lt;- ggplot(mtcars) + \n  aes(gear, disp, group = gear) +\n  geom_boxplot()\n\n# place plots side-by-side \np1 | p2"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#the-patchwork-package-3",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#the-patchwork-package-3",
    "title": "Compound Figures",
    "section": "The patchwork package",
    "text": "The patchwork package\n\n\nlibrary(patchwork)\n\n# make first plot\np1 &lt;- ggplot(mtcars) + \n  geom_point(aes(mpg, disp))\n\n# make second plot\np2 &lt;- ggplot(mtcars) + \n  aes(gear, disp, group = gear) +\n  geom_boxplot()\n\n# place plots on top of one-another \np1 / p2"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#the-patchwork-package-4",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#the-patchwork-package-4",
    "title": "Compound Figures",
    "section": "The patchwork package",
    "text": "The patchwork package\n\n\n# add a few more plots\np3 &lt;- ggplot(mtcars) + \n  geom_smooth(aes(disp, qsec))\n\np4 &lt;- ggplot(mtcars) + \n  geom_bar(aes(carb))\n\n# make complex arrangement \n(p1 | p2 | p3) / p4"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#plot-annotations-and-themes",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#plot-annotations-and-themes",
    "title": "Compound Figures",
    "section": "Plot annotations and themes",
    "text": "Plot annotations and themes\n\n\n(p1 | p2 | p3) / p4 +\n   plot_annotation( \n     tag_levels = \"A\" \n   ) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic labeling of plots"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#plot-annotations-and-themes-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#plot-annotations-and-themes-1",
    "title": "Compound Figures",
    "section": "Plot annotations and themes",
    "text": "Plot annotations and themes\n\n\n(p1 | p2 | p3) / p4 +\n   plot_annotation( \n     tag_levels = \"a\" \n   ) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic labeling of plots"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#plot-annotations-and-themes-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#plot-annotations-and-themes-2",
    "title": "Compound Figures",
    "section": "Plot annotations and themes",
    "text": "Plot annotations and themes\n\n\n(p1 | p2 | p3) / p4 +\n  plot_annotation(\n   tag_levels = \"a\"\n  ) & \n  theme_minimal_grid() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplying one theme to all plots"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#plot-annotations-and-themes-3",
    "href": "materials/1-workshop1/7-custom-data-visualizations/compound_figures_slides.html#plot-annotations-and-themes-3",
    "title": "Compound Figures",
    "section": "Plot annotations and themes",
    "text": "Plot annotations and themes\n\n\n(p1 | p2 | p3) / p4 +\n  plot_annotation(\n   tag_levels = \"a\",\n   title = \"A plot about mtcars\", \n   subtitle = \"With subtitle...\", \n   caption = \"...and caption\" \n  ) &\n  theme_minimal_grid()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitles and captions"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#section",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#section",
    "title": "Figure Design",
    "section": "",
    "text": "How to go from this …\n\n\n… to this?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRequires coordinated modification of multiple elements:\n- geoms (via arguments to geoms)\n- scales (via scale_*() functions)\n- plot appearance (via themes)"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#the-starting-point-a-rough-draft",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#the-starting-point-a-rough-draft",
    "title": "Figure Design",
    "section": "The starting point, a rough draft",
    "text": "The starting point, a rough draft\n\n\nggplot(lincoln_temps) +\n  aes(x = mean_temp, y = month_long) +\n  geom_density_ridges()\nlincoln_temps &lt;- readRDS(\n  url(\"https://wilkelab.org/DSC385/datasets/lincoln_temps.rds\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can download the dataset using this code:"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#set-scale-and-bandwidth-to-shape-ridgelines",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#set-scale-and-bandwidth-to-shape-ridgelines",
    "title": "Figure Design",
    "section": "Set scale and bandwidth to shape ridgelines",
    "text": "Set scale and bandwidth to shape ridgelines\n\n\nggplot(lincoln_temps) +\n  aes(x = mean_temp, y = month_long) +\n  geom_density_ridges(\n    scale = 3, bandwidth = 3.4\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#set-rel_min_height-to-cut-ridgelines-near-zero",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#set-rel_min_height-to-cut-ridgelines-near-zero",
    "title": "Figure Design",
    "section": "Set rel_min_height to cut ridgelines near zero",
    "text": "Set rel_min_height to cut ridgelines near zero\n\n\nggplot(lincoln_temps) +\n  aes(x = mean_temp, y = month_long) +\n  geom_density_ridges(\n    scale = 3, bandwidth = 3.4,\n    rel_min_height = 0.01\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#use-scale_-functions-to-specify-axis-labels",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#use-scale_-functions-to-specify-axis-labels",
    "title": "Figure Design",
    "section": "Use scale_*() functions to specify axis labels",
    "text": "Use scale_*() functions to specify axis labels\n\n\nggplot(lincoln_temps) +\n  aes(x = mean_temp, y = month_long) +\n  geom_density_ridges(\n    scale = 3, bandwidth = 3.4,\n    rel_min_height = 0.01,\n  ) +\n  scale_x_continuous(\n    name = \"mean temperature (°F)\"\n  ) +\n  scale_y_discrete(\n    name = NULL  # NULL means no label\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#specify-scale-expansion",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#specify-scale-expansion",
    "title": "Figure Design",
    "section": "Specify scale expansion",
    "text": "Specify scale expansion\n\n\nggplot(lincoln_temps) +\n  aes(x = mean_temp, y = month_long) +\n  geom_density_ridges(\n    scale = 3, bandwidth = 3.4,\n    rel_min_height = 0.01\n  ) +\n  scale_x_continuous(\n    name = \"mean temperature (°F)\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(\n    name = NULL,\n    expand = expansion(add = c(0.2, 2.6))\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#set-overall-plot-theme",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#set-overall-plot-theme",
    "title": "Figure Design",
    "section": "Set overall plot theme",
    "text": "Set overall plot theme\n\n\nggplot(lincoln_temps) +\n  aes(x = mean_temp, y = month_long) +\n  geom_density_ridges(\n    scale = 3, bandwidth = 3.4,\n    rel_min_height = 0.01\n  ) +\n  scale_x_continuous(\n    name = \"mean temperature (°F)\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(\n    name = NULL,\n    expand = expansion(add = c(0.2, 2.6))\n  ) +\n  theme_minimal_grid()  # from cowplot"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#align-y-axis-labels-to-grid-lines",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#align-y-axis-labels-to-grid-lines",
    "title": "Figure Design",
    "section": "Align y axis labels to grid lines",
    "text": "Align y axis labels to grid lines\n\n\nggplot(lincoln_temps) +\n  aes(x = mean_temp, y = month_long) +\n  geom_density_ridges(\n    scale = 3, bandwidth = 3.4,\n    rel_min_height = 0.01\n  ) +\n  scale_x_continuous(\n    name = \"mean temperature (°F)\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(\n    name = NULL,\n    expand = expansion(add = c(0.2, 2.6))\n  ) +\n  theme_minimal_grid() +\n  theme(\n    axis.text.y = element_text(vjust = 0)\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#change-fill-color-from-default-gray-to-blue",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#change-fill-color-from-default-gray-to-blue",
    "title": "Figure Design",
    "section": "Change fill color from default gray to blue",
    "text": "Change fill color from default gray to blue\n\n\nggplot(lincoln_temps) +\n  aes(x = mean_temp, y = month_long) +\n  geom_density_ridges(\n    scale = 3, bandwidth = 3.4,\n    rel_min_height = 0.01,\n    fill = \"#7DCCFF\"\n  ) +\n  scale_x_continuous(\n    name = \"mean temperature (°F)\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(\n    name = NULL,\n    expand = expansion(add = c(0.2, 2.6))\n  ) +\n  theme_minimal_grid() +\n  theme(\n    axis.text.y = element_text(vjust = 0)\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#draw-lines-in-white-instead-of-black",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#draw-lines-in-white-instead-of-black",
    "title": "Figure Design",
    "section": "Draw lines in white instead of black",
    "text": "Draw lines in white instead of black\n\n\nggplot(lincoln_temps) +\n  aes(x = mean_temp, y = month_long) +\n  geom_density_ridges(\n    scale = 3, bandwidth = 3.4,\n    rel_min_height = 0.01,\n    fill = \"#7DCCFF\",\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"mean temperature (°F)\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(\n    name = NULL,\n    expand = expansion(add = c(0.2, 2.6))\n  ) +\n  theme_minimal_grid() +\n  theme(\n    axis.text.y = element_text(vjust = 0)\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes",
    "title": "Figure Design",
    "section": "Using ready-made themes",
    "text": "Using ready-made themes\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point()\n# default theme is theme_gray()"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-1",
    "title": "Figure Design",
    "section": "Using ready-made themes",
    "text": "Using ready-made themes\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point() + \n  theme_gray()"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-2",
    "title": "Figure Design",
    "section": "Using ready-made themes",
    "text": "Using ready-made themes\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point() + \n  theme_gray(14) # most themes take a font-size argument to scale text size"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-3",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-3",
    "title": "Figure Design",
    "section": "Using ready-made themes",
    "text": "Using ready-made themes\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point() + \n  theme_bw(14)"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-4",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-4",
    "title": "Figure Design",
    "section": "Using ready-made themes",
    "text": "Using ready-made themes\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point() + \n  theme_minimal(14)"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-5",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-5",
    "title": "Figure Design",
    "section": "Using ready-made themes",
    "text": "Using ready-made themes\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point() + \n  theme_classic(14)"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-6",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-6",
    "title": "Figure Design",
    "section": "Using ready-made themes",
    "text": "Using ready-made themes\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point() + \n  theme_half_open()  # from package cowplot"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-7",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-7",
    "title": "Figure Design",
    "section": "Using ready-made themes",
    "text": "Using ready-made themes\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point() + \n  theme_minimal_grid()  # from package cowplot"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-8",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-8",
    "title": "Figure Design",
    "section": "Using ready-made themes",
    "text": "Using ready-made themes\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point() + \n  theme_minimal_hgrid()  # from package cowplot"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-9",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-9",
    "title": "Figure Design",
    "section": "Using ready-made themes",
    "text": "Using ready-made themes\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point() + \n  theme_minimal_vgrid()  # from package cowplot"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-10",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-10",
    "title": "Figure Design",
    "section": "Using ready-made themes",
    "text": "Using ready-made themes\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point() + \n  theme_economist(14)       # from package ggthemes"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-11",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-11",
    "title": "Figure Design",
    "section": "Using ready-made themes",
    "text": "Using ready-made themes\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point() + \n  theme_economist(14) + scale_color_economist() # from package ggthemes"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-12",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#using-ready-made-themes-12",
    "title": "Figure Design",
    "section": "Using ready-made themes",
    "text": "Using ready-made themes\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g, color = species)) +\n  geom_point() + \n  theme_fivethirtyeight(14) + scale_color_fivethirtyeight() # from package ggthemes"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#customizing-theme-elements",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#customizing-theme-elements",
    "title": "Figure Design",
    "section": "Customizing theme elements",
    "text": "Customizing theme elements\n\n\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) +\n  geom_point(aes(color = species)) +\n  theme_minimal_grid()"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#customizing-theme-elements-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#customizing-theme-elements-1",
    "title": "Figure Design",
    "section": "Customizing theme elements",
    "text": "Customizing theme elements\n\n\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) +\n  geom_point(aes(color = species)) +\n  theme_minimal_grid() +\n  theme(\n    # change overall font family\n    # (requires font to be available)\n    text = element_text(\n      family = \"Comic Sans MS\"\n    )\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#customizing-theme-elements-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#customizing-theme-elements-2",
    "title": "Figure Design",
    "section": "Customizing theme elements",
    "text": "Customizing theme elements\n\n\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) +\n  geom_point(aes(color = species)) +\n  theme_minimal_grid() +\n  theme(\n    # change color of axis titles\n    axis.title = element_text(\n      color = \"royalblue2\"\n    )\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#customizing-theme-elements-3",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#customizing-theme-elements-3",
    "title": "Figure Design",
    "section": "Customizing theme elements",
    "text": "Customizing theme elements\n\n\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) +\n  geom_point(aes(color = species)) +\n  theme_minimal_grid() +\n  theme(\n    # change color of only the x axis title\n    axis.title.x = element_text(\n      color = \"royalblue2\"\n    )\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#customizing-theme-elements-4",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#customizing-theme-elements-4",
    "title": "Figure Design",
    "section": "Customizing theme elements",
    "text": "Customizing theme elements\n\n\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) +\n  geom_point(aes(color = species)) +\n  theme_minimal_grid() +\n  theme(\n    # change all text colors?\n    # why does it not work?\n    text = element_text(color = \"royalblue2\")\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#customizing-theme-elements-5",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#customizing-theme-elements-5",
    "title": "Figure Design",
    "section": "Customizing theme elements",
    "text": "Customizing theme elements\n\n\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) +\n  geom_point(aes(color = species)) +\n  theme_minimal_grid() +\n  theme(\n    text = element_text(color = \"royalblue2\"),\n    axis.text = element_text(\n      color = \"royalblue2\"\n    )\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#customizing-theme-elements-6",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#customizing-theme-elements-6",
    "title": "Figure Design",
    "section": "Customizing theme elements",
    "text": "Customizing theme elements\n\n\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) +\n  geom_point(aes(color = species)) +\n  theme_minimal_grid() +\n  theme(\n    text = element_text(color = \"royalblue2\"),\n    axis.text = element_text(\n      color = \"royalblue2\"\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe element axis.text has its own color set in the theme. Therefore it doesn’t inherit from text."
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#horizontal-and-vertical-alignment",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#horizontal-and-vertical-alignment",
    "title": "Figure Design",
    "section": "Horizontal and vertical alignment",
    "text": "Horizontal and vertical alignment\n\n\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) +\n  geom_point(aes(color = species)) +\n  theme_minimal_grid() +\n  theme(\n    axis.title.x = element_text(\n      # horizontal justification\n      # (0 = left)\n      hjust = 0\n    )\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#horizontal-and-vertical-alignment-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#horizontal-and-vertical-alignment-1",
    "title": "Figure Design",
    "section": "Horizontal and vertical alignment",
    "text": "Horizontal and vertical alignment\n\n\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) +\n  geom_point(aes(color = species)) +\n  theme_minimal_grid() +\n  theme(\n    axis.title.x = element_text(\n      # horizontal justification\n      # (0.5 = center)\n      hjust = 0.5\n    )\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#horizontal-and-vertical-alignment-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#horizontal-and-vertical-alignment-2",
    "title": "Figure Design",
    "section": "Horizontal and vertical alignment",
    "text": "Horizontal and vertical alignment\n\n\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) +\n  geom_point(aes(color = species)) +\n  theme_minimal_grid() +\n  theme(\n    axis.title.x = element_text(\n      # horizontal justification\n      # (1 = right)\n      hjust = 1\n    )\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#horizontal-and-vertical-alignment-3",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#horizontal-and-vertical-alignment-3",
    "title": "Figure Design",
    "section": "Horizontal and vertical alignment",
    "text": "Horizontal and vertical alignment\n\n\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) +\n  geom_point(aes(color = species)) +\n  theme_minimal_grid() +\n  theme(\n    axis.text.y = element_text(\n      # vertical justification\n      # (0 = bottom)\n      vjust = 0\n    )\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#horizontal-and-vertical-alignment-4",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#horizontal-and-vertical-alignment-4",
    "title": "Figure Design",
    "section": "Horizontal and vertical alignment",
    "text": "Horizontal and vertical alignment\n\n\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) +\n  geom_point(aes(color = species)) +\n  theme_minimal_grid() +\n  theme(\n    axis.text.y = element_text(\n      # vertical justification\n      # (0.5 = center)\n      vjust = 0.5\n    )\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#horizontal-and-vertical-alignment-5",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#horizontal-and-vertical-alignment-5",
    "title": "Figure Design",
    "section": "Horizontal and vertical alignment",
    "text": "Horizontal and vertical alignment\n\n\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) + \n  geom_point(aes(color = species)) +\n  theme_minimal_grid() +\n  theme(\n    axis.text.y = element_text(\n      # vertical justification\n      # (1 = top)\n      vjust = 1\n    )\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#remove-elements-entirely-element_blank",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#remove-elements-entirely-element_blank",
    "title": "Figure Design",
    "section": "Remove elements entirely: element_blank()",
    "text": "Remove elements entirely: element_blank()\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) +\n  geom_point(aes(color = species)) +\n  theme_minimal_grid() +\n  theme(\n    # all text gone\n    axis.text = element_blank()\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#remove-elements-entirely-element_blank-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#remove-elements-entirely-element_blank-1",
    "title": "Figure Design",
    "section": "Remove elements entirely: element_blank()",
    "text": "Remove elements entirely: element_blank()\n\n\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) +\n  geom_point(aes(color = species)) +\n  theme_minimal_grid() +\n  theme(\n    # no axis titles\n    axis.title = element_blank()\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#set-background-color-element_rect",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#set-background-color-element_rect",
    "title": "Figure Design",
    "section": "Set background color: element_rect()",
    "text": "Set background color: element_rect()\n\n\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) +\n  geom_point(aes(color = species)) +\n  theme_minimal_grid() +\n  theme(\n    plot.background = element_rect(\n      fill = \"aliceblue\"\n    )\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#set-background-color-element_rect-1",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#set-background-color-element_rect-1",
    "title": "Figure Design",
    "section": "Set background color: element_rect()",
    "text": "Set background color: element_rect()\n\n\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) +\n  geom_point(aes(color = species)) +\n  theme_minimal_grid() +\n  theme(\n    panel.background = element_rect(\n      fill = \"aliceblue\"\n    )\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#set-background-color-element_rect-2",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#set-background-color-element_rect-2",
    "title": "Figure Design",
    "section": "Set background color: element_rect()",
    "text": "Set background color: element_rect()\n\n\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) +\n  geom_point(aes(color = species)) +\n  theme_minimal_grid() +\n  theme(\n    legend.box.background = element_rect(\n      fill = \"aliceblue\",\n      color = \"steelblue4\" # line color\n    )\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#set-background-color-element_rect-3",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#set-background-color-element_rect-3",
    "title": "Figure Design",
    "section": "Set background color: element_rect()",
    "text": "Set background color: element_rect()\n\n\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) +\n  geom_point(aes(color = species)) +\n  theme_minimal_grid() +\n  theme(\n    legend.box.background = element_rect(\n      fill = \"aliceblue\",\n      color = \"steelblue4\" # line color\n    ),\n    legend.box.margin = margin(7, 7, 7, 7)\n  )"
  },
  {
    "objectID": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#move-the-legend-legend.position",
    "href": "materials/1-workshop1/7-custom-data-visualizations/figure_design_slides.html#move-the-legend-legend.position",
    "title": "Figure Design",
    "section": "Move the legend: legend.position",
    "text": "Move the legend: legend.position\n\n\nggplot(penguins) +\n  aes(flipper_length_mm, body_mass_g) +\n  geom_point(aes(color = species)) +\n  theme_minimal_grid() +\n  theme(\n    legend.box.background = element_rect(\n      fill = \"aliceblue\", \n      color = \"steelblue4\" # line color\n    ),\n    legend.box.margin = margin(7, 7, 7, 7),\n    # relative position inside plot panel\n    legend.position = c(1, 0),\n    # justification relative to position\n    legend.justification = c(1, 0)\n  )"
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/exercise_1.html#activity",
    "href": "materials/1-workshop1/8-group-projects/exercise_1.html#activity",
    "title": "Exploring Data for Patterns",
    "section": "Activity",
    "text": "Activity\n\nPatterns are the essence of data exploration and our eyes’ ability to pick them out is integral to data understanding. Much of the data we work with, however, do not have a natural form and we need to make decisions about how they are to be represented. Try different ways to visualize the datasets so meaningful patterns may be found.\n\n\nGenetic profiles of cancer\nThese datasets contains 10 cancer samples. Table 1 describes the mutational status for a set of genes (A-E) and whether a mutation if absent (0) or present (1). Table 2 summarizes the expression levels of those genes, ranging from no expression (0) to high expression (3).\n\n\n\nTable 1: Mutational status for a set of genes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample -&gt;\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nGene A\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nGene B\n0\n0\n0\n0\n1\n1\n1\n0\n1\n1\n\n\nGene C\n0\n0\n1\n0\n0\n0\n1\n1\n1\n1\n\n\nGene D\n1\n1\n0\n0\n1\n1\n0\n0\n0\n0\n\n\nGene E\n0\n1\n1\n0\n1\n0\n0\n0\n1\n0\n\n\n\n\n\n\n\n\n\nTable 2: Expression levels for a set of genes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample -&gt;\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nGene A\n2\n1\n1\n2\n2\n0\n2\n1\n1\n2\n\n\nGene B\n1\n1\n2\n1\n0\n0\n0\n2\n0\n0\n\n\nGene C\n1\n1\n3\n1\n2\n2\n3\n0\n3\n0\n\n\nGene D\n0\n0\n2\n1\n3\n3\n2\n1\n1\n1\n\n\nGene E\n1\n3\n3\n1\n3\n1\n2\n1\n3\n2\n\n\n\n\n\n\n\n\n          1. Think about the problem on your own for 5 minutes.\n          2. In your groups, discuss and create different visualizations to highlight underlying patterns\n          3. Summarize the group’s approach\n          4. Elect/volunteer a spokesperson to present the solution\n\n\nConsider the following concepts when creating your visualizations\n\n\n\n\nPatterns\nPatterns are the essence of data exploration. What kinds of representation will produce the most meaningful insights?\n   \n\n\nEncodings\nSome visual estimations are easier to make than others. How might you use encodings that are less accurate but otherwise better at conveying overall trends?\n  \n\n\n\n\nColor\nColor is a powerful encoding that presents several challenges. Have you chosen a color scale that is optimal for that data type?\n   \n\n\nSalience and Relevance\nPop-out effects enable quick recognition. Are the most noticeable elements of your visualizations also the most relevant?"
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/project_a_yogurt_example_notebook.html",
    "href": "materials/1-workshop1/8-group-projects/project_a_yogurt_example_notebook.html",
    "title": "Yogurt study data analysis",
    "section": "",
    "text": "This document presents an example of analysis of the Group 1 dataset."
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/project_a_yogurt_example_notebook.html#section",
    "href": "materials/1-workshop1/8-group-projects/project_a_yogurt_example_notebook.html#section",
    "title": "Yogurt study data analysis",
    "section": "",
    "text": "This document presents an example of analysis of the Group 1 dataset."
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/project_a_yogurt_example_notebook.html#study-description",
    "href": "materials/1-workshop1/8-group-projects/project_a_yogurt_example_notebook.html#study-description",
    "title": "Yogurt study data analysis",
    "section": "Study description",
    "text": "Study description\nThis is a randomized controlled trial to study whether yogurt consumption has an effect on the microbiome post antibiotic treatment. Absolute abundance of bacteria was measured by 3 qPCR assays (for total, L. crispatus, L. iners). Cytokine levels (in copies/ml of vaginal fluid) were also measured by Luminex. Data was collected at two timepoints, pre and post antibiotic treatment."
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/project_a_yogurt_example_notebook.html#checking-if-arms-are-balanced-in-terms-of-demographic-variables",
    "href": "materials/1-workshop1/8-group-projects/project_a_yogurt_example_notebook.html#checking-if-arms-are-balanced-in-terms-of-demographic-variables",
    "title": "Yogurt study data analysis",
    "section": "Checking if arms are balanced in terms of demographic variables",
    "text": "Checking if arms are balanced in terms of demographic variables\nWe first check if the randomization of participant didn’t lead to any unfortunate differences in terms of demographic/clinical variables between the two groups.\nTo do so, we load the demographic data table (`01_participant_metadata_yogurt.csv`), display the distribution of these variables per arm and create “Table 1”.\n\nDemographic data table\n\ndemographic_data &lt;- \n  read_csv(\"data/01_participant_metadata_yogurt.csv\")\n\nRows: 51 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): pid, arm, birth_control, education, sex\ndbl (2): days_since_last_sex, age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nFirst 6 rows of the table:\n\ndemographic_data |&gt; head() |&gt; pander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\npid\narm\ndays_since_last_sex\nbirth_control\n\n\n\n\npid_01\nunchanged_diet\n2\nno hormonal birth control\n\n\npid_02\nunchanged_diet\n4\nDepoprovera\n\n\npid_03\nunchanged_diet\n3\nno hormonal birth control\n\n\npid_04\nyogurt\n1\nDepoprovera\n\n\npid_05\nyogurt\n5\nDepoprovera\n\n\npid_06\nunchanged_diet\n9\nno hormonal birth control\n\n\n\n\n\n\n\n\n\n\n\nage\neducation\nsex\n\n\n\n\n28\ngrade 10-12, not matriculated\nfemale\n\n\n34\nless than grade 9\nfemale\n\n\n32\ngrade 10-12, matriculated\nfemale\n\n\n30\ngrade 10-12, not matriculated\nfemale\n\n\n34\nless than grade 9\nfemale\n\n\n27\ngrade 10-12, matriculated\nfemale\n\n\n\n\n\nNumber of participants per arm:\n\ndemographic_data |&gt; count(arm) |&gt; pander()\n\n\n\n\n\n\n\n\narm\nn\n\n\n\n\nunchanged_diet\n25\n\n\nyogurt\n26\n\n\n\n\n\n\n\nVisualization of the demographic variables of interest by study arm\nThere are 4 variables that we are interested in comparing: age, days_since_last_sex, birth_control, and education.\n\narm_colors &lt;-\n  c(\"unchanged_diet\" = \"darkseagreen3\", yogurt = \"slateblue\")\n\n\ng_age &lt;- \nggplot(demographic_data,\n       aes(x = age, fill = arm)) +\n  geom_histogram(binwidth = 1) + \n  facet_grid(arm ~ .) + \n  guides(fill = \"none\") +\n  scale_fill_manual(values = arm_colors)\n\n\ng_birth_control &lt;- \n  ggplot(demographic_data,\n       aes(x = birth_control, fill = arm)) +\n  geom_bar()  + \n  facet_grid(arm ~ .) + \n  guides(fill = \"none\") +\n  scale_fill_manual(values = arm_colors) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\ng_days_since_last_sex &lt;- \nggplot(demographic_data,\n       aes(x = days_since_last_sex, fill = arm)) +\n  geom_histogram(binwidth = 1)  + \n  facet_grid(arm ~ .) + \n  guides(fill = \"none\") +\n  scale_fill_manual(values = arm_colors)\n\n\ng_education &lt;- \n  ggplot(demographic_data,\n       aes(x = education, fill = arm)) +\n  geom_bar() +\n  facet_grid(arm ~ .) + \n  guides(fill = \"none\") +\n  scale_fill_manual(values = arm_colors) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\ng_age + g_days_since_last_sex + g_birth_control + g_education\n\n\n\n\n\n\n\n\n\n\nTable 1\nWe can now build “Table 1” using the tableone package and its function CreateTableOne.\n\ntable_one &lt;- \n  CreateTableOne(\n    data = demographic_data,\n    strata = \"arm\",\n    vars = c(\"age\", \"days_since_last_sex\",\"birth_control\", \"education\")\n  ) \n\ntable_one |&gt; print(printToggle = FALSE) |&gt; as.data.frame() |&gt; select(-test) |&gt; pander() \n\n\n\n\n\n\n\n\n\n\n \nunchanged_diet\nyogurt\np\n\n\n\n\nn\n25\n26\n\n\n\nage (mean (SD))\n30.44 (3.33)\n31.00 (3.29)\n0.548\n\n\ndays_since_last_sex (mean (SD))\n7.44 (6.70)\n9.50 (11.45)\n0.439\n\n\nbirth_control = no hormonal birth control (%)\n17 (68.0)\n9 (34.6)\n0.035\n\n\neducation (%)\n\n\n0.799\n\n\ngrade 10-12, matriculated\n6 (24.0)\n4 (15.4)\n\n\n\ngrade 10-12, not matriculated\n9 (36.0)\n12 (46.2)\n\n\n\nless than grade 9\n4 (16.0)\n5 (19.2)\n\n\n\npost-secondary\n6 (24.0)\n5 (19.2)\n\n\n\n\n\n\nNote: an alternative way, is to use the gtsummary package\n\nlibrary(gtsummary) # type install.packages(\"gtsummary\") in the console if you get an error \n\n#BlackLivesMatter\n\n\n\ndemographic_data |&gt; \n  select(-pid, -sex) |&gt; \n  tbl_summary(by = arm) |&gt; \n  add_p()\n\nWarning for variable 'days_since_last_sex':\nsimpleWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot compute exact p-value with ties\n\n\nWarning for variable 'age':\nsimpleWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot compute exact p-value with ties\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nunchanged_diet, N = 251\nyogurt, N = 261\np-value2\n\n\n\n\ndays_since_last_sex\n6 (3, 10)\n5 (3, 11)\n0.8\n\n\nbirth_control\n\n\n\n\n0.017\n\n\n    Depoprovera\n8 (32%)\n17 (65%)\n\n\n\n\n    no hormonal birth control\n17 (68%)\n9 (35%)\n\n\n\n\nage\n30.0 (28.0, 33.0)\n31.0 (29.0, 32.8)\n0.6\n\n\neducation\n\n\n\n\n0.8\n\n\n    grade 10-12, matriculated\n6 (24%)\n4 (15%)\n\n\n\n\n    grade 10-12, not matriculated\n9 (36%)\n12 (46%)\n\n\n\n\n    less than grade 9\n4 (16%)\n5 (19%)\n\n\n\n\n    post-secondary\n6 (24%)\n5 (19%)\n\n\n\n\n\n1 Median (IQR); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson’s Chi-squared test; Fisher’s exact test\n\n\n\n\n\n\n\n\n\nThe p-values are slightly different because, by default, the two packages use different tests. We note that in both cases, birth_control is flagged as unbalanced between the two groups. We may thus evaluate if this unbalance might explain any of the differences in the outcomes."
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/project_a_yogurt_example_notebook.html#research-aim-evaluating-the-influence-of-eating-yogurt-on-the-vaginal-microbiota-composition",
    "href": "materials/1-workshop1/8-group-projects/project_a_yogurt_example_notebook.html#research-aim-evaluating-the-influence-of-eating-yogurt-on-the-vaginal-microbiota-composition",
    "title": "Yogurt study data analysis",
    "section": "Research aim: Evaluating the influence of eating yogurt on the vaginal microbiota composition",
    "text": "Research aim: Evaluating the influence of eating yogurt on the vaginal microbiota composition\n\nData exploration\nThe microbiota composition data is in 02_qpcr_results_yogurt.csv file.\nLet’s open it to see how the microbiota composition has been characterized:\n\nqpcr &lt;- \n  read_csv(\n    \"data/02_qpcr_results_yogurt.csv\"\n  )\n\nRows: 102 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): sample_id\ndbl (3): qpcr_bacteria, qpcr_crispatus, qpcr_iners\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nqpcr |&gt; head() |&gt; pander()\n\n\n\n\n\n\n\n\n\n\nsample_id\nqpcr_bacteria\nqpcr_crispatus\nqpcr_iners\n\n\n\n\nUNCH092\n3093064\n0\n26.84\n\n\nUNCH042\n8275\n0\n76.04\n\n\nUNCH015\n7532202\n0\n555\n\n\nUNCH035\n4794320\n12.99\n204309\n\n\nUNCH019\n5328344\n0\n107806\n\n\nUNCH040\n5804\n0\n121045\n\n\n\n\n\nWe see that we have qPCR data for total bacterial load, Lactobacillus crispatus, and Lactobacillus iners.\nLet’s display the general distribution of these three quantities. To be able to display the data on the same graph, we can pivot_longer:\n\nqpcr_long &lt;- \n  qpcr |&gt; \n  pivot_longer(\n    cols = -sample_id,\n    names_to = \"variable\",\n    values_to = \"qpcr_value\" ,\n    names_pattern = \"qpcr_(.*)\"\n  )\n\nggplot(qpcr_long, aes(x = qpcr_value)) +\n  geom_histogram(bins = 30) +\n  facet_grid(variable ~ .)\n\n\n\n\n\n\n\n\nWe see that we may have to transform our data. We can try to see what a log transformation would do:\n\nggplot(qpcr_long, aes(x = qpcr_value)) +\n  geom_histogram(bins = 30) +\n  facet_grid(variable ~ .) +\n  scale_x_log10()\n\nWarning in scale_x_log10(): log-10 transformation introduced infinite values.\n\n\nWarning: Removed 74 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nNotice that we got a warning that the log transformation introduced infinite values. That’s because some values in L. crispatus and L. iners are 0 and log(0) = -\\(\\infty\\).\nThese 0s might be a problem for the downstream analyses. There are different ways to deal with these 0s. Here, we’ll use a simple little trick: we replace the 0s by a value that is half of the smaller value otherwise observed.\nThat value is:\n\nsmallest_non_zero &lt;- qpcr_long$qpcr_value |&gt; unique() |&gt; sort() |&gt; magrittr::extract(2)\nsmallest_non_zero\n\n[1] 0.994\n\n\n\nqpcr_long &lt;- \n  qpcr_long |&gt; \n  mutate(\n    qpcr_value_b = \n      case_when(\n        qpcr_value == 0 ~ smallest_non_zero/2,\n        TRUE ~ qpcr_value\n      ),\n    was_zero = (qpcr_value == 0)\n  )\n\nWe can now redo the histogram with this modified variable:\n\nggplot(qpcr_long, aes(x = qpcr_value_b, fill = was_zero)) +\n  geom_histogram(bins = 30) +\n  facet_grid(variable ~ ., scales = \"free_y\") +\n  scale_x_log10() +\n  scale_fill_manual(\"was zero\", values = c(\"gray40\", \"gray60\"), labels = c(\"no\",\"yes\"))\n\n\n\n\n\n\n\n\nThis gives us a better idea of the overall distributions of the variables describing the microbiota.\n\n\nResearch question\nWe can now define research questions associated with our research aim.\nFor example, we can ask the three following questions:\n\nDoes eating yogurt affect the total bacterial load?\nDoes eating yogurt affect the L. crispatus amounts?\nDoes eating yogurt affect the L. iners amounts?\n\nFor each of them, our Null hypothesis is that there are no differences between the arms and the Alternative is that there is a difference.\n\n\nOutcome variables at baseline\nBefore answering these questions, we may want to verify that the distributions of these three variables are similar at baseline.\nTo be able to check that we need to join by the sample information table (00_sample_ids_yogurt.csv) so we have the visit info\n\nsample_info &lt;- \n  read_csv(\"data/00_sample_ids_yogurt.csv\")\n\nRows: 102 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): pid, time_point, arm, sample_id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nsample_info |&gt; head() |&gt; pander()\n\n\n\n\n\n\n\n\n\n\npid\ntime_point\narm\nsample_id\n\n\n\n\npid_25\nbaseline\nunchanged_diet\nUNCH001\n\n\npid_51\nbaseline\nyogurt\nYOG002\n\n\npid_33\nbaseline\nyogurt\nYOG003\n\n\npid_13\nbaseline\nyogurt\nYOG004\n\n\npid_37\nafter_antibiotic\nunchanged_diet\nUNCH005\n\n\npid_09\nafter_antibiotic\nunchanged_diet\nUNCH006\n\n\n\n\n\nWe transform the categorical variables into factors, and, when relevant, we define the order of the categories (it is relevant for time_point for example).\n\nsample_info &lt;- \n  sample_info |&gt; \n  mutate(time_point = time_point |&gt; factor(levels = c(\"baseline\", \"after_antibiotic\")))\n\n\nqpcr_long_si &lt;-\n  qpcr_long |&gt; \n  left_join(sample_info)\n\nJoining with `by = join_by(sample_id)`\n\n\n\nqpcr_long_si |&gt; \n  filter(time_point == \"baseline\") |&gt; \n  ggplot(aes(x = arm, y = qpcr_value_b |&gt; log10(), fill = arm)) +\n  geom_boxplot(outlier.shape = NA, alpha = 0.75) + \n  # setting \"outlier.shape\" to NA does not show the outliers. \n  # we don't need to show them because with the geom_jitter below, we'll still see them.\n  geom_jitter(width = 0.2, height = 0, size = 0.75) +\n  facet_grid(. ~ variable) +\n  scale_fill_manual(values = arm_colors) +\n  ggtitle(\"Distribution of qPCR values at BASELINE by arm\") +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\nIt looks like almost no one had L. crispatus before intervention but that there might be a small difference in the L. iners amounts.\nLet’s see if that difference is statistically significant. We use a non-parametric test to do that.\n\nqpcr_long_si |&gt; \n  filter(time_point == \"baseline\") |&gt; \n  group_by(variable) |&gt; \n  summarize(\n    wtest = wilcox.test(log10(qpcr_value_b) ~ arm)$p.value\n  )\n\nWarning: There were 3 warnings in `summarize()`.\nThe first warning was:\nℹ In argument: `wtest = wilcox.test(log10(qpcr_value_b) ~ arm)$p.value`.\nℹ In group 1: `variable = \"bacteria\"`.\nCaused by warning in `wilcox.test.default()`:\n! cannot compute exact p-value with ties\nℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings.\n\n\n# A tibble: 3 × 2\n  variable  wtest\n  &lt;chr&gt;     &lt;dbl&gt;\n1 bacteria  0.486\n2 crispatus 0.978\n3 iners     0.119\n\n\nNone of these differences are statistically significant: we do not reject the Null hypothesis that the microbiota composition (i.e., our outcomes of interest) at baseline is similar between the two groups.\n(This is an equivalent of Table 1 but for the outcome of interest at baseline).\n\n\nResults\nWe can now answer our questions.\nLet’s first display our data to highlight the answers to our results.\nWe are interested in these changes at the \"after_antibiotic\" visit since that is after participants had both took the antibiotic and eaten the yogurt.\n\nqpcr_long_si |&gt; \n  filter(time_point == \"after_antibiotic\") |&gt; \n  ggplot(aes(x = arm, y = qpcr_value_b |&gt; log10(), fill = arm)) +\n  geom_boxplot(outlier.shape = NA, alpha = 0.75) + \n  geom_jitter(width = 0.2, height = 0, size = 0.75) +\n  facet_grid(. ~ variable) +\n  scale_fill_manual(values = arm_colors) +\n  guides(fill = \"none\") +\n  ggtitle(\"Distribution of qPCR values AFTER INTERVENTION by arm\")\n\n\n\n\n\n\n\n\nVisually, it looks like the yogurt might have had an effect on L. crispatus.\n!!! NOTE !!! If we had ignored the zeros and done the same visualization using the original data, we would have seen something quite different:\n\nqpcr_long_si |&gt; \n  filter(time_point == \"after_antibiotic\") |&gt; \n  ggplot(aes(x = arm, y = qpcr_value |&gt; log10(), fill = arm)) +\n  geom_boxplot(outlier.shape = NA, alpha = 0.75) +\n  # geom_jitter(width = 0.2, height = 0, size = 0.75) + \n  # (you'll see that geom_jitter will still display the -infinite values at the very bottom of the plot) \n  # but geom_boxplot is not able to show include them to the distribution.\n  facet_grid(. ~ variable) +\n  scale_fill_manual(values = arm_colors) +\n  guides(fill = \"none\") +\n  ggtitle(\"Distribution of qPCR values AFTER INTERVENTION by arm\\nzeros EXCLUDED from the plot\")\n\nWarning: Removed 17 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nLet’s now compute the median values in both arms and test our hypotheses:\n\nmedians &lt;- \nqpcr_long_si |&gt; \n  filter(time_point == \"after_antibiotic\") |&gt; \n  group_by(arm, variable) |&gt; \n  summarize(median_log = median(log10(qpcr_value_b)), .groups = \"drop\") |&gt; \n  pivot_wider(\n    id_cols = variable, names_from = arm, names_prefix = \"median_log_\",\n    values_from = median_log) \n\ntest_res &lt;- \nqpcr_long_si |&gt; \n  filter(time_point == \"after_antibiotic\") |&gt; \n  group_by(variable) |&gt; \n  summarize(\n    wtest_p = wilcox.test(log10(qpcr_value) ~ arm)$p.value\n  ) |&gt; \n  mutate(qval = p.adjust(wtest_p, method = \"BH\"))\n\nWarning: There were 3 warnings in `summarize()`.\nThe first warning was:\nℹ In argument: `wtest_p = wilcox.test(log10(qpcr_value) ~ arm)$p.value`.\nℹ In group 1: `variable = \"bacteria\"`.\nCaused by warning in `wilcox.test.default()`:\n! cannot compute exact p-value with ties\nℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings.\n\nmedians |&gt; left_join(test_res) |&gt; pander()\n\nJoining with `by = join_by(variable)`\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmedian_log_unchanged_diet\nmedian_log_yogurt\nwtest_p\nqval\n\n\n\n\nbacteria\n5.654\n5.163\n0.07192\n0.1136\n\n\ncrispatus\n-0.3036\n2.328\n0.07571\n0.1136\n\n\niners\n5.472\n5.502\n0.8876\n0.8876\n\n\n\n\n\nWe see that, although there is a large difference between the two medians in the two arms for L. crispatus, none of these p-values are smaller than 1/20. So we do not reject any of the null hypotheses.\nBut because of this large difference in medians for L. crispatus, it might be interesting to examine the before/after L. crispatus values in a high resolution (i.e., visualize them for each participants).\n\nqpcr_long_si |&gt; \n  filter(variable == \"crispatus\") |&gt; \n  arrange(time_point |&gt; fct_rev(), -qpcr_value) |&gt; \n  # the fct_rev() function allows to sort factor variables in reverse order than the order of the levels\n  # this is because I want to order participant IDs by the qpcr values at the post-intervention visit\n  mutate(pid = pid |&gt; factor(levels = unique(pid))) |&gt; \n  ggplot(\n    aes(x = time_point, y = pid, fill = log10(qpcr_value_b))\n  ) +\n  geom_tile() + \n  geom_text(aes(label = qpcr_value |&gt; round()), size = 3) + \n  # geom text allows to print text in the viz\n  facet_wrap(arm ~ ., scales = \"free\") +\n  scale_fill_gradient(low = \"white\", high = \"steelblue2\") +\n  guides(fill = \"none\") +\n  labs(title = \"L. crispatus qPCR values at both visits in both arms\")\n\n\n\n\n\n\n\n\nSo, we see, comparing the yogurt and the control group, that, in the yogurt arm, there are more people who ended up having at least some L. crispatus. Whether that’s enough to restore a healthy vaginal microbiota is something to discuss with microbiota experts so one could decide whether it would be worth designing a new, better powered, study to refine our understanding of the effects of eating yogurt."
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/project_a_yogurt_example_notebook.html#longitudinal-visualization",
    "href": "materials/1-workshop1/8-group-projects/project_a_yogurt_example_notebook.html#longitudinal-visualization",
    "title": "Yogurt study data analysis",
    "section": "Longitudinal visualization",
    "text": "Longitudinal visualization\nSo far, this analysis ignored the longitudinal nature of the data.\nLet’s fix that and display the baseline - post-antibiotic trajectories for these three variables in both arms.\n\nqpcr_long_si &lt;- \n  qpcr_long_si |&gt; \n  mutate(\n    time_point = time_point |&gt; fct_relevel(\"baseline\")\n  )\n\nqpcr_long_si  |&gt; \n  ggplot(aes(x = time_point, y = qpcr_value_b |&gt; log10(), col = arm)) +\n  geom_line(aes(group = pid), alpha = 0.5) +\n  geom_point() +\n  scale_color_manual(values = arm_colors) +\n  facet_grid(variable ~ ., scales = \"free\")\n\n\n\n\n\n\n\n\nWe see huge difference between the visits, but not so much between the arms. We might conclude that the antibiotic drove most of these effects."
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/project_a_yogurt_example_notebook.html#what-about-birth-control",
    "href": "materials/1-workshop1/8-group-projects/project_a_yogurt_example_notebook.html#what-about-birth-control",
    "title": "Yogurt study data analysis",
    "section": "What about birth control?",
    "text": "What about birth control?\nRemember that the two arms were not balanced with respect to birth control? (Results from Table 1)\nLet’s do a few visualization to check if the birth control of participant may be related to the outcomes.\n\nqpcr_long_si  |&gt; \n  left_join(demographic_data) |&gt; \n  ggplot(aes(x = time_point, y = qpcr_value_b |&gt; log10(), col = birth_control)) +\n  geom_line(aes(group = pid), alpha = 0.5) +\n  geom_point(alpha = 0.5) +\n  scale_color_manual(values = c(\"deeppink2\",\"green4\")) +\n  facet_grid(variable ~ arm, scales = \"free\")\n\nJoining with `by = join_by(pid, arm)`\n\n\n\n\n\n\n\n\n\n\nqpcr_long_si  |&gt; \n  left_join(demographic_data) |&gt; \n  ggplot(aes(x = arm, y = qpcr_value_b |&gt; log10(), fill = birth_control)) +\n  geom_boxplot(varwidth = TRUE) +\n  scale_fill_manual(values = c(\"deeppink2\",\"green4\")) +\n  facet_grid(variable ~ time_point, scales = \"free\")\n\nJoining with `by = join_by(pid, arm)`\n\n\n\n\n\n\n\n\n\nVisually, we don’t see consistent effects of the birth control on the outcomes."
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/project_a_yogurt_example_notebook.html#additional-analyses",
    "href": "materials/1-workshop1/8-group-projects/project_a_yogurt_example_notebook.html#additional-analyses",
    "title": "Yogurt study data analysis",
    "section": "Additional analyses",
    "text": "Additional analyses\nThere are many additional analyses that could be performed on this data. For example, one could continue this analysis by looking at the impact of the intervention on the cytokines (in the fourth table)."
  },
  {
    "objectID": "materials/1-workshop1/8-group-projects/slides.html#even-numbered-groups",
    "href": "materials/1-workshop1/8-group-projects/slides.html#even-numbered-groups",
    "title": "Welcome to the workshop",
    "section": "Even Numbered Groups:",
    "text": "Even Numbered Groups:\n\n\n\nback to module"
  },
  {
    "objectID": "materials/2-workshop2/0-welcome/index.html",
    "href": "materials/2-workshop2/0-welcome/index.html",
    "title": "Welcome to the workshop",
    "section": "",
    "text": "Get to know your neighbors and instructors and learn what to expect from this workshop",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 0:</b> Welcome to the workshop"
    ]
  },
  {
    "objectID": "materials/2-workshop2/0-welcome/index.html#slides",
    "href": "materials/2-workshop2/0-welcome/index.html#slides",
    "title": "Welcome to the workshop",
    "section": "Slides",
    "text": "Slides\nMake slides full screen",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 0:</b> Welcome to the workshop"
    ]
  },
  {
    "objectID": "materials/2-workshop2/0-welcome/index.html#before-the-workshop",
    "href": "materials/2-workshop2/0-welcome/index.html#before-the-workshop",
    "title": "Welcome to the workshop",
    "section": "Before the workshop",
    "text": "Before the workshop\nBefore the workshop, check the front page for instructions on joining the Discord and filling out the pre-workshop survey.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 0:</b> Welcome to the workshop"
    ]
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/images/ex-2.html",
    "href": "materials/2-workshop2/2-r-refresher/images/ex-2.html",
    "title": "Data Science for Biology Workshop Series",
    "section": "",
    "text": "Spatial Logical Toy Inventor: Erno Rubik Assignee: Politoys Ipari Szovetkezet, Budapest, Hungary The first patent by the inventor is registered in Hungary. Appl. No.: 289,192 Filed: Aug. 3, 1981 Background of the Invention The invention relates to a spatial logical toy having a total of eighteen toy elements which form a regular or an irregular spatial body, preferably an oblong body, in the assembled state. Spatial logical toys are well known, such as that described in the HU-PS No. 170 062 of the same applicant, which relates to a spatial logical toy consisting of twenty-seven solids which form a cube in the assembled state. The toy elements, in the shape of small cubes, may be turned along the spatial axes of the cube by means of connecting elements arranged in the geometric center of the large cube. The surfaces of the small cubes forming each surface of the large cube are colored or carry numbers, figures or any other symbols which can be assembled into the predetermined logical order of sequence by simultaneously rotating the nine toy elements forming the surfaces of the large cube. Summary of the Invention The logical toy according to the present invention represents an improved form of the previously described spatial logical toy. The construction is based on the same principles, however, the internal connection is performed by means of absolutely new and particular solids. The key feature according to the invention, i.e. shape, name, sounds, mode of interconnection and central fixture will be described in detail by means of two preferred embodiments, by the aid of the accompanying drawings, wherein. Component blocks of the spatial logial toy. ex-2-figs.png What is claimed is: 1. In a spatial logical toy assembled from a plurality of toy elements, of which a predetermined number may be rotated in the direction of the spatial axes starting from the geometrical center of the logical toy, the improvement wherein the spatial logical toy is formed by a total of eighteen toy elements. Two sets of eight toy elements each comprise substantially cubiforms with integally formed cam elements and each of the sets comprise eight identical toy elements, and two connecting toy elements, and means for joining the connecting toy elements to coact with the cam elements to form an integrated toy body, the joining means comprising a single screw enclosed by a spring. The spatial logical toy as claimed in claim 1, wherein the toy has the shape of a regular geometrical body in the assembled state. The toy elements thereof belonging to one set comprise eight cubiform homologous elements each having a first cam element connected to one corner thereof. Two confining surfaces of which lie at a unit distance from two surfaces of the cube and are parallel therewith and are cut-off in the form of an ellipsis-quarter, and a third confining surface thereof is coplanar with another surface of the cube and between the two confining faces of the first cam element running parallel with the cube and the cube there is a hollow with a convex spherical surface."
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#section",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#section",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "",
    "text": "Workshop materials are at:\nhttps://elsherbini.github.io/durban-data-science-for-biology/"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#section-1",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#section-1",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "",
    "text": "Goals for this session\n\n\nRStudio and the Quarto notebook\nData types and manipulations\nReview data visualization\nData wrangling and make plots with the tidyverse\nAdvanced table commands\nPlotting distributions with tidyverse and ggplot"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#discussions-discord",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#discussions-discord",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Discussions: discord",
    "text": "Discussions: discord\nAsk questions at #workshop-questions on https://discord.com/channels/1227264067899621427."
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#stickies",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#stickies",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Stickies",
    "text": "Stickies\n\n\n\n\n\n\nDuring an activity, place a yellow sticky on your laptop if you’re good to go and a pink sticky if you want help.\n\n\n\n\nImage by Megan Duffy"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#practicalities",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#practicalities",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Practicalities",
    "text": "Practicalities\n\nWiFi:\nNetwork: Southernsunconference\nNetwork Password: S0uthernsun1"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#getting-on-aws-rstudio",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#getting-on-aws-rstudio",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Getting on AWS & RStudio",
    "text": "Getting on AWS & RStudio\nTo get on your instance, find the address associated with your name. Copy the link into your browser and it will take you to navigate to RStudio\nIP addresses\n\n\nEnter the login credentials\nUsername: genomics\nPassword: evomics2024"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#create-an-r-project",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#create-an-r-project",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Create an R Project",
    "text": "Create an R Project\n\n\n\nFile -&gt; New Project…"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#create-an-r-project-1",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#create-an-r-project-1",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Create an R Project",
    "text": "Create an R Project\n\n\n\nClick on New Directory"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#create-an-r-project-2",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#create-an-r-project-2",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Create an R Project",
    "text": "Create an R Project\n\n\n\nNavigate to the workshop_2 folder name your directory and click “Create Project”"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#create-an-r-project-3",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#create-an-r-project-3",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Create an R Project",
    "text": "Create an R Project\n\n\n\nYou made a project! This creates a file for you with the .qmd extension"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#create-an-r-project-4",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#create-an-r-project-4",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Create an R Project",
    "text": "Create an R Project\n\n\n\nSwitch from “visual” to “source” to see the plain-text version of this document."
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#create-an-r-project-5",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#create-an-r-project-5",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Create an R Project",
    "text": "Create an R Project\n\n\n\nClick on “Render” to ask Quarto to turn this plain-text document into an HTML page"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#create-an-r-project-6",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#create-an-r-project-6",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Create an R Project",
    "text": "Create an R Project\n\n\n\nYour default web-browser will open and show you the rendered document!"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#what-are-the-parts-of-rstudio",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#what-are-the-parts-of-rstudio",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "What are the parts of RStudio?",
    "text": "What are the parts of RStudio?"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#the-text-editor",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#the-text-editor",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "The text editor",
    "text": "The text editor"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#the-console",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#the-console",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "The console",
    "text": "The console"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#the-right-panes",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#the-right-panes",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "The right panes",
    "text": "The right panes"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#installing-and-loading-packages",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#installing-and-loading-packages",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Installing and loading packages",
    "text": "Installing and loading packages\n\nPackages are a collection of functions and objects that are shared for free to use.\nIn the console, you can type install.packages(\"package_name\") to install most R packages.\nSometimes R packages need to be installed a different way, and the documentation of the package will tell you how.\nThen, to load a package, add library(\"package_name\") in a code chunk (usually in the first code cell of your document)"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#quartos-code-chunk",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#quartos-code-chunk",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Quarto’s Code Chunk",
    "text": "Quarto’s Code Chunk\n\n\n\n\n```{r}\n#| echo: false\nrnorm(3)\n```\nWrite a math expression in a chunk and press the green arrow at the top-right of the chunk."
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#example-figures-from-code",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#example-figures-from-code",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Example: Figures from Code",
    "text": "Example: Figures from Code\n\n\n```{r}\nlibrary(palmerpenguins)\nlibrary(ggplot2)\n\nggplot(penguins,\n       aes(x = bill_length_mm,\n           y = bill_depth_mm,\n           col = island)) +\n  geom_point()\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#example-figures-from-code-1",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#example-figures-from-code-1",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Example: Figures from Code",
    "text": "Example: Figures from Code\n\n\n```{r}\n#| fig-width: 5\n#| fig-height: 3\n\nlibrary(palmerpenguins)\nlibrary(ggplot2)\n\nggplot(penguins,\n       aes(x = bill_length_mm,\n           y = bill_depth_mm,\n           col = island)) +\n  geom_point()\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#inline-elements-text-formatting",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#inline-elements-text-formatting",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Inline Elements: Text Formatting",
    "text": "Inline Elements: Text Formatting\n\n\nMarkdown\nMarkdown allows you to format text\nwith *emphasis* and **strong emphasis**.\nYou can also add superscripts^2^, \nsubscripts~2~, and display code \n`verbatim`. Little known fact: you can \nalso ~~strikethrough~~ text and present\nit in [small caps]{.smallcaps}.\n\n\nOutput\nMarkdown allows you to format text with emphasis and strong emphasis. You can also add superscripts2, subscripts2, and display code verbatim. Little known fact: you can also strikethrough text and present it in small caps."
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#metadata-yaml",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#metadata-yaml",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Metadata: YAML",
    "text": "Metadata: YAML\n“Yet Another Markup Language” or “YAML Ain’t Markup Language” is used to provide document level metadata …\n\n\n\n\n[… in key-value pairs,]\n[… that can nest,]\n[… are fussy about indentation,]\n[… and are kept between ---.]\n\n\n\n---\nformat: \n  title: \"Intro to R\"\n  author: \"Yours Truly\"\n  html:\n    toc: true\n    code-fold: true\n---\n\n\nThere are many options for front matter and configuring rendering."
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#writing-code",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#writing-code",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Writing code",
    "text": "Writing code"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#assignment",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#assignment",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Assignment",
    "text": "Assignment\nYou can use &lt;- or = to assign values to variables\na &lt;- 6\nb = 8\nc &lt;- 5.44\nd = TRUE\ne = \"hello world\" \ne &lt;- 'hello world' # same as double quote\nWe will use &lt;- for all examples going forward."
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#naming-variables",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#naming-variables",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Naming variables",
    "text": "Naming variables\nA lot of R people use . inside variable names, but in most languages besides R this would be an error. It’s good practice these days to use the _ underscore if you want separation in your variable names.\nr.people.sometimes.put.dots &lt;- TRUE\ndots.are.confusing &lt;- \"maybe\"\njust_use_underscores &lt;- \"please\""
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#functions",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#functions",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Functions",
    "text": "Functions\nFunctions are named bits of code that take parameters as input and return some output\n\nlibrary(tidyverse)\nword_1 &lt;- \"hello\"\nword_2 &lt;- \"world\"\nstr_c(word_1, word_2, sep = \" \")\n\n[1] \"hello world\"\n\n\nstr_c is a function that puts concatenates strings.\nfunctions can have named parameters as well as positional parameters.\nnamed parameters always take an = sign for assignment."
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#getting-help-with-functions",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#getting-help-with-functions",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Getting help with functions",
    "text": "Getting help with functions\nType ?str_c in the console to get a help page. check out this guide on how to read the R help pages.\nalso try googling str_c R tidyverse to get help.\nchatGPT and phind.com are really good at answering specific questions about R functions - not always correct but most of the time."
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#value-types-in-r",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#value-types-in-r",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Value types in R",
    "text": "Value types in R\nThe type of the value can be\n# numeric\nc(1,2,3,4) \n\n# character\nc(\"a\",\"b\",\"c\",\"d\")\n\n# boolean\nc(TRUE, FALSE)\n\n# factor\nc(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\") %&gt;% as_factor()"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#tibbles-aka-data-frames",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#tibbles-aka-data-frames",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "tibbles (aka data frames)",
    "text": "tibbles (aka data frames)\ntibbles are the big reason R is great for working with tabular data.\nA data frame is a rectangular collection of variables (in the columns) and observations (in the rows).\n\ntable_02\n\n# A tibble: 132 × 6\n   pid    time_point arm     nugent_score crp_blood    ph\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 pid_01 baseline   placebo            8      0.44   5.7\n 2 pid_01 week_1     placebo            7      1.66   5.2\n 3 pid_01 week_7     placebo            7      1.44   5.4\n 4 pid_02 baseline   placebo            7      1.55   5.2\n 5 pid_02 week_1     placebo            7      0.75   4.8\n 6 pid_02 week_7     placebo            4      1.17   4.2\n 7 pid_03 baseline   placebo            6      1.78   4.8\n 8 pid_03 week_1     placebo           10      0.57   5.3\n 9 pid_03 week_7     placebo            7      1.79   5.2\n10 pid_04 baseline   placebo            5      1.76   4.8\n# ℹ 122 more rows"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#read-in-data",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#read-in-data",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Read in data",
    "text": "Read in data\nData is often in tables, and the easiest way to store tabular data is in csv or tsv format.\ncsv - comma separated values\ntsv - tab separated values\nto read in data stored this way use read_csv(filename) or read_tsv(filename)\ntable_02 &lt;- read_csv(\"02_visit_clinical_measurements_UKZN_workshop_2023.csv\")"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#the-pipe-feeds-data-into-functions",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#the-pipe-feeds-data-into-functions",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "The pipe %>% feeds data into functions",
    "text": "The pipe %&gt;% feeds data into functions\n\n```{r}\nhead(table_02)\n```\n\n# A tibble: 6 × 6\n  pid    time_point arm     nugent_score crp_blood    ph\n  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 pid_01 baseline   placebo            8      0.44   5.7\n2 pid_01 week_1     placebo            7      1.66   5.2\n3 pid_01 week_7     placebo            7      1.44   5.4\n4 pid_02 baseline   placebo            7      1.55   5.2\n5 pid_02 week_1     placebo            7      0.75   4.8\n6 pid_02 week_7     placebo            4      1.17   4.2"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#the-pipe-feeds-data-into-functions-1",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#the-pipe-feeds-data-into-functions-1",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "The pipe %>% feeds data into functions",
    "text": "The pipe %&gt;% feeds data into functions\n\n```{r}\n# head(table_02)\ntable_02 %&gt;%\n  head()\n```\n\n# A tibble: 6 × 6\n  pid    time_point arm     nugent_score crp_blood    ph\n  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 pid_01 baseline   placebo            8      0.44   5.7\n2 pid_01 week_1     placebo            7      1.66   5.2\n3 pid_01 week_7     placebo            7      1.44   5.4\n4 pid_02 baseline   placebo            7      1.55   5.2\n5 pid_02 week_1     placebo            7      0.75   4.8\n6 pid_02 week_7     placebo            4      1.17   4.2"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#the-pipe-feeds-data-into-functions-2",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#the-pipe-feeds-data-into-functions-2",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "The pipe %>% feeds data into functions",
    "text": "The pipe %&gt;% feeds data into functions\n\n```{r}\nggplot(table_02, aes(crp_blood, ph, color = arm)) + geom_point()\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#the-pipe-feeds-data-into-functions-3",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#the-pipe-feeds-data-into-functions-3",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "The pipe %>% feeds data into functions",
    "text": "The pipe %&gt;% feeds data into functions\n\n```{r}\ntable_02 %&gt;%\n  ggplot(aes(crp_blood, ph, color = arm)) + geom_point()\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#pick-rows-from-a-table-filter",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#pick-rows-from-a-table-filter",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Pick rows from a table: filter()",
    "text": "Pick rows from a table: filter()"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#filter-only-placebo",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#filter-only-placebo",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Filter only placebo",
    "text": "Filter only placebo\n\n```{r}\ntable_02 %&gt;%\n  filter(arm == \"placebo\")\n```\n\n# A tibble: 69 × 6\n   pid    time_point arm     nugent_score crp_blood    ph\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 pid_01 baseline   placebo            8      0.44   5.7\n 2 pid_01 week_1     placebo            7      1.66   5.2\n 3 pid_01 week_7     placebo            7      1.44   5.4\n 4 pid_02 baseline   placebo            7      1.55   5.2\n 5 pid_02 week_1     placebo            7      0.75   4.8\n 6 pid_02 week_7     placebo            4      1.17   4.2\n 7 pid_03 baseline   placebo            6      1.78   4.8\n 8 pid_03 week_1     placebo           10      0.57   5.3\n 9 pid_03 week_7     placebo            7      1.79   5.2\n10 pid_04 baseline   placebo            5      1.76   4.8\n# ℹ 59 more rows"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#filter-out-samples-with-ph-4",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#filter-out-samples-with-ph-4",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Filter out samples with ph < 4",
    "text": "Filter out samples with ph &lt; 4\n\n```{r}\ntable_02 %&gt;%\n  filter(ph &lt; 4)\n```\n\n# A tibble: 39 × 6\n   pid    time_point arm       nugent_score crp_blood    ph\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 pid_05 week_1     treatment            3      0.19   3.2\n 2 pid_05 week_7     treatment            2      0.45   3.5\n 3 pid_09 week_1     treatment            3      0.27   3.6\n 4 pid_10 week_1     treatment            0      0.01   3.5\n 5 pid_10 week_7     treatment            1      2.87   2.9\n 6 pid_11 week_1     treatment            1      0.1    3.3\n 7 pid_15 week_1     treatment            3      0.84   3.4\n 8 pid_15 week_7     treatment            3      0.68   3.5\n 9 pid_16 week_1     treatment            0      0.03   3.7\n10 pid_16 week_7     treatment            2      0.5    3.2\n# ℹ 29 more rows"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#pick-columns-from-a-table-select",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#pick-columns-from-a-table-select",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Pick columns from a table: select()",
    "text": "Pick columns from a table: select()"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#pick-columns-pid-ph-and-nugent",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#pick-columns-pid-ph-and-nugent",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Pick columns pid, ph, and nugent",
    "text": "Pick columns pid, ph, and nugent\n\n```{r}\ntable_02 %&gt;%\n  select(pid, ph, nugent_score)\n```\n\n# A tibble: 132 × 3\n   pid       ph nugent_score\n   &lt;chr&gt;  &lt;dbl&gt;        &lt;dbl&gt;\n 1 pid_01   5.7            8\n 2 pid_01   5.2            7\n 3 pid_01   5.4            7\n 4 pid_02   5.2            7\n 5 pid_02   4.8            7\n 6 pid_02   4.2            4\n 7 pid_03   4.8            6\n 8 pid_03   5.3           10\n 9 pid_03   5.2            7\n10 pid_04   4.8            5\n# ℹ 122 more rows"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#rename-columns-and-subset-with-select",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#rename-columns-and-subset-with-select",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Rename columns and subset with select",
    "text": "Rename columns and subset with select\n\n```{r}\ntable_02 %&gt;%\n  select(participant_id = pid, ph, nugent_score)\n```\n\n# A tibble: 132 × 3\n   participant_id    ph nugent_score\n   &lt;chr&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n 1 pid_01           5.7            8\n 2 pid_01           5.2            7\n 3 pid_01           5.4            7\n 4 pid_02           5.2            7\n 5 pid_02           4.8            7\n 6 pid_02           4.2            4\n 7 pid_03           4.8            6\n 8 pid_03           5.3           10\n 9 pid_03           5.2            7\n10 pid_04           4.8            5\n# ℹ 122 more rows"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#sort-the-rows-in-a-table-arrange",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#sort-the-rows-in-a-table-arrange",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Sort the rows in a table: arrange()",
    "text": "Sort the rows in a table: arrange()"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#sort-samples-by-ph-ascending",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#sort-samples-by-ph-ascending",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Sort samples by ph ascending",
    "text": "Sort samples by ph ascending\n\n```{r}\ntable_02 %&gt;%\n  arrange(ph)\n```\n\n# A tibble: 132 × 6\n   pid    time_point arm       nugent_score crp_blood    ph\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 pid_31 week_7     treatment            2      1.36   2.8\n 2 pid_10 week_7     treatment            1      2.87   2.9\n 3 pid_28 baseline   treatment            3      0.67   2.9\n 4 pid_26 week_1     treatment            0      0.11   3  \n 5 pid_23 week_7     placebo              3      3.67   3.1\n 6 pid_40 baseline   treatment            3      1.48   3.1\n 7 pid_40 week_1     treatment            2      0.17   3.1\n 8 pid_05 week_1     treatment            3      0.19   3.2\n 9 pid_16 week_7     treatment            2      0.5    3.2\n10 pid_37 week_7     treatment            2      0.7    3.2\n# ℹ 122 more rows"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#sort-samples-by-ph-descending",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#sort-samples-by-ph-descending",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Sort samples by ph, descending",
    "text": "Sort samples by ph, descending\n\n```{r}\ntable_02 %&gt;%\n  arrange(desc(ph))\n```\n\n# A tibble: 132 × 6\n   pid    time_point arm       nugent_score crp_blood    ph\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 pid_29 baseline   placebo              7      2.39   5.8\n 2 pid_01 baseline   placebo              8      0.44   5.7\n 3 pid_16 baseline   treatment            6      1.91   5.7\n 4 pid_06 week_1     placebo              8      1.72   5.6\n 5 pid_26 baseline   treatment            7      0.94   5.6\n 6 pid_13 week_1     placebo              7      2.57   5.5\n 7 pid_23 week_1     placebo              8      0.8    5.5\n 8 pid_27 baseline   placebo              7      1.17   5.5\n 9 pid_01 week_7     placebo              7      1.44   5.4\n10 pid_04 week_7     placebo              7      5.68   5.4\n# ℹ 122 more rows"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#counting-things",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#counting-things",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Counting things",
    "text": "Counting things\nTo demonstrate counting, let’s switch to table_01\n\n```{r}\ntable_01\n```\n\n# A tibble: 44 × 6\n   pid    arm       smoker       age education                     sex  \n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                         &lt;lgl&gt;\n 1 pid_01 placebo   non-smoker    26 grade 10-12, matriculated     FALSE\n 2 pid_02 placebo   smoker        33 grade 10-12, matriculated     FALSE\n 3 pid_03 placebo   smoker        30 post-secondary                FALSE\n 4 pid_04 placebo   non-smoker    34 grade 10-12, not matriculated FALSE\n 5 pid_05 treatment non-smoker    29 grade 10-12, matriculated     FALSE\n 6 pid_06 placebo   smoker        34 post-secondary                FALSE\n 7 pid_07 placebo   non-smoker    31 grade 10-12, not matriculated FALSE\n 8 pid_08 placebo   smoker        30 grade 10-12, not matriculated FALSE\n 9 pid_09 treatment non-smoker    35 grade 10-12, not matriculated FALSE\n10 pid_10 treatment non-smoker    32 less than grade 9             FALSE\n# ℹ 34 more rows"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#counting-things-1",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#counting-things-1",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Counting things",
    "text": "Counting things\n\n```{r}\ntable_01 %&gt;%\n  count(smoker)\n```\n\n# A tibble: 2 × 2\n  smoker         n\n  &lt;chr&gt;      &lt;int&gt;\n1 non-smoker    27\n2 smoker        17"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#counting-things-2",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#counting-things-2",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Counting things",
    "text": "Counting things\n\n```{r}\ntable_01 %&gt;%\n  count(arm, smoker)\n```\n\n# A tibble: 4 × 3\n  arm       smoker         n\n  &lt;chr&gt;     &lt;chr&gt;      &lt;int&gt;\n1 placebo   non-smoker    12\n2 placebo   smoker        11\n3 treatment non-smoker    15\n4 treatment smoker         6"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#lets-take-a-poll",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#lets-take-a-poll",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Let’s take a poll",
    "text": "Let’s take a poll\nGo to the event on wooclap\n\nM2. Does filter get rid of rows that match TRUE, or keep rows that match TRUE?"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#use-the-pipe-to-build-analysis-pipelines",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#use-the-pipe-to-build-analysis-pipelines",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Use the pipe to build analysis pipelines",
    "text": "Use the pipe to build analysis pipelines\n\n```{r}\ntable_01 %&gt;%\n  filter(arm == \"placebo\")\n```\n\n# A tibble: 23 × 6\n   pid    arm     smoker       age education                     sex  \n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                         &lt;lgl&gt;\n 1 pid_01 placebo non-smoker    26 grade 10-12, matriculated     FALSE\n 2 pid_02 placebo smoker        33 grade 10-12, matriculated     FALSE\n 3 pid_03 placebo smoker        30 post-secondary                FALSE\n 4 pid_04 placebo non-smoker    34 grade 10-12, not matriculated FALSE\n 5 pid_06 placebo smoker        34 post-secondary                FALSE\n 6 pid_07 placebo non-smoker    31 grade 10-12, not matriculated FALSE\n 7 pid_08 placebo smoker        30 grade 10-12, not matriculated FALSE\n 8 pid_12 placebo non-smoker    31 grade 10-12, matriculated     FALSE\n 9 pid_13 placebo non-smoker    32 post-secondary                FALSE\n10 pid_14 placebo smoker        32 grade 10-12, matriculated     FALSE\n# ℹ 13 more rows"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#use-the-pipe-to-build-analysis-pipelines-1",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#use-the-pipe-to-build-analysis-pipelines-1",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Use the pipe to build analysis pipelines",
    "text": "Use the pipe to build analysis pipelines\n\n```{r}\ntable_01 %&gt;%\n  filter(age &lt; 30) %&gt;%\n  select(pid, arm, smoker)\n```\n\n# A tibble: 12 × 3\n   pid    arm       smoker    \n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;     \n 1 pid_01 placebo   non-smoker\n 2 pid_05 treatment non-smoker\n 3 pid_15 treatment non-smoker\n 4 pid_17 treatment non-smoker\n 5 pid_21 treatment non-smoker\n 6 pid_27 placebo   smoker    \n 7 pid_30 placebo   non-smoker\n 8 pid_31 treatment non-smoker\n 9 pid_35 placebo   non-smoker\n10 pid_36 placebo   non-smoker\n11 pid_41 treatment smoker    \n12 pid_44 treatment non-smoker"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#use-the-pipe-to-build-analysis-pipelines-2",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#use-the-pipe-to-build-analysis-pipelines-2",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Use the pipe to build analysis pipelines",
    "text": "Use the pipe to build analysis pipelines\n\n```{r}\ntable_01 %&gt;%\n  filter(age &lt; 30) %&gt;%\n  select(pid, arm, smoker) %&gt;%\n  count(arm, smoker)\n```\n\n# A tibble: 4 × 3\n  arm       smoker         n\n  &lt;chr&gt;     &lt;chr&gt;      &lt;int&gt;\n1 placebo   non-smoker     4\n2 placebo   smoker         1\n3 treatment non-smoker     6\n4 treatment smoker         1"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#adding-new-columns-to-a-table",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#adding-new-columns-to-a-table",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Adding new columns to a table",
    "text": "Adding new columns to a table"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#make-a-new-table-column-mutate",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#make-a-new-table-column-mutate",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Make a new table column: mutate()",
    "text": "Make a new table column: mutate()"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#example-c-reactive-protein",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#example-c-reactive-protein",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Example: C-reactive protein",
    "text": "Example: C-reactive protein\nThe crp_blood column is in units of mg/L. What if you needed it in ug/ul? What’s the calculation?\n\n```{r}\ntable_02 %&gt;%\n  select(pid, time_point, crp_blood)\n```\n\n# A tibble: 132 × 3\n   pid    time_point crp_blood\n   &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;\n 1 pid_01 baseline        0.44\n 2 pid_01 week_1          1.66\n 3 pid_01 week_7          1.44\n 4 pid_02 baseline        1.55\n 5 pid_02 week_1          0.75\n 6 pid_02 week_7          1.17\n 7 pid_03 baseline        1.78\n 8 pid_03 week_1          0.57\n 9 pid_03 week_7          1.79\n10 pid_04 baseline        1.76\n# ℹ 122 more rows"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#example-c-reactive-protein-1",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#example-c-reactive-protein-1",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Example: C-reactive protein",
    "text": "Example: C-reactive protein\nThe crp_blood column is in units of mg/L. What if you needed it in ug/ul? What’s the calculation?\nTo get ug/L you would multiply by 1000. To get ug/ul you need to then divide by 1000000\n\n```{r}\ntable_02 %&gt;%\n  select(pid, time_point, crp_blood)\n```\n\n# A tibble: 132 × 3\n   pid    time_point crp_blood\n   &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;\n 1 pid_01 baseline        0.44\n 2 pid_01 week_1          1.66\n 3 pid_01 week_7          1.44\n 4 pid_02 baseline        1.55\n 5 pid_02 week_1          0.75\n 6 pid_02 week_7          1.17\n 7 pid_03 baseline        1.78\n 8 pid_03 week_1          0.57\n 9 pid_03 week_7          1.79\n10 pid_04 baseline        1.76\n# ℹ 122 more rows"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#example-c-reactive-protein-2",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#example-c-reactive-protein-2",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Example: C-reactive protein",
    "text": "Example: C-reactive protein\nThe crp_blood column is in units of mg/L. What if you needed it in ug/ul? What’s the calculation?\nTo get ug/L you would multiply by 1000. To get ug/ul you need to then divide by 1000000\n\n```{r}\ntable_02 %&gt;%\n  select(pid, time_point, crp_blood) %&gt;%\n  mutate(crp_blood_ugul = crp_blood / 1000)\n```\n\n# A tibble: 132 × 4\n   pid    time_point crp_blood crp_blood_ugul\n   &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n 1 pid_01 baseline        0.44        0.00044\n 2 pid_01 week_1          1.66        0.00166\n 3 pid_01 week_7          1.44        0.00144\n 4 pid_02 baseline        1.55        0.00155\n 5 pid_02 week_1          0.75        0.00075\n 6 pid_02 week_7          1.17        0.00117\n 7 pid_03 baseline        1.78        0.00178\n 8 pid_03 week_1          0.57        0.00057\n 9 pid_03 week_7          1.79        0.00179\n10 pid_04 baseline        1.76        0.00176\n# ℹ 122 more rows"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#make-multiple-columns-at-once",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#make-multiple-columns-at-once",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Make multiple columns at once",
    "text": "Make multiple columns at once\n\n```{r}\ntable_02 %&gt;%\n  select(pid, time_point, crp_blood) %&gt;%\n  mutate(crp_blood_ugul = crp_blood / 1000,\n         crp_blood_ugl = crp_blood * 1000)\n```\n\n# A tibble: 132 × 5\n   pid    time_point crp_blood crp_blood_ugul crp_blood_ugl\n   &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n 1 pid_01 baseline        0.44        0.00044           440\n 2 pid_01 week_1          1.66        0.00166          1660\n 3 pid_01 week_7          1.44        0.00144          1440\n 4 pid_02 baseline        1.55        0.00155          1550\n 5 pid_02 week_1          0.75        0.00075           750\n 6 pid_02 week_7          1.17        0.00117          1170\n 7 pid_03 baseline        1.78        0.00178          1780\n 8 pid_03 week_1          0.57        0.00057           570\n 9 pid_03 week_7          1.79        0.00179          1790\n10 pid_04 baseline        1.76        0.00176          1760\n# ℹ 122 more rows"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#plots-map-data-onto-graphical-elements.",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#plots-map-data-onto-graphical-elements.",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Plots map data onto graphical elements.",
    "text": "Plots map data onto graphical elements.\n\n\n\n\nTable 1: 02_visit_clinical_measurements_UKZN_workshop_2023.csv\n\n\n\n\n\n\npid\ntime_point\narm\nnugent_score\ncrp_blood\nph\n\n\n\n\npid_01\nbaseline\nplacebo\n8\n0.44\n5.7\n\n\npid_01\nweek_1\nplacebo\n7\n1.66\n5.2\n\n\npid_01\nweek_7\nplacebo\n7\n1.44\n5.4\n\n\npid_02\nbaseline\nplacebo\n7\n1.55\n5.2\n\n\npid_02\nweek_1\nplacebo\n7\n0.75\n4.8\n\n\npid_02\nweek_7\nplacebo\n4\n1.17\n4.2"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#ph-mapped-to-y-position",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#ph-mapped-to-y-position",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "pH mapped to y position",
    "text": "pH mapped to y position"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#ph-mapped-to-color",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#ph-mapped-to-color",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "pH mapped to color",
    "text": "pH mapped to color"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#commonly-used-aesthetics",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#commonly-used-aesthetics",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Commonly used aesthetics",
    "text": "Commonly used aesthetics\n\nFigure from  Claus O. Wilke. Fundamentals of Data Visualization. O’Reilly, 2019"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#the-same-data-values-can-be-mapped-to-different-aesthetics",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#the-same-data-values-can-be-mapped-to-different-aesthetics",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "The same data values can be mapped to different aesthetics",
    "text": "The same data values can be mapped to different aesthetics\n\nFigure from  Claus O. Wilke. Fundamentals of Data Visualization. O’Reilly, 2019"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#we-can-use-many-different-aesthetics-at-once",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#we-can-use-many-different-aesthetics-at-once",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "We can use many different aesthetics at once",
    "text": "We can use many different aesthetics at once"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#we-define-the-mapping-with-aes",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#we-define-the-mapping-with-aes",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "We define the mapping with aes()",
    "text": "We define the mapping with aes()\n\n```{r}\ntable_02 %&gt;%\n  ggplot(mapping = aes(x = time_point, y = ph, color = ph)) +\n  geom_jitter()\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#we-frequently-omit-argument-names",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#we-frequently-omit-argument-names",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "We frequently omit argument names",
    "text": "We frequently omit argument names\nLong form, all arguments are named:\n\n```{r}\n#| eval: false\n\nggplot(\n  data= table_02,\n  mapping = aes(x = time_point, y = ph, color = ph)\n) +\n  geom_jitter()\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#we-frequently-omit-argument-names-1",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#we-frequently-omit-argument-names-1",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "We frequently omit argument names",
    "text": "We frequently omit argument names\nAbbreviated form, common arguments remain unnamed:\n\n```{r}\n#| eval: false\n\nggplot(table_02, aes(x = time_point, y = ph, color = ph)) +\n  geom_jitter()\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#the-geom-determines-how-the-data-is-shown",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#the-geom-determines-how-the-data-is-shown",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "The geom determines how the data is shown",
    "text": "The geom determines how the data is shown\n\n```{r}\nggplot(table_02, aes(x = time_point, y = ph, color = ph)) +\n  geom_point()\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#the-geom-determines-how-the-data-is-shown-1",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#the-geom-determines-how-the-data-is-shown-1",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "The geom determines how the data is shown",
    "text": "The geom determines how the data is shown\n\n```{r}\nggplot(table_02, aes(x = time_point, y = ph, color = ph)) +\n  geom_boxplot()\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#the-geom-determines-how-the-data-is-shown-2",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#the-geom-determines-how-the-data-is-shown-2",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "The geom determines how the data is shown",
    "text": "The geom determines how the data is shown\n\n```{r}\nggplot(table_02, aes(x = time_point, y = ph, color = ph)) +\n  geom_jitter()\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#different-geoms-have-parameters-for-control",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#different-geoms-have-parameters-for-control",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Different geoms have parameters for control",
    "text": "Different geoms have parameters for control\n\n```{r}\nggplot(table_02, aes(x = time_point, y = ph, color = ph)) +\n  geom_jitter(size=3)\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#different-geoms-have-parameters-for-control-1",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#different-geoms-have-parameters-for-control-1",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Different geoms have parameters for control",
    "text": "Different geoms have parameters for control\n\n```{r}\nggplot(table_02, aes(x = time_point, y = ph, color = ph)) +\n  geom_jitter(size=3, width = 0.2)\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#important-color-and-fill-apply-to-different-elements",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#important-color-and-fill-apply-to-different-elements",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Important: color and fill apply to different elements",
    "text": "Important: color and fill apply to different elements\ncolor Applies color to points, lines, text, borders\nfill Applies color to any filled areas"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#many-geoms-have-both-color-and-fill-aesthetics",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#many-geoms-have-both-color-and-fill-aesthetics",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Many geoms have both color and fill aesthetics",
    "text": "Many geoms have both color and fill aesthetics\n\n\n```{r}\n#| output-location: column\nggplot(\n  data = table_02,\n  mapping = aes(\n    x = time_point,\n    y = ph,\n    color = time_point\n  )\n) + geom_boxplot()\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#many-geoms-have-both-color-and-fill-aesthetics-1",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#many-geoms-have-both-color-and-fill-aesthetics-1",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Many geoms have both color and fill aesthetics",
    "text": "Many geoms have both color and fill aesthetics\n\n\n```{r}\n#| output-location: column\nggplot(\n  data = table_02,\n  mapping = aes(\n    x = time_point,\n    y = ph,\n    fill = time_point\n  )\n) + geom_boxplot()\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#many-geoms-have-both-color-and-fill-aesthetics-2",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#many-geoms-have-both-color-and-fill-aesthetics-2",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Many geoms have both color and fill aesthetics",
    "text": "Many geoms have both color and fill aesthetics\n\n\n```{r}\n#| output-location: column\nggplot(\n  data = table_02,\n  mapping = aes(\n    x = time_point,\n    y = ph,\n    fill = time_point,\n    color = time_point\n  )\n) + geom_boxplot()\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#aesthetics-can-also-be-used-as-parameters-in-geoms",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#aesthetics-can-also-be-used-as-parameters-in-geoms",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Aesthetics can also be used as parameters in geoms",
    "text": "Aesthetics can also be used as parameters in geoms\n\n\n```{r}\n#| output-location: column\nggplot(\n  data = table_02,\n  mapping = aes(\n    x = time_point,\n    y = ph\n  )\n) + geom_boxplot()\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#aesthetics-can-also-be-used-as-parameters-in-geoms-1",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#aesthetics-can-also-be-used-as-parameters-in-geoms-1",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Aesthetics can also be used as parameters in geoms",
    "text": "Aesthetics can also be used as parameters in geoms\n\n\n```{r}\n#| output-location: column\nggplot(\n  data = table_02,\n  mapping = aes(\n    x = time_point,\n    y = ph\n  )\n) + geom_boxplot(fill=\"orange\")\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#analyze-subsets-group_by-and-summarize",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#analyze-subsets-group_by-and-summarize",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Analyze subsets: group_by() and summarize()",
    "text": "Analyze subsets: group_by() and summarize()"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#example-application-of-grouping-counting",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#example-application-of-grouping-counting",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Example application of grouping: Counting",
    "text": "Example application of grouping: Counting\nPreviously we used count, now we group the data\n\n```{r}\ntable_01 %&gt;%\n  group_by(smoker)\n```\n\n# A tibble: 44 × 6\n# Groups:   smoker [2]\n   pid    arm       smoker       age education                     sex  \n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                         &lt;lgl&gt;\n 1 pid_01 placebo   non-smoker    26 grade 10-12, matriculated     FALSE\n 2 pid_02 placebo   smoker        33 grade 10-12, matriculated     FALSE\n 3 pid_03 placebo   smoker        30 post-secondary                FALSE\n 4 pid_04 placebo   non-smoker    34 grade 10-12, not matriculated FALSE\n 5 pid_05 treatment non-smoker    29 grade 10-12, matriculated     FALSE\n 6 pid_06 placebo   smoker        34 post-secondary                FALSE\n 7 pid_07 placebo   non-smoker    31 grade 10-12, not matriculated FALSE\n 8 pid_08 placebo   smoker        30 grade 10-12, not matriculated FALSE\n 9 pid_09 treatment non-smoker    35 grade 10-12, not matriculated FALSE\n10 pid_10 treatment non-smoker    32 less than grade 9             FALSE\n# ℹ 34 more rows"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#example-application-of-grouping-counting-1",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#example-application-of-grouping-counting-1",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Example application of grouping: Counting",
    "text": "Example application of grouping: Counting\nPreviously we used count, now we group the data, and then summarise\n\n```{r}\ntable_01 %&gt;%\n  group_by(smoker) %&gt;%\n  summarise(\n    n = n() # n() returns the number of observations per group\n    )\n```\n\n# A tibble: 2 × 2\n  smoker         n\n  &lt;chr&gt;      &lt;int&gt;\n1 non-smoker    27\n2 smoker        17"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#example-application-of-grouping-counting-2",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#example-application-of-grouping-counting-2",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Example application of grouping: Counting",
    "text": "Example application of grouping: Counting\nNow let’s group by multiple variables\n\n```{r}\ntable_01 %&gt;%\n  group_by(smoker, arm)\n```\n\n# A tibble: 44 × 6\n# Groups:   smoker, arm [4]\n   pid    arm       smoker       age education                     sex  \n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                         &lt;lgl&gt;\n 1 pid_01 placebo   non-smoker    26 grade 10-12, matriculated     FALSE\n 2 pid_02 placebo   smoker        33 grade 10-12, matriculated     FALSE\n 3 pid_03 placebo   smoker        30 post-secondary                FALSE\n 4 pid_04 placebo   non-smoker    34 grade 10-12, not matriculated FALSE\n 5 pid_05 treatment non-smoker    29 grade 10-12, matriculated     FALSE\n 6 pid_06 placebo   smoker        34 post-secondary                FALSE\n 7 pid_07 placebo   non-smoker    31 grade 10-12, not matriculated FALSE\n 8 pid_08 placebo   smoker        30 grade 10-12, not matriculated FALSE\n 9 pid_09 treatment non-smoker    35 grade 10-12, not matriculated FALSE\n10 pid_10 treatment non-smoker    32 less than grade 9             FALSE\n# ℹ 34 more rows"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#example-application-of-grouping-counting-3",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#example-application-of-grouping-counting-3",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Example application of grouping: Counting",
    "text": "Example application of grouping: Counting\nNow let’s group by multiple variables, and summarise\n\n```{r}\ntable_01 %&gt;%\n  group_by(smoker, arm) %&gt;%\n    summarise(\n    n = n() # n() returns the number of observations per group\n    )\n```\n\n# A tibble: 4 × 3\n# Groups:   smoker [2]\n  smoker     arm           n\n  &lt;chr&gt;      &lt;chr&gt;     &lt;int&gt;\n1 non-smoker placebo      12\n2 non-smoker treatment    15\n3 smoker     placebo      11\n4 smoker     treatment     6"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#example-application-of-grouping-counting-4",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#example-application-of-grouping-counting-4",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Example application of grouping: Counting",
    "text": "Example application of grouping: Counting\ncount(...) is a short-cut for group_by(...) %&gt;% summarize(n = n()) group_by() and summarise()is the general method\n\n```{r}\ntable_01 %&gt;%\n  count(smoker, arm)\n\ntable_01 %&gt;%\n  group_by(smoker, arm) %&gt;%\n  summarise(median_age = median(age))\n```\n\n# A tibble: 4 × 3\n  smoker     arm           n\n  &lt;chr&gt;      &lt;chr&gt;     &lt;int&gt;\n1 non-smoker placebo      12\n2 non-smoker treatment    15\n3 smoker     placebo      11\n4 smoker     treatment     6\n# A tibble: 4 × 3\n# Groups:   smoker [2]\n  smoker     arm       median_age\n  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;\n1 non-smoker placebo         31  \n2 non-smoker treatment       30  \n3 smoker     placebo         33  \n4 smoker     treatment       33.5"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#you-can-make-multiple-summarise-at-once",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#you-can-make-multiple-summarise-at-once",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "You can make multiple summarise at once",
    "text": "You can make multiple summarise at once\n\n```{r}\ntable_01 %&gt;%\n  group_by(smoker, arm) %&gt;%\n  summarise(\n    n = n(),\n    median_age = median(age)\n    )\n```\n\n# A tibble: 4 × 4\n# Groups:   smoker [2]\n  smoker     arm           n median_age\n  &lt;chr&gt;      &lt;chr&gt;     &lt;int&gt;      &lt;dbl&gt;\n1 non-smoker placebo      12       31  \n2 non-smoker treatment    15       30  \n3 smoker     placebo      11       33  \n4 smoker     treatment     6       33.5"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#reshape-pivot_wider-and-pivot_longer",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#reshape-pivot_wider-and-pivot_longer",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Reshape: pivot_wider() and pivot_longer()",
    "text": "Reshape: pivot_wider() and pivot_longer()"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#reshaping-example-making-a-wide-summary-table",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#reshaping-example-making-a-wide-summary-table",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Reshaping example: Making a wide summary table",
    "text": "Reshaping example: Making a wide summary table\n\n```{r}\ntable_01 %&gt;%\n  count(education, arm) %&gt;%\n  pivot_wider(names_from = arm, values_from = n)\n```\n\n# A tibble: 4 × 3\n  education                     placebo treatment\n  &lt;chr&gt;                           &lt;int&gt;     &lt;int&gt;\n1 grade 10-12, matriculated           7         9\n2 grade 10-12, not matriculated      11         7\n3 less than grade 9                   2         4\n4 post-secondary                      3         1"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#reshaping-example-making-a-wide-summary-table-1",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#reshaping-example-making-a-wide-summary-table-1",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Reshaping example: Making a wide summary table",
    "text": "Reshaping example: Making a wide summary table\n\n```{r}\neducation_wide &lt;- table_01 %&gt;%\n  count(education, arm) %&gt;%\n  pivot_wider(names_from = arm, values_from = n)\n\neducation_wide %&gt;%\n  pivot_longer(-education, names_to = \"arm\", values_to = \"n\")\n```\n\n# A tibble: 8 × 3\n  education                     arm           n\n  &lt;chr&gt;                         &lt;chr&gt;     &lt;int&gt;\n1 grade 10-12, matriculated     placebo       7\n2 grade 10-12, matriculated     treatment     9\n3 grade 10-12, not matriculated placebo      11\n4 grade 10-12, not matriculated treatment     7\n5 less than grade 9             placebo       2\n6 less than grade 9             treatment     4\n7 post-secondary                placebo       3\n8 post-secondary                treatment     1"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#we-use-joins-to-add-columns-from-one-table-into-another",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#we-use-joins-to-add-columns-from-one-table-into-another",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "We use joins to add columns from one table into another",
    "text": "We use joins to add columns from one table into another"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#there-are-different-types-of-joins",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#there-are-different-types-of-joins",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "There are different types of joins",
    "text": "There are different types of joins\nThe differences are all about how to handle when the two tables have different key values\n\n\n\nleft_join() - the resulting table always has the same key_values as the “left” table\nright_join() - the resulting table always has the same key_values as the “right” table\ninner_join() - the resulting table always only keeps the key_values that are in both tables\nfull_join() - the resulting table always has all key_values found in both tables"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#left-join",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#left-join",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Left Join",
    "text": "Left Join\nleft_join() - the resulting table always has the same key_values as the “left” table"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#inner_join",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#inner_join",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "inner_join",
    "text": "inner_join\ninner_join() - the resulting table always only keeps the key_values that are in both tables\n\n\ntable_a %&gt;% inner_join(table_b)"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#note-merging-tables-vertically-is-bind_rows-not-a-join",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#note-merging-tables-vertically-is-bind_rows-not-a-join",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Note, merging tables vertically is bind_rows(), not a join",
    "text": "Note, merging tables vertically is bind_rows(), not a join\n\n\ntable_a %&gt;% bind_rows(table_b)"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#histograms-and-density-plots",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#histograms-and-density-plots",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Histograms and density plots",
    "text": "Histograms and density plots\n\n\n\n\n\n\n\nage\nsex\nclass\nsurvived\n\n\n\n\n0.17\nfemale\n3rd\nsurvived\n\n\n0.33\nmale\n3rd\ndied\n\n\n0.80\nmale\n2nd\nsurvived\n\n\n0.83\nmale\n2nd\nsurvived\n\n\n0.83\nmale\n3rd\nsurvived\n\n\n0.92\nmale\n1st\nsurvived\n\n\n1.00\nfemale\n2nd\nsurvived\n\n\n1.00\nfemale\n3rd\nsurvived\n\n\n1.00\nmale\n2nd\nsurvived\n\n\n1.00\nmale\n2nd\nsurvived\n\n\n1.00\nmale\n3rd\nsurvived\n\n\n1.50\nfemale\n3rd\ndied\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\nclass\nsurvived\n\n\n\n\n1.5\nfemale\n3rd\ndied\n\n\n2.0\nfemale\n1st\ndied\n\n\n2.0\nfemale\n2nd\nsurvived\n\n\n2.0\nfemale\n3rd\ndied\n\n\n2.0\nfemale\n3rd\ndied\n\n\n2.0\nmale\n2nd\nsurvived\n\n\n2.0\nmale\n2nd\nsurvived\n\n\n2.0\nmale\n2nd\nsurvived\n\n\n3.0\nfemale\n2nd\nsurvived\n\n\n3.0\nfemale\n3rd\nsurvived\n\n\n3.0\nmale\n2nd\nsurvived\n\n\n3.0\nmale\n2nd\nsurvived\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\nclass\nsurvived\n\n\n\n\n3\nmale\n3rd\nsurvived\n\n\n3\nmale\n3rd\nsurvived\n\n\n4\nfemale\n2nd\nsurvived\n\n\n4\nfemale\n2nd\nsurvived\n\n\n4\nfemale\n3rd\nsurvived\n\n\n4\nfemale\n3rd\nsurvived\n\n\n4\nmale\n1st\nsurvived\n\n\n4\nmale\n3rd\ndied\n\n\n4\nmale\n3rd\nsurvived\n\n\n5\nfemale\n3rd\nsurvived\n\n\n5\nfemale\n3rd\nsurvived\n\n\n5\nmale\n3rd\ndied"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#histograms-depend-on-the-chosen-bin-width",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#histograms-depend-on-the-chosen-bin-width",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Histograms depend on the chosen bin width",
    "text": "Histograms depend on the chosen bin width"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#making-histograms-with-ggplot-geom_histogram",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#making-histograms-with-ggplot-geom_histogram",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Making histograms with ggplot: geom_histogram()",
    "text": "Making histograms with ggplot: geom_histogram()\n\n```{r}\nggplot(titanic, aes(age)) +\n  geom_histogram()\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#setting-the-bin-width",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#setting-the-bin-width",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Setting the bin width",
    "text": "Setting the bin width\n\n```{r}\nggplot(titanic, aes(age)) +\n  geom_histogram(binwidth = 5)\n```\n\n\n\nDo you like where there bins are? What does the first bin say?"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#always-set-the-center-as-well-to-half-the-bin_width",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#always-set-the-center-as-well-to-half-the-bin_width",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Always set the center as well, to half the bin_width",
    "text": "Always set the center as well, to half the bin_width\n\n```{r}\nggplot(titanic, aes(age)) +\n  geom_histogram(binwidth = 5, center=2.5)\n```\n\n\n\nSetting center 2.5 makes the bars start 0-5, 5-10, etc. instead of 2.5-7.5, etc. You could instead use the argument boundary=5 to accomplish the same behavior."
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#making-density-plots-with-ggplot-geom_density",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#making-density-plots-with-ggplot-geom_density",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Making density plots with ggplot: geom_density()",
    "text": "Making density plots with ggplot: geom_density()\n\n```{r}\nggplot(titanic, aes(age)) +\n  geom_density(fill = \"skyblue\")\n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#making-density-plots-with-ggplot-geom_density-1",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#making-density-plots-with-ggplot-geom_density-1",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Making density plots with ggplot: geom_density()",
    "text": "Making density plots with ggplot: geom_density()\n\n```{r}\nggplot(titanic, aes(age)) +\n  geom_density()\n```\n\n\nwithout fill"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#boxplots-showing-values-along-y-conditions-along-x",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#boxplots-showing-values-along-y-conditions-along-x",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Boxplots: Showing values along y, conditions along x",
    "text": "Boxplots: Showing values along y, conditions along x\n\nA boxplot is a crude way of visualizing a distribution."
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#how-to-read-a-boxplot",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#how-to-read-a-boxplot",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "How to read a boxplot",
    "text": "How to read a boxplot"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#if-you-like-density-plots-consider-violins",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#if-you-like-density-plots-consider-violins",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "If you like density plots, consider violins",
    "text": "If you like density plots, consider violins\n\n\nA violin plot is a density plot rotated 90 degrees and then mirrored."
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#how-to-read-a-violin-plot",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#how-to-read-a-violin-plot",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "How to read a violin plot",
    "text": "How to read a violin plot"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#making-boxplots-violins-etc.-in-ggplot2-1",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#making-boxplots-violins-etc.-in-ggplot2-1",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Making boxplots, violins, etc. in ggplot2",
    "text": "Making boxplots, violins, etc. in ggplot2\n\n\n\n\n\n\n\n\nPlot type\nGeom\nNotes\n\n\n\n\nboxplot\ngeom_boxplot()\n\n\n\nviolin plot\ngeom_violin()\n\n\n\nstrip chart\ngeom_point()\nJittering requires position_jitter()\n\n\nsina plot\ngeom_sina()\nFrom package ggforce\n\n\nscatter-density plot\ngeom_quasirandom()\nFrom package ggbeeswarm\n\n\nridgeline\ngeom_density_ridges()\nFrom package ggridges"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#examples-boxplot",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#examples-boxplot",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Examples: Boxplot",
    "text": "Examples: Boxplot\n\n```{r}\nggplot(lincoln_temps, aes(x = month, y = mean_temp)) +\n  geom_boxplot(fill = \"skyblue\") \n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#examples-violins",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#examples-violins",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Examples: Violins",
    "text": "Examples: Violins\n\n```{r}\nggplot(lincoln_temps, aes(x = month, y = mean_temp)) +\n  geom_violin(fill = \"skyblue\") \n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#examples-strip-chart-no-jitter",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#examples-strip-chart-no-jitter",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Examples: Strip chart (no jitter)",
    "text": "Examples: Strip chart (no jitter)\n\n```{r}\nggplot(lincoln_temps, aes(x = month, y = mean_temp)) +\n  geom_point(color = \"skyblue\") \n```"
  },
  {
    "objectID": "materials/2-workshop2/2-r-refresher/slides.html#examples-strip-chart-w-jitter",
    "href": "materials/2-workshop2/2-r-refresher/slides.html#examples-strip-chart-w-jitter",
    "title": "R refresher (Brief review of Workshop I)",
    "section": "Examples: Strip chart (w/ jitter)",
    "text": "Examples: Strip chart (w/ jitter)\n\n```{r}\nggplot(lincoln_temps, aes(x = month, y = mean_temp)) +\n  geom_jitter(color = \"skyblue\") \n```\n\n\n\n\n\nback to module"
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html",
    "title": "16S Data Analysis Part I",
    "section": "",
    "text": "We’ve inferred and classified amplicon sequence variants (ASVs) using the dada2 package.\nNow let’s do some exploratory data analysis (EDA) using the phyloseq package and other tools."
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html#eda-mindset",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html#eda-mindset",
    "title": "16S Data Analysis Part I",
    "section": "EDA mindset",
    "text": "EDA mindset\nExploratory mindset:\n\n“I’m very curious about my data”\n“It’s unlikely that my data are perfect”\n“I’d like to familiarize myself with the data”"
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html#get-set-up",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html#get-set-up",
    "title": "16S Data Analysis Part I",
    "section": "0. Get set up",
    "text": "0. Get set up\n\nLoad packages\nLoad some packages including the phyloseq package\n\nlibrary(phyloseq)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(rstatix)\nlibrary(pander)\nlibrary(here)\n\nTo get Help with any function, type a question mark (?) in front of the function name at the Console. For example, type ?dplyr::filter (or ?filter) and hit enter to get Help with the dplyr function filter().\n\n\nManage file paths\nShow path to the current project\n\nhere::here()\n\n[1] \"/Users/jelsherbini/dev/durban-data-science-for-biology\"\n\n\nShow paths to the data we intend to analyze\n\nfs::dir_ls(here(\"materials/2-workshop2/4-16s-pt1/data\"))\n\n/Users/jelsherbini/dev/durban-data-science-for-biology/materials/2-workshop2/4-16s-pt1/data/instructional\n/Users/jelsherbini/dev/durban-data-science-for-biology/materials/2-workshop2/4-16s-pt1/data/practice\n\n\nCreate an object containing the path to the instructional data\n\nin_path &lt;- file.path(here(\"materials/2-workshop2/4-16s-pt1/data/instructional/\"))\nin_path\n\n[1] \"/Users/jelsherbini/dev/durban-data-science-for-biology/materials/2-workshop2/4-16s-pt1/data/instructional/\"\n\n\nJust for fun, let’s be curious about this object in_path. What type of object is it? How many strings does it contain? How many characters in this string?\n\nclass(in_path)\n\n[1] \"character\"\n\nlength(in_path)\n\n[1] 1\n\nnchar(in_path)\n\n[1] 106\n\n\nWe can answer these questions. It’s a character vector containing 1 string consisting of 106 characters.\nWe’ll use this object to construct commands for loading our data.\n\n\nLoad instructional data\nList the files we intend to load\n\nlist.files(in_path)\n\n[1] \"count_table_instructional.rds\" \"sample_data_instructional.rds\"\n[3] \"tax_table_instructional.rds\"  \n\n\nThe count_table and tax_table are the outputs of dada2. The sample_data is a version of the file 05_amplicon_sample_ids_UKZN_workshop_2023.csv that I’ve modified for use here.\nNote: An .rds file contains a single R object which has been saved to a file. The file can be loaded back into the R environment using the readRDS() function.\nLet’s load the instructional data\n\nsample_df &lt;- readRDS(paste0(in_path, \"sample_data_instructional.rds\"))\ncount_tab &lt;- readRDS(paste0(in_path, \"count_table_instructional.rds\"))\ntax_tab &lt;- readRDS(paste0(in_path, \"tax_table_instructional.rds\"))\n\nNote: The paste0() function is a variation of paste() that concatenates strings without any separator."
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html#get-to-know-the-data",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html#get-to-know-the-data",
    "title": "16S Data Analysis Part I",
    "section": "1. Get to know the data",
    "text": "1. Get to know the data\nFamiliarize yourself with the data:\n\nThink about how you would describe or summarize it\nExamine the objects for form, size, and content\nReview and visualize your study design\nIdentify any missing data\n\n\nSample data\nLet’s begin with the sample data.\nGlimpse the object using head()\n\nsample_df %&gt;% \n  head(n = 3) %&gt;% pander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\namplicon_sample_id\namplicon_type\ninstrument_run\nreads_out\npid\n\n\n\n\nGUT0003\nstudy_sample\nrun1\n112769\npid_01\n\n\nVAG0142\nstudy_sample\nrun2\n73426\npid_01\n\n\nGUT0050\nstudy_sample\nrun1\n20211\npid_01\n\n\n\n\n\n\n\n\n\n\n\narm\ntime_point\nsample_type\n\n\n\n\nplacebo\nbaseline\ngut_biopsy\n\n\nplacebo\nbaseline\nvaginal\n\n\nplacebo\nweek_1\ngut_biopsy\n\n\n\n\n\nGlimpse the object using tail()\n\nsample_df %&gt;% \n  tail(n = 3) %&gt;% pander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\n \namplicon_sample_id\namplicon_type\ninstrument_run\nreads_out\n\n\n\n\n272\nZYMO.DNA.CTRL.P6.run2\nzymo_mock\nrun2\n41915\n\n\n273\nZYMO.DNA.CTRL.P7.run2\nzymo_mock\nrun2\n48564\n\n\n274\nZYMO.DNA.CTRL.P8.run2\nzymo_mock\nrun2\n38318\n\n\n\n\n\n\n\n\n\n\n\n\n\n \npid\narm\ntime_point\nsample_type\n\n\n\n\n272\nNA\nNA\nNA\nNA\n\n\n273\nNA\nNA\nNA\nNA\n\n\n274\nNA\nNA\nNA\nNA\n\n\n\n\n\nDoes this object contain any missing data? How much?\n\nsum(is.na(sample_df))\n\n[1] 40\n\n\nYes it does.\nThis object is a data.frame with 274 rows (observations) and 8 columns (variables). We acknowledge the presence of NA values (missing data).\nNote: I’ve added a variable sample_df$reads_out containing the total number of reads output by dada2. This variable can be taken from the portion of the dada2 workflow in which reads are tracked through the pipeline.\nIn our sample data, we see variables describing the amplicons. How many amplicons did we sequence on each run?\n\nsample_df %&gt;% \n  count(instrument_run, amplicon_type) %&gt;% pander()\n\n\n\n\n\n\n\n\n\ninstrument_run\namplicon_type\nn\n\n\n\n\nrun1\nstudy_sample\n132\n\n\nrun1\nzymo_mock\n5\n\n\nrun2\nstudy_sample\n132\n\n\nrun2\nzymo_mock\n5\n\n\n\n\n\nOn each instrument run, we sequenced n = 132 study samples and n = 5 mock communities (positive controls).\nHere, the mock communities are the ZymoBIOMICS Microbial Community DNA Standard. Later, we’ll use these mocks to assess for batch effects (run1 vs run2).\nWe also have variables pertaining to the study design. Let’s report the number of participants in the study without counting the NA values associated with the mocks.\nHere are two options\n\nlength(unique(sample_df$pid)[!is.na(unique(sample_df$pid))]) # Option 1 using base R\n\n[1] 44\n\nsample_df %&gt;% drop_na() %$% n_distinct(pid) # Option 2 using tidyverse & magrittr\n\n[1] 44\n\n\nWhich option do you find more readable?\nNote: The %$% operator is known as the “exposition pipe operator”. It’s part of the magrittr package and is used to expose the variables in a dataframe to the environment in an expression.\nNow let’s review the study design. Often, a visualization works well for this purpose.\nPlot our study design\n\nsample_df %&gt;% \n  drop_na() %&gt;% # Without this line, what happens? Why? \n  mutate(has_reads = reads_out &gt; 0) %&gt;% \n  ggplot(aes(x = time_point, y = pid, color = has_reads)) +\n  geom_point() +\n  facet_grid(rows = vars(arm), cols = vars(sample_type), scales = \"free_y\") +\n  labs(title = \"study design\", y = \"participant_id\")\n\n\n\n\n\n\n\n\nWe recall that our study involves 44 participants, randomized to placebo or treatment, who contributed a gut and vaginal sample at baseline (before intervention) and at 1 and 7 weeks after intervention.\nWe also note that three samples failed to return any reads after processing through dada2. (What happened?)\nLet’s make a plot that looks closely at our sequencing yields\n\nsample_df %&gt;% \n  filter(reads_out &gt; 0) %&gt;% \n  mutate(sample_type = ifelse(is.na(sample_type), \"zymo\", sample_type)) %&gt;% \n  ggplot(aes(x = reads_out, fill = sample_type)) +\n  geom_histogram(bins = 50) +\n  facet_wrap(.~instrument_run) +\n  labs(title = \"sequencing yield\")\n\n\n\n\n\n\n\n\nThese histograms have very different shapes. If this is surprising or concerning to us, we might investigate why this happened. It’s also clear that we have additional samples with relatively low yield. How would you quickly modify the code above to display only those samples with fewer than 500 reads?\nWhen our work involves batches, we always want to include replicate samples (here, the zymo mocks) that allow us to assess for batch effects. Looking at the plot above, why was this extra important in the context of this study?\nFinally, we can compute some summary statistics for this variable sample_df$reads_out for each instrument run\n\nsample_df %&gt;% \n  group_by(instrument_run) %&gt;% \n  get_summary_stats(reads_out, type = \"five_number\") %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninstrument_run\nvariable\nn\nmin\nmax\nq1\nmedian\nq3\n\n\n\n\nrun1\nreads_out\n137\n0\n257270\n24926\n62910\n124354\n\n\nrun2\nreads_out\n137\n0\n96147\n45022\n57113\n69108\n\n\n\n\n\nIn the code above we’ve used the function get_summary_stats() from the package rstatix. This function allows for different types of summary statistics (we’ve used type = “five number”). We note that we have a median of around 60k reads per sample.\n\n\nCount table\nNow let’s look at the count table. This table contains the count of each amplicon sequence variant (ASV) in each sample. It will become the core of our phyloseq object.\nDescribe the count table\n\nclass(count_tab)\n\n[1] \"matrix\" \"array\" \n\nnrow(count_tab)\n\n[1] 271\n\nncol(count_tab)\n\n[1] 6422\n\nsum(is.na(count_tab))\n\n[1] 0\n\n\nThe count table is a matrix (a 2D array) with 271 rows and 6422 columns. There is no missing data. Based on these numbers, it seems that rows are samples and columns are ASVs.\nLet’s make sure. How are the rows and columns named (labelled)?\nWhat are the row names?\n\nrownames(count_tab) %&gt;% head(n = 3)\n\n[1] \"GUT0001\" \"GUT0002\" \"GUT0003\"\n\n\nIn this count table, the rows are samples. They are named using the amplicon_sample_id.\nIs every sample in the count table listed in the sample data?\n\nsample_df %&gt;% \n  filter(!amplicon_sample_id %in% rownames(count_tab)) %&gt;% pander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\namplicon_sample_id\namplicon_type\ninstrument_run\nreads_out\npid\n\n\n\n\nGUT0071\nstudy_sample\nrun1\n0\npid_08\n\n\nGUT0117\nstudy_sample\nrun1\n0\npid_22\n\n\nVAG0280\nstudy_sample\nrun2\n0\npid_40\n\n\n\n\n\n\n\n\n\n\n\narm\ntime_point\nsample_type\n\n\n\n\nplacebo\nweek_7\ngut_biopsy\n\n\ntreatment\nweek_1\ngut_biopsy\n\n\ntreatment\nweek_1\nvaginal\n\n\n\n\n# And this difficult-to-read command should evaluate to TRUE\nsum(rownames(count_tab) %in% sample_df$amplicon_sample_id) == nrow(count_tab)\n\n[1] TRUE\n\n\nYes, all of the samples in the count table are present in the sample data, except for the three samples that failed (and that’s okay, we can leave these three samples in the sample data).\nWhat are the column names?\n\ncolnames(count_tab) %&gt;% head(n = 3)\n\n[1] \"TAGGGAATCTTCCACAATGGACGCAAGTCTGATGGAGCAACGCCGCGTGAGTGAAGAAGGGTTTCGGCTCGTAAAGCTCTGTTGTTGGTGAAGAAGGACAGGGGTAGTAACTGACCTTTGTTTGACGGTAATCAATTAGAAAGTCACGGCTAACTACGTGCCAGCAGCCGCGGTAATACGTAGGTGGCAAGCGTTGTCCGGATTTATTGGGCGTAAAGCGAGTGCAGGCGGCTCGATAAGTCTGATGTGAAAGCCTTCGGCTCAACCGGAGAATTGCATCAGAAACTGTCGAGCTTGAGTACAGAAGAGGAGAGTGGAACTCCATGTGTAGCGGTGAAATGCGTAGATATATGGAAGAACACCGGTGGCGAAGGCGGCTCTCTGGTCTGTTACTGACGCTGAGGCTCGAAAGCATGGGTAGCGAACAGG\"\n[2] \"TGAGGAATATTGGTCAATGGGCGAGAGCCTGAACCAGCCAAGTAGCGTGAAGGATGACTGCCCTATGGGTTGTAAACTTCTTTTATAAAGGAATAAAGTCGGGTATGGATACCCGTTTGCATGTACTTTATGAATAAGGATCGGCTAACTCCGTGCCAGCAGCCGCGGTAATACGGAGGATCCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGAGCGTAGATGGATGTTTAAGTCAGTTGTGAAAGTTTGCGGCTCAACCGTAAAATTGCAGTTGATACTGGATATCTTGAGTGCAGTTGAGGCAGGCGGAATTCGTGGTGTAGCGGTGAAATGCTTAGATATCACGAAGAACTCCGATTGCGAAGGCAGCCTGCTAAGCTGCAACTGACATTGAGGCTCGAAAGTGTGGGTATCAAACAGG\"     \n[3] \"TGGGGAATATTGCACAATGGGCGCAAGCCTGATGCAGCCATGCCGCGTGTATGAAGAAGGCCTTCGGGTTGTAAAGTACTTTCAGCGGGGAGGAAGGGAGTAAAGTTAATACCTTTGCTCATTGACGTTACCCGCAGAAGAAGCACCGGCTAACTCCGTGCCAGCAGCCGCGGTAATACGGAGGGTGCAAGCGTTAATCGGAATTACTGGGCGTAAAGCGCACGCAGGCGGTTTGTTAAGTCAGATGTGAAATCCCCGGGCTCAACCTGGGAACTGCATCTGATACTGGCAAGCTTGAGTCTCGTAGAGGGGGGTAGAATTCCAGGTGTAGCGGTGAAATGCGTAGAGATCTGGAGGAATACCGGTGGCGAAGGCGGCCCCCTGGACGAAGACTGACGCTCAGGTGCGAAAGCGTGGGGAGCAAACAGG\"\n\n\nWow, how long are these names?\n\ncolnames(count_tab) %&gt;% head(n = 3) %&gt;% nchar()\n\n[1] 429 424 429\n\n\nIn this count table, the columns are ASVs and they are named using … the ASV sequences themselves!! Here, these labels can be &gt;400 characters long. (Discuss pros, cons, and workarounds for using ASV sequences as labels.)\nFinally, how “sparse” is our count table? In other words, what fraction of the elements are zero?\n\nsum(count_tab == 0) / length(count_tab)\n\n[1] 0.9858483\n\n\nOver 98% of the elements are zero. This count table is very sparse.\nCan we create a plot that helps us think about this?\nLet’s try\n\ndata.frame(asv_prev = colSums(count_tab &gt; 0)) %&gt;% \n  ggplot(aes(x = asv_prev)) +\n  geom_histogram(bins = 50) +\n  labs(x = \"Number of samples in which the ASV was detected\",\n       y = \"Number of ASVs\")\n\n\n\n\n\n\n\n\nThe table is sparse because most ASVs are absent from most samples. There are very few ASVs that are widespread (detected in many or most samples). How could we modify the above code to see the number of ASVs detected in many (say, &gt;50) samples? Hint: Use the filter() function.\nLet’s save this ASV prevalence data in a dataframe. We can also add the total count for the ASV.\n\nprevalence_df &lt;- data.frame(asv_sum = colSums(count_tab),\n                            asv_prev = colSums(count_tab &gt; 0)) %&gt;% \n  rownames_to_column(var = \"asv_seq\") \n\nHow prevalent is the most prevalent ASV?\n\nprevalence_df %&gt;% \n  filter(asv_prev == max(asv_prev))\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                        asv_seq\n1 TAGGGAATCTTCCACAATGGACGCAAGTCTGATGGAGCAACGCCGCGTGAGTGAAGAAGGGTTTCGGCTCGTAAAGCTCTGTTGTTGGTGAAGAAGGACAGGGGTAGTAACTGACCTTTGTTTGACGGTAATCAATTAGAAAGTCACGGCTAACTACGTGCCAGCAGCCGCGGTAATACGTAGGTGGCAAGCGTTGTCCGGATTTATTGGGCGTAAAGCGAGTGCAGGCGGCTCGATAAGTCTGATGTGAAAGCCTTCGGCTCAACCGGAGAATTGCATCAGAAACTGTCGAGCTTGAGTACAGAAGAGGAGAGTGGAACTCCATGTGTAGCGGTGAAATGCGTAGATATATGGAAGAACACCGGTGGCGAAGGCGGCTCTCTGGTCTGTTACTGACGCTGAGGCTCGAAAGCATGGGTAGCGAACAGG\n  asv_sum asv_prev\n1 2147189      228\n\n\nThis ASV was detected in 228 out of 271 samples. Huh, I wonder who this is?\n\n\nTaxonomy table\nNow let’s look at the taxonomy table. This table contains the taxonomic assignments made using dada2.\nDescribe the taxonomy table\n\nclass(tax_tab)\n\n[1] \"matrix\" \"array\" \n\nnrow(tax_tab)\n\n[1] 6422\n\nncol(tax_tab)\n\n[1] 8\n\nsum(is.na(tax_tab))\n\n[1] 7388\n\n\nThe taxonomy table is a matrix (a 2D array) with 6422 rows and 8 columns. Missing data are present (in cases where the taxonomic assignment was unknown or ambiguous). Based on these numbers, it seems that rows are ASVs. What are the columns?\n\ncolnames(tax_tab)\n\n[1] \"Kingdom\" \"Phylum\"  \"Class\"   \"Order\"   \"Family\"  \"Genus\"   \"Species\"\n[8] \"Label\"  \n\n\nThe columns are taxonomic ranks. Note that I’ve added a variable at far right called “Label” which contains a short label for each ASV (e.g., “asv1”, “asv2”, etc).\nAt this point, it’s generally a good idea to explore various features of the ASVs.\nTo do this, one can begin by creating a dataframe from the taxonomy table. I think of this dataframe as a place to store any ASV-associated information, not only taxonomic assignments.\nSo let’s do this\n\nasv_df &lt;- tax_tab %&gt;%\n  data.frame() %&gt;% \n  rownames_to_column(var = \"asv_seq\") %&gt;% \n  left_join(prevalence_df, by = \"asv_seq\") %&gt;% # Add data from above \n  mutate(asv_len = nchar(asv_seq))\n\nNow we can explore, for example, the distribution of ASV lengths in relation to ASV prevalence\n\nasv_df %&gt;% \n  ggplot(aes(x = asv_len, y = asv_prev, color = Kingdom)) +\n  geom_point(alpha = 0.5, size = 3, stroke = FALSE) +\n  labs(title = \"How prevalent are any length outliers?\",\n       x = \"ASV length\",\n       y = \"ASV prevalence\")\n\n\n\n\n\n\n\n\nWe have length outliers; they are not very prevalent. After some follow-up, we might decide to filter them out. Maybe they are problematic ASVs, e.g., chimeric or non-specific (non-SSU rRNA) amplicons.\nWe also notice three non-bacterial ASVs. Interesting! Do they make sense, given the body sites we’ve sampled?\n\nasv_df %&gt;% \n  filter(Kingdom != \"Bacteria\") %&gt;% \n  select(-asv_seq) %&gt;% \n  select(Genus:asv_len) %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\n\nGenus\nSpecies\nLabel\nasv_sum\nasv_prev\nasv_len\n\n\n\n\nMethanobrevibacter\nNA\nasv1136\n670\n4\n388\n\n\nTrichomonas\nNA\nasv1815\n219\n2\n457\n\n\nMethanosphaera\nNA\nasv4889\n8\n1\n388\n\n\n\n\n\nOur sample types are gut and vaginal. So, yes, these make sense.\nLet’s look at a few of the oddly short ASVs\n\nasv_df %&gt;% \n  filter(asv_prev &gt; 1, asv_len &lt; 280) %&gt;% \n  select(-asv_seq) %&gt;% \n  head(n = 3) %&gt;% pander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\n\nKingdom\nPhylum\nClass\nOrder\nFamily\nGenus\n\n\n\n\nBacteria\nCyanobacteria\nOxyphotobacteria\nChloroplast\nNA\nNA\n\n\nBacteria\nCyanobacteria\nOxyphotobacteria\nChloroplast\nNA\nNA\n\n\nBacteria\nCyanobacteria\nOxyphotobacteria\nChloroplast\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\nLabel\nasv_sum\nasv_prev\nasv_len\n\n\n\n\nNA\nasv2318\n95\n4\n270\n\n\nNA\nasv2338\n93\n2\n271\n\n\nNA\nasv2576\n63\n2\n271\n\n\n\n\n\nThese are chloroplast sequences. We might consider filtering out ASVs assigned to the Order Chloroplast. But we should state our reasoning. For example, “chloroplasts are not, in and of themselves, members of the gut or vaginal microbiota”.\nLet’s look at a few long ASVs\n\nasv_df %&gt;% \n  filter(asv_len &gt; 450) %&gt;% \n  select(-asv_seq) %&gt;% \n  select(Genus:asv_len) %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\n\nGenus\nSpecies\nLabel\nasv_sum\nasv_prev\nasv_len\n\n\n\n\nTrichomonas\nNA\nasv1815\n219\n2\n457\n\n\nBacteroides\nNA\nasv5826\n3\n1\n460\n\n\nAlistipes\nNA\nasv6152\n2\n1\n460\n\n\nBacteroides\nNA\nasv6203\n2\n1\n475\n\n\nUreaplasma\nNA\nasv6384\n2\n1\n498\n\n\n\n\n\nOf these five, I’d guess the Trichomonas ASV is real (and from vaginal samples) and the others are problematic. Just my hunch.\nIn general, we’d continue to explore the ASV data until we developed an intuition of which ASVs, if any, we might want to set aside. For example, here we might retain ASVs with length &gt; 385-nt and &lt; 460-nt, that aren’t Order Chloroplast (or Family Mitochondria).\nLet’s make a vector containing the ASVs we wish to keep\n\nkeepers &lt;- asv_df %&gt;% \n  filter(asv_len &gt; 385 & asv_len &lt; 460) %&gt;% \n  # Careful below! Don't drop NA unless you intend to\n  filter(Order != \"Chloroplast\" | is.na(Order)) %&gt;% \n  filter(Family != \"Mitochondria\" | is.na(Family)) %$%\n  unique(asv_seq)\n\n# Retaining these ASVs removes how many ASVs?\nnrow(asv_df) - length(keepers)\n\n[1] 146\n\n\nBtw, who was it - the most prevalent ASV in our dataset?\n\nasv_df %&gt;% \n  filter(asv_prev == max(asv_prev)) %&gt;% \n  select(Genus:asv_len) %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\n\nGenus\nSpecies\nLabel\nasv_sum\nasv_prev\nasv_len\n\n\n\n\nLactobacillus\niners\nasv1\n2147189\n228\n429\n\n\n\n\n\nInteresting, we think of L. iners as a vaginal species. Does is also appear in the gut?"
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html#make-phyloseq-object",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html#make-phyloseq-object",
    "title": "16S Data Analysis Part I",
    "section": "2. Make phyloseq object",
    "text": "2. Make phyloseq object\nLet’s build our phyloseq object using the phyloseq() function from the phyloseq package\n\nps &lt;- phyloseq(sample_data(sample_df %&gt;% \n                             column_to_rownames(var = \"amplicon_sample_id\")),\n               otu_table(count_tab, taxa_are_rows = FALSE),\n               tax_table(tax_tab))\nps # Prints a concise summary\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 6422 taxa and 271 samples ]\nsample_data() Sample Data:       [ 271 samples by 7 sample variables ]\ntax_table()   Taxonomy Table:    [ 6422 taxa by 8 taxonomic ranks ]\n\n\nA phyloseq object is a special data structure for organizing, linking, storing, and analyzing multiple related types of data from sequencing-based studies (e.g., marker gene surveys).\nHere we’re using three slots (additional slots exist for a phylogenetic tree and reference sequences). Phyloseq checks that sample and ASV labels match across the different slots. Note that the three samples (in our sample data) that did not return reads have been dropped.\n\nAccessor functions\nPhyloseq provides various helpful “accessor” functions enabling queries of the phyloseq object. We won’t demonstrate all of them here, but they are worth learning.\nFor example\n\nsample_variables(ps)\n\n[1] \"amplicon_type\"  \"instrument_run\" \"reads_out\"      \"pid\"           \n[5] \"arm\"            \"time_point\"     \"sample_type\"   \n\n\n\nrank_names(ps)\n\n[1] \"Kingdom\" \"Phylum\"  \"Class\"   \"Order\"   \"Family\"  \"Genus\"   \"Species\"\n[8] \"Label\"  \n\n\n\nsample_sums(ps) %&gt;% min()\n\n[1] 1\n\n\n\ntaxa_sums(ps) %&gt;% min()\n\n[1] 1\n\n\nUse the functions sample_data(), otu_table() and tax_table() to access (extract) the component objects.\nFor example\n\nsample_data(ps) %&gt;% \n  data.frame() %&gt;% \n  rownames_to_column(var = \"amplicon_sample_id\") %&gt;% \n  head(n = 3) %&gt;% pander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\namplicon_sample_id\namplicon_type\ninstrument_run\nreads_out\npid\n\n\n\n\nGUT0001\nstudy_sample\nrun1\n191197\npid_32\n\n\nGUT0002\nstudy_sample\nrun1\n17206\npid_14\n\n\nGUT0003\nstudy_sample\nrun1\n112769\npid_01\n\n\n\n\n\n\n\n\n\n\n\narm\ntime_point\nsample_type\n\n\n\n\ntreatment\nweek_1\ngut_biopsy\n\n\nplacebo\nweek_7\ngut_biopsy\n\n\nplacebo\nbaseline\ngut_biopsy\n\n\n\n\n\n\n\nProcessor functions\nPhyloseq also provides various helpful “processor” functions. These functions allow for pruning, subsetting, filtering, transforming, and glomming (aggregating) the data.\nFor example, we could retain ASVs we wish to keep and samples with adequate sequencing depth\n\nps_clean &lt;- ps %&gt;% \n  prune_taxa(keepers, .) %&gt;% # What is the dot? \n  prune_samples(sample_sums(.) &gt; 100, .) %&gt;% \n  filter_taxa(., function(x) sum(x &gt; 0) &gt; 0, prune = TRUE) # Removes empty ASVs\nps_clean\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 6271 taxa and 265 samples ]\nsample_data() Sample Data:       [ 265 samples by 7 sample variables ]\ntax_table()   Taxonomy Table:    [ 6271 taxa by 8 taxonomic ranks ]\n\n\nCheck out our new phyloseq object. Are the low-yield samples gone?\n\nmin(sample_sums(ps_clean))\n\n[1] 330\n\nmin(taxa_sums(ps_clean))\n\n[1] 1\n\n\nYes! Are the chloroplasts gone?\n\ntax_table(ps_clean) %&gt;% \n  data.frame() %&gt;% \n  filter(Order == \"Chloroplast\")\n\n[1] Kingdom Phylum  Class   Order   Family  Genus   Species Label  \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nYes. I miss them a little.\nWe will encounter other “processor” functions as we continue to explore our data.\nNow let’s explore our data in three ways:\n\nPlotting taxon relative abundances (the plot_bar() function)\nWithin-sample diversity (also known as alpha diversity)\nBetween-sample diversity (beta diversity; distance metrics and ordination)\n\nAnd for each way, let’s look at our zymo mocks (e.g., run1 vs run2) and our study samples (e.g., gut vs vaginal; baseline placebo vs treatment)."
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html#plot-bars",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html#plot-bars",
    "title": "16S Data Analysis Part I",
    "section": "3. Plot bars",
    "text": "3. Plot bars\nVisualizing taxon (relative) abundances\nComposition assessment using stacked bars works well for small numbers of samples and/or small numbers of taxa. With more than around 8-10(?) taxa, they get too busy to track visually (in my opinion). Nor do I advocate for binning samples to categories prior to plotting, as the meaning of such bins often seems vague (again, my opinion). Stacked bars should work well for our zymo mocks; for our study samples, I’m not so sure. But let’s try!\n\nZymo mocks\nIn theory, the Zymo mock should contain 8 bacterial genera. You can review the list of species here.\nThe following code performs three “pre-processing steps” before plotting using plot_bar() and adding some ggplot layers. We can take it step by step.\n\nps_clean %&gt;% \n  \n  # Subset to control samples\n  subset_samples(., amplicon_type == \"zymo_mock\") %&gt;% \n  \n  # Transform counts to relative abundance\n  transform_sample_counts(., function(x) x/sum(x)) %&gt;%\n  \n  # Remove ASVs present in fewer than 6 samples\n  # What happens when we drop the number below 6?\n  # Toggle the number down, rerun, and find out\n  filter_taxa(., function(x) sum(x &gt; 0) &gt;= 6, prune = TRUE) %&gt;%\n  \n  # Plot the stacked bars\n  plot_bar(fill = \"Genus\") + \n  \n  # Add some ggplot2 layers\n  facet_wrap(.~instrument_run, scales = \"free_x\") +\n  labs(title = \"Zymo mocks\")\n\n\n\n\n\n\n\n\nIn the above plotted bars, each division is an ASV and each color is a Genus. So there are two Staphylococcus ASVs, for example.\nWe observe that for the Zymo genera (those we expect to be present), things look good - the stacked bars are consistent within and between batches. However …\n\nps_clean %&gt;% \n  \n  # Subset to control samples\n  subset_samples(., amplicon_type == \"zymo_mock\") %&gt;% \n  \n  # Transform counts to relative abundance\n  transform_sample_counts(., function(x) x/sum(x)) %&gt;%\n  \n  # Subset taxa to Mycoplasma or Sneathia\n  subset_taxa(., Genus == \"Sneathia\") %&gt;% \n  \n  # Plot the stacked bars\n  plot_bar(fill = \"Species\") + \n  \n  # Add some ggplot2 layers\n  facet_wrap(.~instrument_run, scales = \"free_x\") +\n  labs(title = \"Zymo mocks\")\n\n\n\n\n\n\n\n\nWhen we look at the non-Zymo genera (those we don’t expect to be present), which are at very low abundance, we see there may be sources of contamination that vary by batch. Reflect on what this might mean for our downstream analyses.\n\n\nStudy samples\nFor each body site, let’s compare the placebo and treatment groups at baseline (before intervention)\n\nps_clean %&gt;% \n  \n  # Subset to vaginal at baseline\n  subset_samples(., sample_type == \"vaginal\" &\n                   time_point == \"baseline\") %&gt;%\n  \n  # Drop empty ASVs\n  filter_taxa(., function(x) sum(x &gt; 0) &gt; 0, prune = TRUE) %&gt;%\n  \n  # Transform counts to relative abundance\n  transform_sample_counts(., function(x) x/sum(x)) %&gt;%\n  \n  # Aggregate families\n  tax_glom(., taxrank = \"Family\") %&gt;% # What happens to NA? \n\n  # Drop sporadic families\n  filter_taxa(., function(x) sum(x &gt; 0.01) &gt; 2, prune = TRUE) %&gt;%\n  \n  # Plot the stacked bars\n  plot_bar(x = \"pid\", fill = \"Family\") + \n  \n  # Add some ggplot2 layers\n  facet_wrap(.~arm, scales = \"free_x\") + \n  # theme(legend.position = \"none\") +\n  labs(title = \"vaginal, baseline\")\n\n\n\n\n\n\n\n\n\nps_clean %&gt;% \n  \n  # Subset to gut at baseline\n  subset_samples(., sample_type == \"gut_biopsy\" &\n                   time_point == \"baseline\") %&gt;%\n  \n  # Drop empty ASVs\n  filter_taxa(., function(x) sum(x &gt; 0) &gt; 0, prune = TRUE) %&gt;%\n  \n  # Transform counts to relative abundance\n  transform_sample_counts(., function(x) x/sum(x)) %&gt;%\n  \n  # Aggregate families\n  tax_glom(., taxrank = \"Family\") %&gt;% # What happens to NA? \n\n  # Drop sporadic families\n  filter_taxa(., function(x) sum(x &gt; 0.01) &gt; 2, prune = TRUE) %&gt;%\n  \n  # Plot the stacked bars\n  plot_bar(x = \"pid\", fill = \"Family\") + \n  \n  # Add some ggplot2 layers\n  facet_wrap(.~arm, scales = \"free_x\") + \n  # theme(legend.position = \"none\") +\n  labs(title = \"gut, baseline\")\n\n\n\n\n\n\n\n\nDo the arms look fairly similar prior to intervention? So, stacked bars for the study samples are a little unsatisfying, I think. A lot of work subsetting and aggregating for not much insight. What do you think?\nBut we can pick better questions for this plot_bar() function.\nFor example, we were curious if asv1, L. iners, appeared in the gut samples; in fact, it must, given its prevalence. This would be surprising (I think?) because L. iners is thought of as a vagina-specific organism.\nSo let’s look\n\nps_clean %&gt;% \n  subset_samples(., sample_type == \"gut_biopsy\") %&gt;% \n  transform_sample_counts(., function(x) x/sum(x)) %&gt;% \n  subset_taxa(., Label == \"asv1\") %&gt;% \n  plot_bar(x = \"pid\", fill = \"instrument_run\") +\n  facet_wrap(.~time_point, scales = \"free_x\") +\n  labs(title = \"asv1 (L. iners) in gut samples\",\n       x = \"participant_id\",\n       y = \"asv1_frequency\") +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\nYes, asv1 (L. iners) is present at low frequency in many of our gut_biopsy samples.\nAll of the gut samples were sequenced in run1. Do the zymo mocks from this batch (run1), which are not expected to contain L. iners, also harbor low levels of asv1?\n\nps_clean %&gt;% \n  subset_samples(., amplicon_type == \"zymo_mock\") %&gt;% \n  transform_sample_counts(., function(x) x/sum(x)) %&gt;% \n  subset_taxa(., Label == \"asv1\") %&gt;% \n  plot_bar() +\n  facet_wrap(.~instrument_run, scales = \"free_x\") +\n  labs(title = \"asv1 (L. iners) in zymo mocks\",\n       x = \"amplicon_sample_id\",\n       y = \"asv1_frequency\")\n\n\n\n\n\n\n\n\nIndeed, they do. So what do we think is going on here with asv1 (L. iners)? Where did it come from? What other samples, if any, were included in this batch? What might we do differently next time?"
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html#alpha-diversity",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html#alpha-diversity",
    "title": "16S Data Analysis Part I",
    "section": "4. Alpha diversity",
    "text": "4. Alpha diversity\nAlpha diversity metrics are crucial in ecology and microbiome studies because they provide insights into the richness and evenness of species within a single ecosystem or community sample. This allows researchers to understand:\n\nSpecies Richness: How many different types of species exist in a single community? This can be particularly important in assessing the overall biodiversity.\nSpecies Evenness: How evenly these species are distributed within the community. In other words, whether a few species dominate the community, or whether many species coexist in relatively similar numbers.\n\nThe data can be used to compare different communities or to see how a community changes over time in response to various factors such as perturbation, disease, or interventions.\nIn general, various alpha diversity metrics differ in whether, and to what degree, they emphasize richness or evenness.\nThe phyloseq package provides an all-in-one function plot_richness() that both calculates and plots an array of alpha diversity measures. But to be honest, I don’t often use it, opting instead to calculate the metrics using the estimate_richness() function. This places them in a dataframe, to which we can add our sample data and create plots using ggplot2.\nLook at the documentation by calling ?estimate_richness at the console prompt. Which alpha diversity measures are available to us within the function estimate_richness()?\nNote: Some estimates of alpha diversity (e.g., Chao1, ACE) must be calculated on unfiltered data. Specifically, singletons should be present. (That is, ASVs that appear only once (count of 1) within samples should be present.) This is because singletons are considered as a term in these estimates. Does our data contain singletons?\n\nsum(otu_table(ps_clean) == 1)\n\n[1] 42\n\n\nYes, but not many. (Think about how dada2 works and why this might be.) We won’t use Chao1 or ACE here, but it’s still a good idea to calculate whatever alpha diversity measure you choose using relatively unfiltered data.\nLet’s calculate alpha diversity using three different measures. The result will return as a dataframe. Then let’s join our alpha diversity data to our sample data.\n\nmeasures &lt;- c(\"Observed\", \"Shannon\", \"Simpson\")\n\nalpha_df &lt;- ps_clean %&gt;% \n  estimate_richness(measures = measures) %&gt;% \n  rownames_to_column(var = \"amplicon_sample_id\") %&gt;% \n  left_join(sample_df, by = \"amplicon_sample_id\")\n\nalpha_df %&gt;% \n  head() %&gt;% pander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\namplicon_sample_id\nObserved\nShannon\nSimpson\namplicon_type\n\n\n\n\nGUT0001\n126\n3.286\n0.9296\nstudy_sample\n\n\nGUT0002\n112\n3.454\n0.9274\nstudy_sample\n\n\nGUT0003\n117\n3.277\n0.9403\nstudy_sample\n\n\nGUT0004\n105\n3.163\n0.9113\nstudy_sample\n\n\nGUT0005\n122\n3.265\n0.9067\nstudy_sample\n\n\nGUT0006\n239\n4.333\n0.9764\nstudy_sample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninstrument_run\nreads_out\npid\narm\ntime_point\nsample_type\n\n\n\n\nrun1\n191197\npid_32\ntreatment\nweek_1\ngut_biopsy\n\n\nrun1\n17206\npid_14\nplacebo\nweek_7\ngut_biopsy\n\n\nrun1\n112769\npid_01\nplacebo\nbaseline\ngut_biopsy\n\n\nrun1\n25242\npid_06\nplacebo\nbaseline\ngut_biopsy\n\n\nrun1\n50533\npid_09\ntreatment\nweek_1\ngut_biopsy\n\n\nrun1\n142644\npid_18\ntreatment\nweek_7\ngut_biopsy\n\n\n\n\n\nNow let’s make some plots comparing alpha diversity across samples of interest.\n\nZymo mocks\nIn the absence of any batch effect, we would expect no difference in alpha diversity between mocks from run1 vs run2\n\nset.seed(123) # For reproducibility\n\nalpha_df %&gt;% \n  filter(amplicon_type == \"zymo_mock\") %&gt;% \n  pivot_longer(cols = all_of(measures),\n               names_to = \"adiv_measure\",\n               values_to = \"adiv_value\") %&gt;% \n  ggplot(aes(x = instrument_run, y = adiv_value)) +\n  geom_boxplot(alpha = 0.5, outlier.shape = NA) +\n  geom_jitter(width = 0.2) +\n  facet_wrap(.~adiv_measure, scales = \"free_y\")\n\n\n\n\n\n\n\n\nHowever, they are different, suggesting a batch effect. There seem to be “extra” taxa in mocks from run1 (keeping in mind that the Zymo mock contains eight bacterial species). We could discuss and test whether rarefying the data or aggregating taxa change this result.\nNote: The traditional Simpson diversity index represents the probability that two individuals randomly selected from a sample will belong to the same species. Higher values indicate lower diversity. Therefore, it is common to transform Simpson to 1 - Simpson. How would you modify the last chunk to plot 1 - Simpson in place of Simpson?\n\n\nStudy samples\nIf our allocation of participants to study arms was random, we might expect no difference in alpha diversity between the placebo and treatment groups at baseline (before the study intervention). Is this true?\nFirst we plot\n\nlibrary(ggbeeswarm) # For function geom_quasirandom()\n\n\nalpha_df %&gt;% \n  filter(time_point == \"baseline\") %&gt;% \n  pivot_longer(cols = all_of(measures),\n               names_to = \"adiv_measure\",\n               values_to = \"adiv_value\") %&gt;% \n  ggplot(aes(x = arm, y = adiv_value)) +\n  geom_boxplot(alpha = 0.4, outlier.shape = NA) +\n  geom_quasirandom(stroke = FALSE, width = 0.3) +\n  facet_wrap(sample_type~adiv_measure, scales = \"free_y\") +\n  labs(title = \"baseline (pre-intervention)\")\n\n\n\n\n\n\n\n\nWe notice some small differences. If these are of interest or concern to us, we can follow up. For example, for the baseline vaginal Shannon diversity index, we might ask whether the difference between groups (arms) was significant?\nLet’s use a Wilcoxon test using the function wilcox_test() from the package rstatix. (It’s the same as base R’s wilcox.test().)\n\nalpha_df %&gt;% \n  filter(time_point == \"baseline\",\n         sample_type == \"vaginal\") %&gt;% \n  wilcox_test(Shannon ~ arm, paired = FALSE) %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\n\n\n.y.\ngroup1\ngroup2\nn1\nn2\nstatistic\np\n\n\n\n\nShannon\nplacebo\ntreatment\n23\n20\n287\n0.171\n\n\n\n\n\nAt baseline, prior to intervention, there was no significant difference between the two groups (arms) in terms of vaginal bacterial diversity, as measured using the Shannon diversity index (Wilcoxon rank sum test; P = 0.171).\nIn the chunk above, change the time_point to “week_1” (that’s after intervention) and re-run the code. What do you notice? How about for “week_7”?\nLet’s visualize the trends. We’ll use the function stat_compare_means() from the package ggpubr to compare between arms at each time point using the same type of test we ran above (a Wilcoxon rank sum test).\n\nlibrary(ggpubr)\n\nLet’s start with the vaginal data\n\nalpha_df %&gt;% \n  filter(sample_type == \"vaginal\") %&gt;%  \n  ggplot(aes(x = arm, y = Shannon)) +\n  geom_boxplot(alpha = 0.4, outlier.shape = NA) +\n  geom_quasirandom(stroke = FALSE, width = 0.2) +\n  facet_wrap(.~time_point) +\n  stat_compare_means(method = \"wilcox.test\", paired = FALSE) + \n  labs(title = \"Vaginal samples\",\n       x = \"Study arm\",\n       y = \"Shannon diversity index\")\n\n\n\n\n\n\n\n\nHow would you describe these trends?\nWhat do we notice if we modify the chunk above to perform the same analysis for gut samples? Or for different measures of alpha diversity?\nFinally, there’s an alternative way to view these data, that is, viewing it as longitudinal data. In this view, participants are compared to themselves prior to intervention.\nLet’s visualize\n\nalpha_df %&gt;% \n  drop_na() %&gt;% \n  ggplot(aes(x = time_point, y = Shannon)) +\n  geom_line(aes(group = pid), alpha = 0.5) +\n  geom_point(stroke = FALSE) +\n  facet_grid(sample_type ~ arm, scales = \"free\") +\n  labs(x = \"Timepoint\",\n       y = \"Shannon diversity index\")\n\n\n\n\n\n\n\n\nIndividuals are dynamic!\nAs a follow-up, we might be interested in comparing week_1 to baseline within arms using a paired test so that participants are compared to themselves.\nHere’s an example\n\n# Get and arrange the data\ntest_df &lt;- alpha_df %&gt;% \n  filter(arm == \"treatment\",\n         time_point != \"week_7\",\n         sample_type == \"vaginal\") %&gt;% \n  group_by(pid) %&gt;% filter(n() == 2) %&gt;% ungroup() %&gt;% # Retain complete cases \n  arrange(pid, time_point) # Important step if paired tests will be performed\n\n\n# Display summary statistics\ntest_df %&gt;% \n  group_by(time_point) %&gt;% \n  get_summary_stats(Shannon, type = \"five_number\") %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntime_point\nvariable\nn\nmin\nmax\nq1\nmedian\nq3\n\n\n\n\nbaseline\nShannon\n18\n0.169\n3.438\n1.038\n2.033\n2.859\n\n\nweek_1\nShannon\n18\n0.148\n2.893\n0.301\n0.875\n2.272\n\n\n\n\n\n\n# Do a paired test using base R\nwilcox.test(x = test_df$Shannon[test_df$time_point == \"baseline\"],\n            y = test_df$Shannon[test_df$time_point == \"week_1\"],\n            paired = TRUE, alternative = \"two.sided\")\n\n\n    Wilcoxon signed rank exact test\n\ndata:  test_df$Shannon[test_df$time_point == \"baseline\"] and test_df$Shannon[test_df$time_point == \"week_1\"]\nV = 125, p-value = 0.08977\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n# But of course there's also the friendlier way\ntest_df %&gt;%\n  wilcox_test(Shannon ~ time_point, paired = TRUE) %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\n\n\n.y.\ngroup1\ngroup2\nn1\nn2\nstatistic\np\n\n\n\n\nShannon\nbaseline\nweek_1\n18\n18\n125\n0.0898\n\n\n\n\n\nHow would you express this result? If you modified the chunks above to examine the placebo arm (or week_7, or gut samples), what do you find?"
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html#beta-diversity",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html#beta-diversity",
    "title": "16S Data Analysis Part I",
    "section": "5. Beta diversity",
    "text": "5. Beta diversity\nVariation (sometimes called turnover) in species composition over space and time; in relation to any variable of interest such as perturbation, disease, or intervention\nBeta diversity refers to between-sample diversity. At its core are measures of how much diversity is shared (or not shared) between communities. These are measures of pairwise resemblance; referred to as similarity, dissimilarity, or distance metrics. These measures differ in how they are calculated; whether and to what degree they consider abundance information; and whether they treat taxa as independent or (phylogenetically) related units. Different measures emphasize different aspects of the data and therefore may give different results. Insight may be gained from these differing results.\nWhen we calculate distances (i.e., between-sample diversity) among many, many pairs of samples, we typically can’t ascertain directly (by plotting or by eye) the main patterns or structures in the data. To explore these patterns or structures, we employ unsupervised methods that reduce this “high-dimensionality”. These methods include unconstrained ordination and clustering. They are considered unsupervised because the goal is not to predict or explain the importance of one particular variable, but to visualize and understand the primary sources of variation within the data, whatever they might be. In an ordination plot, similar samples are placed closer together, and dissimilar samples are placed further away.\nThe phyloseq package provides functions for calculating measures of pairwise resemblance and performing ordination. Core functions include distance(), ordinate() and plot_ordination(). Please see the documentation for the function vegdist() from the vegan package for details on many dissimilarity measures.\nSteps in a typical exploratory beta-diversity workflow might include:\n\nSubet samples and/or filter taxa\nTransform count data (if necessary or desired)\nCalculate distances\nOrdinate\nPlot\nRepeat using alternative distance measures or ordination methods\n\n\nZymo mocks\nWe can demo this very quickly using our Zymo mocks.\nIf batch effects were negligible, then we’d expect little separation by batch. In other words, we do not expect strong clustering by batch.\nFirst we’ll use the Jaccard dissimilarity measure, which is a binary measure, meaning it considers only presence-absence. And as an ordination method we’ll use principal coordinate analysis (PCoA; also called multidimensional scaling, or MDS).\n\nps_clean %&gt;% \n  subset_samples(., amplicon_type == \"zymo_mock\") %&gt;%\n  filter_taxa(., function(x) sum(x &gt; 0) &gt; 0, prune = TRUE) %&gt;%\n  ordinate(., distance = \"jaccard\", binary = TRUE, method = \"MDS\") %&gt;% \n  plot_ordination(ps_clean, ., type = \"samples\", color = \"instrument_run\")\n\n\n\n\n\n\n\n\nAgain, we see evidence of a batch effect. At this point, we’re not surprised - we’ve seen evidence of this in our bar plots and in our alpha diversity plots.\nWhat happens to the plot above if we change the distance metric to Bray-Curtis (distance = \"bray\"), a quantitative measure, meaning that it takes abundance information into account? (Don’t forget to remove or set to false the binary argument.) Does the pattern change? Does our conclusion change?\nYou may have noticed several distance metrics listed under the function vegan::vegdist() that aren’t available in phyloseq. One of them is the robust Aitchison distance. This metric is a popular choice among microbiome researchers because of the way it handles the compositionality of our data using a centered log-ratio (CLR) transformation. This is especially important when we have relatively few taxa, such as after agglomeration (aggregation) at high taxonomic level (e.g., Phylum or Class). Here’s an example using this distance metric.\n\nlibrary(vegan)\n\n\n# Get the count table (what vegan calls the \"community data matrix\")\nzym_tab &lt;- ps_clean %&gt;% \n  subset_samples(., amplicon_type == \"zymo_mock\") %&gt;%\n  filter_taxa(., function(x) sum(x &gt; 0) &gt; 0, prune = TRUE) %&gt;%\n  otu_table() \n\n# Use vegan's vegdist() to calculate a robust Aitchison distance matrix\nait_dist_zym &lt;- vegdist(x = zym_tab, method = \"robust.aitchison\")\n\n# Input this distance matrix into phyloseq's ordinate() and plot\nps_clean %&gt;%\n  ordinate(., distance = ait_dist_zym, method = \"MDS\") %&gt;% \n  plot_ordination(ps_clean, ., type = \"samples\", color = \"instrument_run\")\n\n\n\n\n\n\n\n\nLooks … familiar.\nNote that plot_ordination() includes an argument justDF that outputs the data underlying the plot rather than the plot itself. This can be helpful if you’d prefer to customize your plot using a different package.\n\n# Highlight `justDF` argument\nps_clean %&gt;%\n  ordinate(., distance = ait_dist_zym, method = \"MDS\") %&gt;% \n  plot_ordination(ps_clean, ., axes = c(1:3), justDF = TRUE) %&gt;% \n  select(Axis.1:reads_out) %&gt;% head(n = 3) %&gt;% pander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\n \nAxis.1\nAxis.2\nAxis.3\namplicon_type\n\n\n\n\nZYMO.DNA.CTRL.P1.run1\n-7.108\n-0.9808\n-0.5162\nzymo_mock\n\n\nZYMO.DNA.CTRL.P2.run1\n-7.97\n-4.122\n0.02995\nzymo_mock\n\n\nZYMO.DNA.CTRL.P3.run1\n-4.194\n-4.718\n0.2392\nzymo_mock\n\n\n\n\n\n\n\n\n\n\n\n \ninstrument_run\nreads_out\n\n\n\n\nZYMO.DNA.CTRL.P1.run1\nrun1\n147436\n\n\nZYMO.DNA.CTRL.P2.run1\nrun1\n144714\n\n\nZYMO.DNA.CTRL.P3.run1\nrun1\n114688\n\n\n\n\n\n\n\nStudy samples\nThe next three plots should give a taste of how clustering patterns may change given, e.g., the choice of distance metric. The first is binary Jaccard, the second is Bray-Curtis, and the third is robust Aitchison.\n\nps_clean %&gt;% \n  subset_samples(., !is.na(sample_type)) %&gt;%\n  filter_taxa(., function(x) sum(x &gt; 0) &gt; 0, prune = TRUE) %&gt;%\n  ordinate(., distance = \"jaccard\", binary = TRUE, method = \"MDS\") %&gt;% \n  plot_ordination(ps_clean, ., type = \"samples\", color = \"sample_type\") +\n  labs(title = \"binary Jaccard\")\n\n\n\n\n\n\n\n\n\nps_clean %&gt;% \n  subset_samples(., !is.na(sample_type)) %&gt;%\n  filter_taxa(., function(x) sum(x &gt; 0) &gt; 0, prune = TRUE) %&gt;%\n  ordinate(., distance = \"bray\", binary = FALSE, method = \"MDS\") %&gt;% \n  plot_ordination(ps_clean, ., type = \"samples\", color = \"sample_type\") +\n  labs(title = \"Bray-Curtis\")\n\n\n\n\n\n\n\n\n\nsam_tab &lt;- ps_clean %&gt;% \n  subset_samples(., !is.na(sample_type)) %&gt;%\n  filter_taxa(., function(x) sum(x &gt; 0) &gt; 0, prune = TRUE) %&gt;%\n  otu_table() \n\nait_dist_sam &lt;- vegdist(x = sam_tab, method = \"robust.aitchison\")\n\nps_clean %&gt;%\n  ordinate(., distance = ait_dist_sam, method = \"MDS\") %&gt;% \n  plot_ordination(ps_clean, ., type = \"samples\", color = \"sample_type\") +\n  labs(title = \"robust Aitchison\")\n\n\n\n\n\n\n\n\nWe see some different shapes. Nonetheless, all plots suggest that the primary source of variation in our data is by body site (gut versus vaginal). However, we can’t rule out that what we see here is driven by a batch effect. So to be sure, we’d need to address that issue – by correcting for the batch effect or by regenerating the data.\nTry re-running the three plots with different taxa filtering or subsetting. Or using different distance measures. Or different ordination methods (e.g., NMDS)."
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html#lets-practice",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html#lets-practice",
    "title": "16S Data Analysis Part I",
    "section": "6. Let’s practice!",
    "text": "6. Let’s practice!\nLet’s practice what we’ve learned using some new data. I’ve provided two “practice” phyloseq objects, both based on published data.\nOne is gut (stool) microbiome data from mildly lactose-intolerant adults sampled over time as they eliminated and then reintroduced milk into their diet (dairy.ps).\nThe other is vaginal microbiome data from women sampled soon before and soon after giving birth, some via vaginal delivery and others via c-section (delivery.ps).\n\nManage paths\nShow paths to the data we intend to analyze\n\nfs::dir_ls(here(\"materials/2-workshop2/4-16s-pt1/data\"))\n\n/Users/jelsherbini/dev/durban-data-science-for-biology/materials/2-workshop2/4-16s-pt1/data/instructional\n/Users/jelsherbini/dev/durban-data-science-for-biology/materials/2-workshop2/4-16s-pt1/data/practice\n\n\nCreate an object containing the path to the practice data\n\npt_path &lt;- file.path(here(\"materials/2-workshop2/4-16s-pt1/data/practice/\"))\npt_path\n\n[1] \"/Users/jelsherbini/dev/durban-data-science-for-biology/materials/2-workshop2/4-16s-pt1/data/practice/\"\n\n\nList the files we intend to load\n\nlist.files(pt_path)\n\n[1] \"dairy_phyloseq.rds\"    \"delivery_phyloseq.rds\"\n\n\n\n\nLoad data\nLet’s load the practice data\n\nps_dairy &lt;- readRDS(paste0(pt_path, \"dairy_phyloseq.rds\"))\nps_deliv &lt;- readRDS(paste0(pt_path, \"delivery_phyloseq.rds\"))\n\n\n\nDairy dataset\nProduce a concise summary of ps_dairy\n\nps_dairy\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 3001 taxa and 434 samples ]\nsample_data() Sample Data:       [ 434 samples by 4 sample variables ]\ntax_table()   Taxonomy Table:    [ 3001 taxa by 6 taxonomic ranks ]\n\n\nAccess the sample variables for ps_dairy\n\nsample_variables(ps_dairy)\n\n[1] \"participant_id\" \"previous_diet\"  \"study_day\"      \"study_phase\"   \n\n\nReview the study design by making a plot; use the variables participant_id, study_day, and study_phase\n\nps_dairy %&gt;% \n  sample_data() %&gt;% \n  ggplot(aes(x = study_day, y = participant_id, color = study_phase)) +\n  geom_point() +\n  labs(title = \"dairy study; stool samples\")\n\n\n\n\n\n\n\n\nQuestion: What is the primary source of variation among the samples in ps_dairy? Is it participant_id or study_phase? Hint: Begin with binary Jaccard\n\nps_dairy %&gt;% \n  ordinate(., distance = \"jaccard\", binary = TRUE, method = \"MDS\") %&gt;% \n  plot_ordination(ps_dairy, ., type = \"samples\", color = \"participant_id\") +\n  labs(title = \"binary Jaccard\")\n\n\n\n\n\n\n\nps_dairy %&gt;% \n  ordinate(., distance = \"jaccard\", binary = TRUE, method = \"MDS\") %&gt;% \n  plot_ordination(ps_dairy, ., type = \"samples\", color = \"study_phase\") +\n  labs(title = \"binary Jaccard\")\n\n\n\n\n\n\n\n\nYour answer?\n\n\nDelivery dataset\nProduce a concise summary of ps_deliv\n\nps_deliv\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1551 taxa and 140 samples ]\nsample_data() Sample Data:       [ 140 samples by 7 sample variables ]\ntax_table()   Taxonomy Table:    [ 1551 taxa by 12 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 1551 tips and 1550 internal nodes ]\n\n\nAccess the sample variables for ps_deliv\n\nsample_variables(ps_deliv)\n\n[1] \"pregnancy_id\"       \"day_vs_delivery\"    \"status\"            \n[4] \"delivery_mode\"      \"sp_lacto_crispatus\" \"sp_lacto_gasseri\"  \n[7] \"sp_lacto_iners\"    \n\n\nReview the study design by making a plot; use the variables pregnancy_id, day_vs_delivery, and status; facet by delivery_mode\n\nps_deliv %&gt;% \n  sample_data() %&gt;%\n  ggplot(aes(x = day_vs_delivery, y = pregnancy_id, color = fct_rev(status))) +\n  geom_vline(xintercept = 0, linetype = 2, linewidth = 0.3) +\n  geom_point() +\n  facet_wrap(~delivery_mode, scales = \"free_y\") +\n  labs(title = \"delivery study; vaginal samples\",\n       color = \"status\")\n\n\n\n\n\n\n\n\nQuestion: Does the Shannon diversity index change with delivery? Does this depend on delivery mode?\n\n# Calculate Shannon\nshan_del &lt;- ps_deliv %&gt;% \n  estimate_richness(measures = \"Shannon\") %&gt;% \n  rownames_to_column(var = \"sample_id\")\n \n# Access the sample data and attach Shannon\nsam_shan_del &lt;- ps_deliv %&gt;% \n  sample_data() %&gt;% data.frame() %&gt;% \n  rownames_to_column(var = \"sample_id\") %&gt;% \n  left_join(shan_del, by = \"sample_id\") %&gt;%\n  group_by(pregnancy_id) %&gt;% filter(n() == 2) %&gt;% ungroup() %&gt;% \n  arrange(pregnancy_id, status) \n  \n# Plot\nsam_shan_del %&gt;% \n  ggplot(aes(x = fct_rev(status), y = Shannon)) +\n  geom_boxplot(alpha = 0.4, outlier.shape = NA) +\n  geom_quasirandom(stroke = FALSE, width = 0.3) +\n  facet_wrap(~delivery_mode) +\n  stat_compare_means(method = \"wilcox.test\", paired = TRUE) + \n  labs(x = \"Pregnancy status\",\n       y = \"Shannon diversity index\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nYour answer?\nBonus question: Try ordinating the samples in ps_deliv using Bray-Curtis distances. Color by the relative abundance of Lactobacillus crispatus, which I’ve included as a variable within the sample data (sp_lacto_crispatus), and also facet by status.\nNote that dominant taxa are strong drivers of clustering among vaginal samples. Can we “turn down the volume” on these dominant taxa so we can see more of what’s going on here? Try applying a square-root transformation before the ordination step.\n\nps_deliv %&gt;% \n  transform_sample_counts(., function(x) x^(1/2)) %&gt;%\n  ordinate(., distance = \"bray\", binary = FALSE, method = \"MDS\") %&gt;% \n  plot_ordination(ps_deliv, ., type = \"samples\", color = \"sp_lacto_crispatus\") +\n  facet_wrap(~fct_rev(status))\n\n\n\n\n\n\n\n\nJust as we transferred our alpha diversity measures to a dataframe, one can also transfer any number of taxon relative abundances to a dataframe as well.\nThat’s it for now. Nice work!! 🤓"
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html#reproducibility-receipt",
    "href": "materials/2-workshop2/4-16s-pt1/16S_part1_code.html#reproducibility-receipt",
    "title": "16S Data Analysis Part I",
    "section": "7. Reproducibility receipt",
    "text": "7. Reproducibility receipt\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Africa/Johannesburg\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] vegan_2.6-4      lattice_0.22-6   permute_0.9-7    ggpubr_0.6.0    \n [5] ggbeeswarm_0.7.2 here_1.0.1       pander_0.6.5     rstatix_0.7.2   \n [9] magrittr_2.0.3   lubridate_1.9.3  forcats_1.0.0    stringr_1.5.1   \n[13] dplyr_1.1.4      purrr_1.0.2      readr_2.1.5      tidyr_1.3.1     \n[17] tibble_3.2.1     ggplot2_3.5.1    tidyverse_2.0.0  phyloseq_1.46.0 \n\nloaded via a namespace (and not attached):\n [1] ade4_1.7-22             tidyselect_1.2.1        vipor_0.4.7            \n [4] farver_2.1.1            Biostrings_2.70.3       bitops_1.0-7           \n [7] fastmap_1.1.1           RCurl_1.98-1.14         digest_0.6.35          \n[10] timechange_0.3.0        lifecycle_1.0.4         cluster_2.1.6          \n[13] survival_3.5-8          compiler_4.3.3          rlang_1.1.5            \n[16] tools_4.3.3             igraph_2.0.3            utf8_1.2.4             \n[19] yaml_2.3.8              data.table_1.15.4       ggsignif_0.6.4         \n[22] knitr_1.49              labeling_0.4.3          htmlwidgets_1.6.4      \n[25] plyr_1.8.9              abind_1.4-5             withr_3.0.0            \n[28] BiocGenerics_0.48.1     grid_4.3.3              stats4_4.3.3           \n[31] fansi_1.0.6             multtest_2.58.0         biomformat_1.30.0      \n[34] colorspace_2.1-1        Rhdf5lib_1.24.2         scales_1.3.0           \n[37] iterators_1.0.14        MASS_7.3-60.0.1         cli_3.6.3              \n[40] rmarkdown_2.29          crayon_1.5.2            generics_0.1.3         \n[43] rstudioapi_0.16.0       reshape2_1.4.4          tzdb_0.4.0             \n[46] ape_5.8                 rhdf5_2.46.1            zlibbioc_1.48.2        \n[49] splines_4.3.3           parallel_4.3.3          XVector_0.42.0         \n[52] vctrs_0.6.5             Matrix_1.6-5            carData_3.0-5          \n[55] jsonlite_1.8.8          car_3.1-2               IRanges_2.36.0         \n[58] hms_1.1.3               S4Vectors_0.40.2        beeswarm_0.4.0         \n[61] foreach_1.5.2           glue_1.8.0              codetools_0.2-20       \n[64] stringi_1.8.3           gtable_0.3.4            GenomeInfoDb_1.38.8    \n[67] munsell_0.5.1           pillar_1.9.0            htmltools_0.5.8.1      \n[70] rhdf5filters_1.14.1     GenomeInfoDbData_1.2.11 R6_2.5.1               \n[73] rprojroot_2.0.4         evaluate_0.23           Biobase_2.62.0         \n[76] backports_1.4.1         broom_1.0.5             Rcpp_1.0.12            \n[79] nlme_3.1-164            mgcv_1.9-1              xfun_0.50              \n[82] fs_1.6.5                pkgconfig_2.0.3"
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/index.html",
    "href": "materials/2-workshop2/4-16s-pt1/index.html",
    "title": "16S Data Analysis Part I",
    "section": "",
    "text": "Make slides full screen\n\n\n\nWe’ve inferred and classified amplicon sequence variants (ASVs) using the dada2 package.\nNow let’s do some exploratory data analysis (EDA) using the phyloseq package and other tools.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 4</b>: Exploring High Dimensional Data"
    ]
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/index.html#slides",
    "href": "materials/2-workshop2/4-16s-pt1/index.html#slides",
    "title": "16S Data Analysis Part I",
    "section": "",
    "text": "Make slides full screen\n\n\n\nWe’ve inferred and classified amplicon sequence variants (ASVs) using the dada2 package.\nNow let’s do some exploratory data analysis (EDA) using the phyloseq package and other tools.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 4</b>: Exploring High Dimensional Data"
    ]
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/index.html#eda-mindset",
    "href": "materials/2-workshop2/4-16s-pt1/index.html#eda-mindset",
    "title": "16S Data Analysis Part I",
    "section": "EDA mindset",
    "text": "EDA mindset\nExploratory mindset:\n\n“I’m very curious about my data”\n“It’s unlikely that my data are perfect”\n“I’d like to familiarize myself with the data”",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 4</b>: Exploring High Dimensional Data"
    ]
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/index.html#get-set-up",
    "href": "materials/2-workshop2/4-16s-pt1/index.html#get-set-up",
    "title": "16S Data Analysis Part I",
    "section": "0. Get set up",
    "text": "0. Get set up\n\nLoad packages\nLoad some packages including the phyloseq package\n\nlibrary(phyloseq)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(rstatix)\nlibrary(pander)\nlibrary(here)\n\nTo get Help with any function, type a question mark (?) in front of the function name at the Console. For example, type ?dplyr::filter (or ?filter) and hit enter to get Help with the dplyr function filter().\n\n\nManage file paths\nShow path to the current project\n\nhere::here()\n\n[1] \"/Users/jelsherbini/dev/durban-data-science-for-biology\"\n\n\nShow paths to the data we intend to analyze\n\nfs::dir_ls(here(\"materials/2-workshop2/4-16s-pt1/data\"))\n\n/Users/jelsherbini/dev/durban-data-science-for-biology/materials/2-workshop2/4-16s-pt1/data/instructional\n/Users/jelsherbini/dev/durban-data-science-for-biology/materials/2-workshop2/4-16s-pt1/data/practice\n\n\nCreate an object containing the path to the instructional data\n\nin_path &lt;- file.path(here(\"materials/2-workshop2/4-16s-pt1/data/instructional/\"))\nin_path\n\n[1] \"/Users/jelsherbini/dev/durban-data-science-for-biology/materials/2-workshop2/4-16s-pt1/data/instructional/\"\n\n\nJust for fun, let’s be curious about this object in_path. What type of object is it? How many strings does it contain? How many characters in this string?\n\nclass(in_path)\n\n[1] \"character\"\n\nlength(in_path)\n\n[1] 1\n\nnchar(in_path)\n\n[1] 106\n\n\nWe can answer these questions. It’s a character vector containing 1 string consisting of 106 characters.\nWe’ll use this object to construct commands for loading our data.\n\n\nLoad instructional data\nList the files we intend to load\n\nlist.files(in_path)\n\n[1] \"count_table_instructional.rds\" \"sample_data_instructional.rds\"\n[3] \"tax_table_instructional.rds\"  \n\n\nThe count_table and tax_table are the outputs of dada2. The sample_data is a version of the file 05_amplicon_sample_ids_UKZN_workshop_2023.csv that I’ve modified for use here.\nNote: An .rds file contains a single R object which has been saved to a file. The file can be loaded back into the R environment using the readRDS() function.\nLet’s load the instructional data\n\nsample_df &lt;- readRDS(paste0(in_path, \"sample_data_instructional.rds\"))\ncount_tab &lt;- readRDS(paste0(in_path, \"count_table_instructional.rds\"))\ntax_tab &lt;- readRDS(paste0(in_path, \"tax_table_instructional.rds\"))\n\nNote: The paste0() function is a variation of paste() that concatenates strings without any separator.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 4</b>: Exploring High Dimensional Data"
    ]
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/index.html#get-to-know-the-data",
    "href": "materials/2-workshop2/4-16s-pt1/index.html#get-to-know-the-data",
    "title": "16S Data Analysis Part I",
    "section": "1. Get to know the data",
    "text": "1. Get to know the data\nFamiliarize yourself with the data:\n\nThink about how you would describe or summarize it\nExamine the objects for form, size, and content\nReview and visualize your study design\nIdentify any missing data\n\n\nSample data\nLet’s begin with the sample data.\nGlimpse the object using head()\n\nsample_df %&gt;% \n  head(n = 3) %&gt;% pander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\namplicon_sample_id\namplicon_type\ninstrument_run\nreads_out\npid\n\n\n\n\nGUT0003\nstudy_sample\nrun1\n112769\npid_01\n\n\nVAG0142\nstudy_sample\nrun2\n73426\npid_01\n\n\nGUT0050\nstudy_sample\nrun1\n20211\npid_01\n\n\n\n\n\n\n\n\n\n\n\narm\ntime_point\nsample_type\n\n\n\n\nplacebo\nbaseline\ngut_biopsy\n\n\nplacebo\nbaseline\nvaginal\n\n\nplacebo\nweek_1\ngut_biopsy\n\n\n\n\n\nGlimpse the object using tail()\n\nsample_df %&gt;% \n  tail(n = 3) %&gt;% pander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\n \namplicon_sample_id\namplicon_type\ninstrument_run\nreads_out\n\n\n\n\n272\nZYMO.DNA.CTRL.P6.run2\nzymo_mock\nrun2\n41915\n\n\n273\nZYMO.DNA.CTRL.P7.run2\nzymo_mock\nrun2\n48564\n\n\n274\nZYMO.DNA.CTRL.P8.run2\nzymo_mock\nrun2\n38318\n\n\n\n\n\n\n\n\n\n\n\n\n\n \npid\narm\ntime_point\nsample_type\n\n\n\n\n272\nNA\nNA\nNA\nNA\n\n\n273\nNA\nNA\nNA\nNA\n\n\n274\nNA\nNA\nNA\nNA\n\n\n\n\n\nDoes this object contain any missing data? How much?\n\nsum(is.na(sample_df))\n\n[1] 40\n\n\nYes it does.\nThis object is a data.frame with 274 rows (observations) and 8 columns (variables). We acknowledge the presence of NA values (missing data).\nNote: I’ve added a variable sample_df$reads_out containing the total number of reads output by dada2. This variable can be taken from the portion of the dada2 workflow in which reads are tracked through the pipeline.\nIn our sample data, we see variables describing the amplicons. How many amplicons did we sequence on each run?\n\nsample_df %&gt;% \n  count(instrument_run, amplicon_type) %&gt;% pander()\n\n\n\n\n\n\n\n\n\ninstrument_run\namplicon_type\nn\n\n\n\n\nrun1\nstudy_sample\n132\n\n\nrun1\nzymo_mock\n5\n\n\nrun2\nstudy_sample\n132\n\n\nrun2\nzymo_mock\n5\n\n\n\n\n\nOn each instrument run, we sequenced n = 132 study samples and n = 5 mock communities (positive controls).\nHere, the mock communities are the ZymoBIOMICS Microbial Community DNA Standard. Later, we’ll use these mocks to assess for batch effects (run1 vs run2).\nWe also have variables pertaining to the study design. Let’s report the number of participants in the study without counting the NA values associated with the mocks.\nHere are two options\n\nlength(unique(sample_df$pid)[!is.na(unique(sample_df$pid))]) # Option 1 using base R\n\n[1] 44\n\nsample_df %&gt;% drop_na() %$% n_distinct(pid) # Option 2 using tidyverse & magrittr\n\n[1] 44\n\n\nWhich option do you find more readable?\nNote: The %$% operator is known as the “exposition pipe operator”. It’s part of the magrittr package and is used to expose the variables in a dataframe to the environment in an expression.\nNow let’s review the study design. Often, a visualization works well for this purpose.\nPlot our study design\n\nsample_df %&gt;% \n  drop_na() %&gt;% # Without this line, what happens? Why? \n  mutate(has_reads = reads_out &gt; 0) %&gt;% \n  ggplot(aes(x = time_point, y = pid, color = has_reads)) +\n  geom_point() +\n  facet_grid(rows = vars(arm), cols = vars(sample_type), scales = \"free_y\") +\n  labs(title = \"study design\", y = \"participant_id\")\n\n\n\n\n\n\n\n\nWe recall that our study involves 44 participants, randomized to placebo or treatment, who contributed a gut and vaginal sample at baseline (before intervention) and at 1 and 7 weeks after intervention.\nWe also note that three samples failed to return any reads after processing through dada2. (What happened?)\nLet’s make a plot that looks closely at our sequencing yields\n\nsample_df %&gt;% \n  filter(reads_out &gt; 0) %&gt;% \n  mutate(sample_type = ifelse(is.na(sample_type), \"zymo\", sample_type)) %&gt;% \n  ggplot(aes(x = reads_out, fill = sample_type)) +\n  geom_histogram(bins = 50) +\n  facet_wrap(.~instrument_run) +\n  labs(title = \"sequencing yield\")\n\n\n\n\n\n\n\n\nThese histograms have very different shapes. If this is surprising or concerning to us, we might investigate why this happened. It’s also clear that we have additional samples with relatively low yield. How would you quickly modify the code above to display only those samples with fewer than 500 reads?\nWhen our work involves batches, we always want to include replicate samples (here, the zymo mocks) that allow us to assess for batch effects. Looking at the plot above, why was this extra important in the context of this study?\nFinally, we can compute some summary statistics for this variable sample_df$reads_out for each instrument run\n\nsample_df %&gt;% \n  group_by(instrument_run) %&gt;% \n  get_summary_stats(reads_out, type = \"five_number\") %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninstrument_run\nvariable\nn\nmin\nmax\nq1\nmedian\nq3\n\n\n\n\nrun1\nreads_out\n137\n0\n257270\n24926\n62910\n124354\n\n\nrun2\nreads_out\n137\n0\n96147\n45022\n57113\n69108\n\n\n\n\n\nIn the code above we’ve used the function get_summary_stats() from the package rstatix. This function allows for different types of summary statistics (we’ve used type = “five number”). We note that we have a median of around 60k reads per sample.\n\n\nCount table\nNow let’s look at the count table. This table contains the count of each amplicon sequence variant (ASV) in each sample. It will become the core of our phyloseq object.\nDescribe the count table\n\nclass(count_tab)\n\n[1] \"matrix\" \"array\" \n\nnrow(count_tab)\n\n[1] 271\n\nncol(count_tab)\n\n[1] 6422\n\nsum(is.na(count_tab))\n\n[1] 0\n\n\nThe count table is a matrix (a 2D array) with 271 rows and 6422 columns. There is no missing data. Based on these numbers, it seems that rows are samples and columns are ASVs.\nLet’s make sure. How are the rows and columns named (labelled)?\nWhat are the row names?\n\nrownames(count_tab) %&gt;% head(n = 3)\n\n[1] \"GUT0001\" \"GUT0002\" \"GUT0003\"\n\n\nIn this count table, the rows are samples. They are named using the amplicon_sample_id.\nIs every sample in the count table listed in the sample data?\n\nsample_df %&gt;% \n  filter(!amplicon_sample_id %in% rownames(count_tab)) %&gt;% pander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\namplicon_sample_id\namplicon_type\ninstrument_run\nreads_out\npid\n\n\n\n\nGUT0071\nstudy_sample\nrun1\n0\npid_08\n\n\nGUT0117\nstudy_sample\nrun1\n0\npid_22\n\n\nVAG0280\nstudy_sample\nrun2\n0\npid_40\n\n\n\n\n\n\n\n\n\n\n\narm\ntime_point\nsample_type\n\n\n\n\nplacebo\nweek_7\ngut_biopsy\n\n\ntreatment\nweek_1\ngut_biopsy\n\n\ntreatment\nweek_1\nvaginal\n\n\n\n\n# And this difficult-to-read command should evaluate to TRUE\nsum(rownames(count_tab) %in% sample_df$amplicon_sample_id) == nrow(count_tab)\n\n[1] TRUE\n\n\nYes, all of the samples in the count table are present in the sample data, except for the three samples that failed (and that’s okay, we can leave these three samples in the sample data).\nWhat are the column names?\n\ncolnames(count_tab) %&gt;% head(n = 3)\n\n[1] \"TAGGGAATCTTCCACAATGGACGCAAGTCTGATGGAGCAACGCCGCGTGAGTGAAGAAGGGTTTCGGCTCGTAAAGCTCTGTTGTTGGTGAAGAAGGACAGGGGTAGTAACTGACCTTTGTTTGACGGTAATCAATTAGAAAGTCACGGCTAACTACGTGCCAGCAGCCGCGGTAATACGTAGGTGGCAAGCGTTGTCCGGATTTATTGGGCGTAAAGCGAGTGCAGGCGGCTCGATAAGTCTGATGTGAAAGCCTTCGGCTCAACCGGAGAATTGCATCAGAAACTGTCGAGCTTGAGTACAGAAGAGGAGAGTGGAACTCCATGTGTAGCGGTGAAATGCGTAGATATATGGAAGAACACCGGTGGCGAAGGCGGCTCTCTGGTCTGTTACTGACGCTGAGGCTCGAAAGCATGGGTAGCGAACAGG\"\n[2] \"TGAGGAATATTGGTCAATGGGCGAGAGCCTGAACCAGCCAAGTAGCGTGAAGGATGACTGCCCTATGGGTTGTAAACTTCTTTTATAAAGGAATAAAGTCGGGTATGGATACCCGTTTGCATGTACTTTATGAATAAGGATCGGCTAACTCCGTGCCAGCAGCCGCGGTAATACGGAGGATCCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGAGCGTAGATGGATGTTTAAGTCAGTTGTGAAAGTTTGCGGCTCAACCGTAAAATTGCAGTTGATACTGGATATCTTGAGTGCAGTTGAGGCAGGCGGAATTCGTGGTGTAGCGGTGAAATGCTTAGATATCACGAAGAACTCCGATTGCGAAGGCAGCCTGCTAAGCTGCAACTGACATTGAGGCTCGAAAGTGTGGGTATCAAACAGG\"     \n[3] \"TGGGGAATATTGCACAATGGGCGCAAGCCTGATGCAGCCATGCCGCGTGTATGAAGAAGGCCTTCGGGTTGTAAAGTACTTTCAGCGGGGAGGAAGGGAGTAAAGTTAATACCTTTGCTCATTGACGTTACCCGCAGAAGAAGCACCGGCTAACTCCGTGCCAGCAGCCGCGGTAATACGGAGGGTGCAAGCGTTAATCGGAATTACTGGGCGTAAAGCGCACGCAGGCGGTTTGTTAAGTCAGATGTGAAATCCCCGGGCTCAACCTGGGAACTGCATCTGATACTGGCAAGCTTGAGTCTCGTAGAGGGGGGTAGAATTCCAGGTGTAGCGGTGAAATGCGTAGAGATCTGGAGGAATACCGGTGGCGAAGGCGGCCCCCTGGACGAAGACTGACGCTCAGGTGCGAAAGCGTGGGGAGCAAACAGG\"\n\n\nWow, how long are these names?\n\ncolnames(count_tab) %&gt;% head(n = 3) %&gt;% nchar()\n\n[1] 429 424 429\n\n\nIn this count table, the columns are ASVs and they are named using … the ASV sequences themselves!! Here, these labels can be &gt;400 characters long. (Discuss pros, cons, and workarounds for using ASV sequences as labels.)\nFinally, how “sparse” is our count table? In other words, what fraction of the elements are zero?\n\nsum(count_tab == 0) / length(count_tab)\n\n[1] 0.9858483\n\n\nOver 98% of the elements are zero. This count table is very sparse.\nCan we create a plot that helps us think about this?\nLet’s try\n\ndata.frame(asv_prev = colSums(count_tab &gt; 0)) %&gt;% \n  ggplot(aes(x = asv_prev)) +\n  geom_histogram(bins = 50) +\n  labs(x = \"Number of samples in which the ASV was detected\",\n       y = \"Number of ASVs\")\n\n\n\n\n\n\n\n\nThe table is sparse because most ASVs are absent from most samples. There are very few ASVs that are widespread (detected in many or most samples). How could we modify the above code to see the number of ASVs detected in many (say, &gt;50) samples? Hint: Use the filter() function.\nLet’s save this ASV prevalence data in a dataframe. We can also add the total count for the ASV.\n\nprevalence_df &lt;- data.frame(asv_sum = colSums(count_tab),\n                            asv_prev = colSums(count_tab &gt; 0)) %&gt;% \n  rownames_to_column(var = \"asv_seq\") \n\nHow prevalent is the most prevalent ASV?\n\nprevalence_df %&gt;% \n  filter(asv_prev == max(asv_prev))\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                        asv_seq\n1 TAGGGAATCTTCCACAATGGACGCAAGTCTGATGGAGCAACGCCGCGTGAGTGAAGAAGGGTTTCGGCTCGTAAAGCTCTGTTGTTGGTGAAGAAGGACAGGGGTAGTAACTGACCTTTGTTTGACGGTAATCAATTAGAAAGTCACGGCTAACTACGTGCCAGCAGCCGCGGTAATACGTAGGTGGCAAGCGTTGTCCGGATTTATTGGGCGTAAAGCGAGTGCAGGCGGCTCGATAAGTCTGATGTGAAAGCCTTCGGCTCAACCGGAGAATTGCATCAGAAACTGTCGAGCTTGAGTACAGAAGAGGAGAGTGGAACTCCATGTGTAGCGGTGAAATGCGTAGATATATGGAAGAACACCGGTGGCGAAGGCGGCTCTCTGGTCTGTTACTGACGCTGAGGCTCGAAAGCATGGGTAGCGAACAGG\n  asv_sum asv_prev\n1 2147189      228\n\n\nThis ASV was detected in 228 out of 271 samples. Huh, I wonder who this is?\n\n\nTaxonomy table\nNow let’s look at the taxonomy table. This table contains the taxonomic assignments made using dada2.\nDescribe the taxonomy table\n\nclass(tax_tab)\n\n[1] \"matrix\" \"array\" \n\nnrow(tax_tab)\n\n[1] 6422\n\nncol(tax_tab)\n\n[1] 8\n\nsum(is.na(tax_tab))\n\n[1] 7388\n\n\nThe taxonomy table is a matrix (a 2D array) with 6422 rows and 8 columns. Missing data are present (in cases where the taxonomic assignment was unknown or ambiguous). Based on these numbers, it seems that rows are ASVs. What are the columns?\n\ncolnames(tax_tab)\n\n[1] \"Kingdom\" \"Phylum\"  \"Class\"   \"Order\"   \"Family\"  \"Genus\"   \"Species\"\n[8] \"Label\"  \n\n\nThe columns are taxonomic ranks. Note that I’ve added a variable at far right called “Label” which contains a short label for each ASV (e.g., “asv1”, “asv2”, etc).\nAt this point, it’s generally a good idea to explore various features of the ASVs.\nTo do this, one can begin by creating a dataframe from the taxonomy table. I think of this dataframe as a place to store any ASV-associated information, not only taxonomic assignments.\nSo let’s do this\n\nasv_df &lt;- tax_tab %&gt;%\n  data.frame() %&gt;% \n  rownames_to_column(var = \"asv_seq\") %&gt;% \n  left_join(prevalence_df, by = \"asv_seq\") %&gt;% # Add data from above \n  mutate(asv_len = nchar(asv_seq))\n\nNow we can explore, for example, the distribution of ASV lengths in relation to ASV prevalence\n\nasv_df %&gt;% \n  ggplot(aes(x = asv_len, y = asv_prev, color = Kingdom)) +\n  geom_point(alpha = 0.5, size = 3, stroke = FALSE) +\n  labs(title = \"How prevalent are any length outliers?\",\n       x = \"ASV length\",\n       y = \"ASV prevalence\")\n\n\n\n\n\n\n\n\nWe have length outliers; they are not very prevalent. After some follow-up, we might decide to filter them out. Maybe they are problematic ASVs, e.g., chimeric or non-specific (non-SSU rRNA) amplicons.\nWe also notice three non-bacterial ASVs. Interesting! Do they make sense, given the body sites we’ve sampled?\n\nasv_df %&gt;% \n  filter(Kingdom != \"Bacteria\") %&gt;% \n  select(-asv_seq) %&gt;% \n  select(Genus:asv_len) %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\n\nGenus\nSpecies\nLabel\nasv_sum\nasv_prev\nasv_len\n\n\n\n\nMethanobrevibacter\nNA\nasv1136\n670\n4\n388\n\n\nTrichomonas\nNA\nasv1815\n219\n2\n457\n\n\nMethanosphaera\nNA\nasv4889\n8\n1\n388\n\n\n\n\n\nOur sample types are gut and vaginal. So, yes, these make sense.\nLet’s look at a few of the oddly short ASVs\n\nasv_df %&gt;% \n  filter(asv_prev &gt; 1, asv_len &lt; 280) %&gt;% \n  select(-asv_seq) %&gt;% \n  head(n = 3) %&gt;% pander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\n\nKingdom\nPhylum\nClass\nOrder\nFamily\nGenus\n\n\n\n\nBacteria\nCyanobacteria\nOxyphotobacteria\nChloroplast\nNA\nNA\n\n\nBacteria\nCyanobacteria\nOxyphotobacteria\nChloroplast\nNA\nNA\n\n\nBacteria\nCyanobacteria\nOxyphotobacteria\nChloroplast\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\nLabel\nasv_sum\nasv_prev\nasv_len\n\n\n\n\nNA\nasv2318\n95\n4\n270\n\n\nNA\nasv2338\n93\n2\n271\n\n\nNA\nasv2576\n63\n2\n271\n\n\n\n\n\nThese are chloroplast sequences. We might consider filtering out ASVs assigned to the Order Chloroplast. But we should state our reasoning. For example, “chloroplasts are not, in and of themselves, members of the gut or vaginal microbiota”.\nLet’s look at a few long ASVs\n\nasv_df %&gt;% \n  filter(asv_len &gt; 450) %&gt;% \n  select(-asv_seq) %&gt;% \n  select(Genus:asv_len) %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\n\nGenus\nSpecies\nLabel\nasv_sum\nasv_prev\nasv_len\n\n\n\n\nTrichomonas\nNA\nasv1815\n219\n2\n457\n\n\nBacteroides\nNA\nasv5826\n3\n1\n460\n\n\nAlistipes\nNA\nasv6152\n2\n1\n460\n\n\nBacteroides\nNA\nasv6203\n2\n1\n475\n\n\nUreaplasma\nNA\nasv6384\n2\n1\n498\n\n\n\n\n\nOf these five, I’d guess the Trichomonas ASV is real (and from vaginal samples) and the others are problematic. Just my hunch.\nIn general, we’d continue to explore the ASV data until we developed an intuition of which ASVs, if any, we might want to set aside. For example, here we might retain ASVs with length &gt; 385-nt and &lt; 460-nt, that aren’t Order Chloroplast (or Family Mitochondria).\nLet’s make a vector containing the ASVs we wish to keep\n\nkeepers &lt;- asv_df %&gt;% \n  filter(asv_len &gt; 385 & asv_len &lt; 460) %&gt;% \n  # Careful below! Don't drop NA unless you intend to\n  filter(Order != \"Chloroplast\" | is.na(Order)) %&gt;% \n  filter(Family != \"Mitochondria\" | is.na(Family)) %$%\n  unique(asv_seq)\n\n# Retaining these ASVs removes how many ASVs?\nnrow(asv_df) - length(keepers)\n\n[1] 146\n\n\nBtw, who was it - the most prevalent ASV in our dataset?\n\nasv_df %&gt;% \n  filter(asv_prev == max(asv_prev)) %&gt;% \n  select(Genus:asv_len) %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\n\nGenus\nSpecies\nLabel\nasv_sum\nasv_prev\nasv_len\n\n\n\n\nLactobacillus\niners\nasv1\n2147189\n228\n429\n\n\n\n\n\nInteresting, we think of L. iners as a vaginal species. Does is also appear in the gut?",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 4</b>: Exploring High Dimensional Data"
    ]
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/index.html#make-phyloseq-object",
    "href": "materials/2-workshop2/4-16s-pt1/index.html#make-phyloseq-object",
    "title": "16S Data Analysis Part I",
    "section": "2. Make phyloseq object",
    "text": "2. Make phyloseq object\nLet’s build our phyloseq object using the phyloseq() function from the phyloseq package\n\nps &lt;- phyloseq(sample_data(sample_df %&gt;% \n                             column_to_rownames(var = \"amplicon_sample_id\")),\n               otu_table(count_tab, taxa_are_rows = FALSE),\n               tax_table(tax_tab))\nps # Prints a concise summary\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 6422 taxa and 271 samples ]\nsample_data() Sample Data:       [ 271 samples by 7 sample variables ]\ntax_table()   Taxonomy Table:    [ 6422 taxa by 8 taxonomic ranks ]\n\n\nA phyloseq object is a special data structure for organizing, linking, storing, and analyzing multiple related types of data from sequencing-based studies (e.g., marker gene surveys).\nHere we’re using three slots (additional slots exist for a phylogenetic tree and reference sequences). Phyloseq checks that sample and ASV labels match across the different slots. Note that the three samples (in our sample data) that did not return reads have been dropped.\n\nAccessor functions\nPhyloseq provides various helpful “accessor” functions enabling queries of the phyloseq object. We won’t demonstrate all of them here, but they are worth learning.\nFor example\n\nsample_variables(ps)\n\n[1] \"amplicon_type\"  \"instrument_run\" \"reads_out\"      \"pid\"           \n[5] \"arm\"            \"time_point\"     \"sample_type\"   \n\n\n\nrank_names(ps)\n\n[1] \"Kingdom\" \"Phylum\"  \"Class\"   \"Order\"   \"Family\"  \"Genus\"   \"Species\"\n[8] \"Label\"  \n\n\n\nsample_sums(ps) %&gt;% min()\n\n[1] 1\n\n\n\ntaxa_sums(ps) %&gt;% min()\n\n[1] 1\n\n\nUse the functions sample_data(), otu_table() and tax_table() to access (extract) the component objects.\nFor example\n\nsample_data(ps) %&gt;% \n  data.frame() %&gt;% \n  rownames_to_column(var = \"amplicon_sample_id\") %&gt;% \n  head(n = 3) %&gt;% pander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\namplicon_sample_id\namplicon_type\ninstrument_run\nreads_out\npid\n\n\n\n\nGUT0001\nstudy_sample\nrun1\n191197\npid_32\n\n\nGUT0002\nstudy_sample\nrun1\n17206\npid_14\n\n\nGUT0003\nstudy_sample\nrun1\n112769\npid_01\n\n\n\n\n\n\n\n\n\n\n\narm\ntime_point\nsample_type\n\n\n\n\ntreatment\nweek_1\ngut_biopsy\n\n\nplacebo\nweek_7\ngut_biopsy\n\n\nplacebo\nbaseline\ngut_biopsy\n\n\n\n\n\n\n\nProcessor functions\nPhyloseq also provides various helpful “processor” functions. These functions allow for pruning, subsetting, filtering, transforming, and glomming (aggregating) the data.\nFor example, we could retain ASVs we wish to keep and samples with adequate sequencing depth\n\nps_clean &lt;- ps %&gt;% \n  prune_taxa(keepers, .) %&gt;% # What is the dot? \n  prune_samples(sample_sums(.) &gt; 100, .) %&gt;% \n  filter_taxa(., function(x) sum(x &gt; 0) &gt; 0, prune = TRUE) # Removes empty ASVs\nps_clean\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 6271 taxa and 265 samples ]\nsample_data() Sample Data:       [ 265 samples by 7 sample variables ]\ntax_table()   Taxonomy Table:    [ 6271 taxa by 8 taxonomic ranks ]\n\n\nCheck out our new phyloseq object. Are the low-yield samples gone?\n\nmin(sample_sums(ps_clean))\n\n[1] 330\n\nmin(taxa_sums(ps_clean))\n\n[1] 1\n\n\nYes! Are the chloroplasts gone?\n\ntax_table(ps_clean) %&gt;% \n  data.frame() %&gt;% \n  filter(Order == \"Chloroplast\")\n\n[1] Kingdom Phylum  Class   Order   Family  Genus   Species Label  \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nYes. I miss them a little.\nWe will encounter other “processor” functions as we continue to explore our data.\nNow let’s explore our data in three ways:\n\nPlotting taxon relative abundances (the plot_bar() function)\nWithin-sample diversity (also known as alpha diversity)\nBetween-sample diversity (beta diversity; distance metrics and ordination)\n\nAnd for each way, let’s look at our zymo mocks (e.g., run1 vs run2) and our study samples (e.g., gut vs vaginal; baseline placebo vs treatment).",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 4</b>: Exploring High Dimensional Data"
    ]
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/index.html#plot-bars",
    "href": "materials/2-workshop2/4-16s-pt1/index.html#plot-bars",
    "title": "16S Data Analysis Part I",
    "section": "3. Plot bars",
    "text": "3. Plot bars\nVisualizing taxon (relative) abundances\nComposition assessment using stacked bars works well for small numbers of samples and/or small numbers of taxa. With more than around 8-10(?) taxa, they get too busy to track visually (in my opinion). Nor do I advocate for binning samples to categories prior to plotting, as the meaning of such bins often seems vague (again, my opinion). Stacked bars should work well for our zymo mocks; for our study samples, I’m not so sure. But let’s try!\n\nZymo mocks\nIn theory, the Zymo mock should contain 8 bacterial genera. You can review the list of species here.\nThe following code performs three “pre-processing steps” before plotting using plot_bar() and adding some ggplot layers. We can take it step by step.\n\nps_clean %&gt;% \n  \n  # Subset to control samples\n  subset_samples(., amplicon_type == \"zymo_mock\") %&gt;% \n  \n  # Transform counts to relative abundance\n  transform_sample_counts(., function(x) x/sum(x)) %&gt;%\n  \n  # Remove ASVs present in fewer than 6 samples\n  # What happens when we drop the number below 6?\n  # Toggle the number down, rerun, and find out\n  filter_taxa(., function(x) sum(x &gt; 0) &gt;= 6, prune = TRUE) %&gt;%\n  \n  # Plot the stacked bars\n  plot_bar(fill = \"Genus\") + \n  \n  # Add some ggplot2 layers\n  facet_wrap(.~instrument_run, scales = \"free_x\") +\n  labs(title = \"Zymo mocks\")\n\n\n\n\n\n\n\n\nIn the above plotted bars, each division is an ASV and each color is a Genus. So there are two Staphylococcus ASVs, for example.\nWe observe that for the Zymo genera (those we expect to be present), things look good - the stacked bars are consistent within and between batches. However …\n\nps_clean %&gt;% \n  \n  # Subset to control samples\n  subset_samples(., amplicon_type == \"zymo_mock\") %&gt;% \n  \n  # Transform counts to relative abundance\n  transform_sample_counts(., function(x) x/sum(x)) %&gt;%\n  \n  # Subset taxa to Mycoplasma or Sneathia\n  subset_taxa(., Genus == \"Sneathia\") %&gt;% \n  \n  # Plot the stacked bars\n  plot_bar(fill = \"Species\") + \n  \n  # Add some ggplot2 layers\n  facet_wrap(.~instrument_run, scales = \"free_x\") +\n  labs(title = \"Zymo mocks\")\n\n\n\n\n\n\n\n\nWhen we look at the non-Zymo genera (those we don’t expect to be present), which are at very low abundance, we see there may be sources of contamination that vary by batch. Reflect on what this might mean for our downstream analyses.\n\n\nStudy samples\nFor each body site, let’s compare the placebo and treatment groups at baseline (before intervention)\n\nps_clean %&gt;% \n  \n  # Subset to vaginal at baseline\n  subset_samples(., sample_type == \"vaginal\" &\n                   time_point == \"baseline\") %&gt;%\n  \n  # Drop empty ASVs\n  filter_taxa(., function(x) sum(x &gt; 0) &gt; 0, prune = TRUE) %&gt;%\n  \n  # Transform counts to relative abundance\n  transform_sample_counts(., function(x) x/sum(x)) %&gt;%\n  \n  # Aggregate families\n  tax_glom(., taxrank = \"Family\") %&gt;% # What happens to NA? \n\n  # Drop sporadic families\n  filter_taxa(., function(x) sum(x &gt; 0.01) &gt; 2, prune = TRUE) %&gt;%\n  \n  # Plot the stacked bars\n  plot_bar(x = \"pid\", fill = \"Family\") + \n  \n  # Add some ggplot2 layers\n  facet_wrap(.~arm, scales = \"free_x\") + \n  # theme(legend.position = \"none\") +\n  labs(title = \"vaginal, baseline\")\n\n\n\n\n\n\n\n\n\nps_clean %&gt;% \n  \n  # Subset to gut at baseline\n  subset_samples(., sample_type == \"gut_biopsy\" &\n                   time_point == \"baseline\") %&gt;%\n  \n  # Drop empty ASVs\n  filter_taxa(., function(x) sum(x &gt; 0) &gt; 0, prune = TRUE) %&gt;%\n  \n  # Transform counts to relative abundance\n  transform_sample_counts(., function(x) x/sum(x)) %&gt;%\n  \n  # Aggregate families\n  tax_glom(., taxrank = \"Family\") %&gt;% # What happens to NA? \n\n  # Drop sporadic families\n  filter_taxa(., function(x) sum(x &gt; 0.01) &gt; 2, prune = TRUE) %&gt;%\n  \n  # Plot the stacked bars\n  plot_bar(x = \"pid\", fill = \"Family\") + \n  \n  # Add some ggplot2 layers\n  facet_wrap(.~arm, scales = \"free_x\") + \n  # theme(legend.position = \"none\") +\n  labs(title = \"gut, baseline\")\n\n\n\n\n\n\n\n\nDo the arms look fairly similar prior to intervention? So, stacked bars for the study samples are a little unsatisfying, I think. A lot of work subsetting and aggregating for not much insight. What do you think?\nBut we can pick better questions for this plot_bar() function.\nFor example, we were curious if asv1, L. iners, appeared in the gut samples; in fact, it must, given its prevalence. This would be surprising (I think?) because L. iners is thought of as a vagina-specific organism.\nSo let’s look\n\nps_clean %&gt;% \n  subset_samples(., sample_type == \"gut_biopsy\") %&gt;% \n  transform_sample_counts(., function(x) x/sum(x)) %&gt;% \n  subset_taxa(., Label == \"asv1\") %&gt;% \n  plot_bar(x = \"pid\", fill = \"instrument_run\") +\n  facet_wrap(.~time_point, scales = \"free_x\") +\n  labs(title = \"asv1 (L. iners) in gut samples\",\n       x = \"participant_id\",\n       y = \"asv1_frequency\") +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\nYes, asv1 (L. iners) is present at low frequency in many of our gut_biopsy samples.\nAll of the gut samples were sequenced in run1. Do the zymo mocks from this batch (run1), which are not expected to contain L. iners, also harbor low levels of asv1?\n\nps_clean %&gt;% \n  subset_samples(., amplicon_type == \"zymo_mock\") %&gt;% \n  transform_sample_counts(., function(x) x/sum(x)) %&gt;% \n  subset_taxa(., Label == \"asv1\") %&gt;% \n  plot_bar() +\n  facet_wrap(.~instrument_run, scales = \"free_x\") +\n  labs(title = \"asv1 (L. iners) in zymo mocks\",\n       x = \"amplicon_sample_id\",\n       y = \"asv1_frequency\")\n\n\n\n\n\n\n\n\nIndeed, they do. So what do we think is going on here with asv1 (L. iners)? Where did it come from? What other samples, if any, were included in this batch? What might we do differently next time?",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 4</b>: Exploring High Dimensional Data"
    ]
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/index.html#alpha-diversity",
    "href": "materials/2-workshop2/4-16s-pt1/index.html#alpha-diversity",
    "title": "16S Data Analysis Part I",
    "section": "4. Alpha diversity",
    "text": "4. Alpha diversity\nAlpha diversity metrics are crucial in ecology and microbiome studies because they provide insights into the richness and evenness of species within a single ecosystem or community sample. This allows researchers to understand:\n\nSpecies Richness: How many different types of species exist in a single community? This can be particularly important in assessing the overall biodiversity.\nSpecies Evenness: How evenly these species are distributed within the community. In other words, whether a few species dominate the community, or whether many species coexist in relatively similar numbers.\n\nThe data can be used to compare different communities or to see how a community changes over time in response to various factors such as perturbation, disease, or interventions.\nIn general, various alpha diversity metrics differ in whether, and to what degree, they emphasize richness or evenness.\nThe phyloseq package provides an all-in-one function plot_richness() that both calculates and plots an array of alpha diversity measures. But to be honest, I don’t often use it, opting instead to calculate the metrics using the estimate_richness() function. This places them in a dataframe, to which we can add our sample data and create plots using ggplot2.\nLook at the documentation by calling ?estimate_richness at the console prompt. Which alpha diversity measures are available to us within the function estimate_richness()?\nNote: Some estimates of alpha diversity (e.g., Chao1, ACE) must be calculated on unfiltered data. Specifically, singletons should be present. (That is, ASVs that appear only once (count of 1) within samples should be present.) This is because singletons are considered as a term in these estimates. Does our data contain singletons?\n\nsum(otu_table(ps_clean) == 1)\n\n[1] 42\n\n\nYes, but not many. (Think about how dada2 works and why this might be.) We won’t use Chao1 or ACE here, but it’s still a good idea to calculate whatever alpha diversity measure you choose using relatively unfiltered data.\nLet’s calculate alpha diversity using three different measures. The result will return as a dataframe. Then let’s join our alpha diversity data to our sample data.\n\nmeasures &lt;- c(\"Observed\", \"Shannon\", \"Simpson\")\n\nalpha_df &lt;- ps_clean %&gt;% \n  estimate_richness(measures = measures) %&gt;% \n  rownames_to_column(var = \"amplicon_sample_id\") %&gt;% \n  left_join(sample_df, by = \"amplicon_sample_id\")\n\nalpha_df %&gt;% \n  head() %&gt;% pander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\namplicon_sample_id\nObserved\nShannon\nSimpson\namplicon_type\n\n\n\n\nGUT0001\n126\n3.286\n0.9296\nstudy_sample\n\n\nGUT0002\n112\n3.454\n0.9274\nstudy_sample\n\n\nGUT0003\n117\n3.277\n0.9403\nstudy_sample\n\n\nGUT0004\n105\n3.163\n0.9113\nstudy_sample\n\n\nGUT0005\n122\n3.265\n0.9067\nstudy_sample\n\n\nGUT0006\n239\n4.333\n0.9764\nstudy_sample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninstrument_run\nreads_out\npid\narm\ntime_point\nsample_type\n\n\n\n\nrun1\n191197\npid_32\ntreatment\nweek_1\ngut_biopsy\n\n\nrun1\n17206\npid_14\nplacebo\nweek_7\ngut_biopsy\n\n\nrun1\n112769\npid_01\nplacebo\nbaseline\ngut_biopsy\n\n\nrun1\n25242\npid_06\nplacebo\nbaseline\ngut_biopsy\n\n\nrun1\n50533\npid_09\ntreatment\nweek_1\ngut_biopsy\n\n\nrun1\n142644\npid_18\ntreatment\nweek_7\ngut_biopsy\n\n\n\n\n\nNow let’s make some plots comparing alpha diversity across samples of interest.\n\nZymo mocks\nIn the absence of any batch effect, we would expect no difference in alpha diversity between mocks from run1 vs run2\n\nset.seed(123) # For reproducibility\n\nalpha_df %&gt;% \n  filter(amplicon_type == \"zymo_mock\") %&gt;% \n  pivot_longer(cols = all_of(measures),\n               names_to = \"adiv_measure\",\n               values_to = \"adiv_value\") %&gt;% \n  ggplot(aes(x = instrument_run, y = adiv_value)) +\n  geom_boxplot(alpha = 0.5, outlier.shape = NA) +\n  geom_jitter(width = 0.2) +\n  facet_wrap(.~adiv_measure, scales = \"free_y\")\n\n\n\n\n\n\n\n\nHowever, they are different, suggesting a batch effect. There seem to be “extra” taxa in mocks from run1 (keeping in mind that the Zymo mock contains eight bacterial species). We could discuss and test whether rarefying the data or aggregating taxa change this result.\nNote: The traditional Simpson diversity index represents the probability that two individuals randomly selected from a sample will belong to the same species. Higher values indicate lower diversity. Therefore, it is common to transform Simpson to 1 - Simpson. How would you modify the last chunk to plot 1 - Simpson in place of Simpson?\n\n\nStudy samples\nIf our allocation of participants to study arms was random, we might expect no difference in alpha diversity between the placebo and treatment groups at baseline (before the study intervention). Is this true?\nFirst we plot\n\nlibrary(ggbeeswarm) # For function geom_quasirandom()\n\n\nalpha_df %&gt;% \n  filter(time_point == \"baseline\") %&gt;% \n  pivot_longer(cols = all_of(measures),\n               names_to = \"adiv_measure\",\n               values_to = \"adiv_value\") %&gt;% \n  ggplot(aes(x = arm, y = adiv_value)) +\n  geom_boxplot(alpha = 0.4, outlier.shape = NA) +\n  geom_quasirandom(stroke = FALSE, width = 0.3) +\n  facet_wrap(sample_type~adiv_measure, scales = \"free_y\") +\n  labs(title = \"baseline (pre-intervention)\")\n\n\n\n\n\n\n\n\nWe notice some small differences. If these are of interest or concern to us, we can follow up. For example, for the baseline vaginal Shannon diversity index, we might ask whether the difference between groups (arms) was significant?\nLet’s use a Wilcoxon test using the function wilcox_test() from the package rstatix. (It’s the same as base R’s wilcox.test().)\n\nalpha_df %&gt;% \n  filter(time_point == \"baseline\",\n         sample_type == \"vaginal\") %&gt;% \n  wilcox_test(Shannon ~ arm, paired = FALSE) %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\n\n\n.y.\ngroup1\ngroup2\nn1\nn2\nstatistic\np\n\n\n\n\nShannon\nplacebo\ntreatment\n23\n20\n287\n0.171\n\n\n\n\n\nAt baseline, prior to intervention, there was no significant difference between the two groups (arms) in terms of vaginal bacterial diversity, as measured using the Shannon diversity index (Wilcoxon rank sum test; P = 0.171).\nIn the chunk above, change the time_point to “week_1” (that’s after intervention) and re-run the code. What do you notice? How about for “week_7”?\nLet’s visualize the trends. We’ll use the function stat_compare_means() from the package ggpubr to compare between arms at each time point using the same type of test we ran above (a Wilcoxon rank sum test).\n\nlibrary(ggpubr)\n\nLet’s start with the vaginal data\n\nalpha_df %&gt;% \n  filter(sample_type == \"vaginal\") %&gt;%  \n  ggplot(aes(x = arm, y = Shannon)) +\n  geom_boxplot(alpha = 0.4, outlier.shape = NA) +\n  geom_quasirandom(stroke = FALSE, width = 0.2) +\n  facet_wrap(.~time_point) +\n  stat_compare_means(method = \"wilcox.test\", paired = FALSE) + \n  labs(title = \"Vaginal samples\",\n       x = \"Study arm\",\n       y = \"Shannon diversity index\")\n\n\n\n\n\n\n\n\nHow would you describe these trends?\nWhat do we notice if we modify the chunk above to perform the same analysis for gut samples? Or for different measures of alpha diversity?\nFinally, there’s an alternative way to view these data, that is, viewing it as longitudinal data. In this view, participants are compared to themselves prior to intervention.\nLet’s visualize\n\nalpha_df %&gt;% \n  drop_na() %&gt;% \n  ggplot(aes(x = time_point, y = Shannon)) +\n  geom_line(aes(group = pid), alpha = 0.5) +\n  geom_point(stroke = FALSE) +\n  facet_grid(sample_type ~ arm, scales = \"free\") +\n  labs(x = \"Timepoint\",\n       y = \"Shannon diversity index\")\n\n\n\n\n\n\n\n\nIndividuals are dynamic!\nAs a follow-up, we might be interested in comparing week_1 to baseline within arms using a paired test so that participants are compared to themselves.\nHere’s an example\n\n# Get and arrange the data\ntest_df &lt;- alpha_df %&gt;% \n  filter(arm == \"treatment\",\n         time_point != \"week_7\",\n         sample_type == \"vaginal\") %&gt;% \n  group_by(pid) %&gt;% filter(n() == 2) %&gt;% ungroup() %&gt;% # Retain complete cases \n  arrange(pid, time_point) # Important step if paired tests will be performed\n\n\n# Display summary statistics\ntest_df %&gt;% \n  group_by(time_point) %&gt;% \n  get_summary_stats(Shannon, type = \"five_number\") %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntime_point\nvariable\nn\nmin\nmax\nq1\nmedian\nq3\n\n\n\n\nbaseline\nShannon\n18\n0.169\n3.438\n1.038\n2.033\n2.859\n\n\nweek_1\nShannon\n18\n0.148\n2.893\n0.301\n0.875\n2.272\n\n\n\n\n\n\n# Do a paired test using base R\nwilcox.test(x = test_df$Shannon[test_df$time_point == \"baseline\"],\n            y = test_df$Shannon[test_df$time_point == \"week_1\"],\n            paired = TRUE, alternative = \"two.sided\")\n\n\n    Wilcoxon signed rank exact test\n\ndata:  test_df$Shannon[test_df$time_point == \"baseline\"] and test_df$Shannon[test_df$time_point == \"week_1\"]\nV = 125, p-value = 0.08977\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n# But of course there's also the friendlier way\ntest_df %&gt;%\n  wilcox_test(Shannon ~ time_point, paired = TRUE) %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\n\n\n.y.\ngroup1\ngroup2\nn1\nn2\nstatistic\np\n\n\n\n\nShannon\nbaseline\nweek_1\n18\n18\n125\n0.0898\n\n\n\n\n\nHow would you express this result? If you modified the chunks above to examine the placebo arm (or week_7, or gut samples), what do you find?",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 4</b>: Exploring High Dimensional Data"
    ]
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/index.html#beta-diversity",
    "href": "materials/2-workshop2/4-16s-pt1/index.html#beta-diversity",
    "title": "16S Data Analysis Part I",
    "section": "5. Beta diversity",
    "text": "5. Beta diversity\nVariation (sometimes called turnover) in species composition over space and time; in relation to any variable of interest such as perturbation, disease, or intervention\nBeta diversity refers to between-sample diversity. At its core are measures of how much diversity is shared (or not shared) between communities. These are measures of pairwise resemblance; referred to as similarity, dissimilarity, or distance metrics. These measures differ in how they are calculated; whether and to what degree they consider abundance information; and whether they treat taxa as independent or (phylogenetically) related units. Different measures emphasize different aspects of the data and therefore may give different results. Insight may be gained from these differing results.\nWhen we calculate distances (i.e., between-sample diversity) among many, many pairs of samples, we typically can’t ascertain directly (by plotting or by eye) the main patterns or structures in the data. To explore these patterns or structures, we employ unsupervised methods that reduce this “high-dimensionality”. These methods include unconstrained ordination and clustering. They are considered unsupervised because the goal is not to predict or explain the importance of one particular variable, but to visualize and understand the primary sources of variation within the data, whatever they might be. In an ordination plot, similar samples are placed closer together, and dissimilar samples are placed further away.\nThe phyloseq package provides functions for calculating measures of pairwise resemblance and performing ordination. Core functions include distance(), ordinate() and plot_ordination(). Please see the documentation for the function vegdist() from the vegan package for details on many dissimilarity measures.\nSteps in a typical exploratory beta-diversity workflow might include:\n\nSubet samples and/or filter taxa\nTransform count data (if necessary or desired)\nCalculate distances\nOrdinate\nPlot\nRepeat using alternative distance measures or ordination methods\n\n\nZymo mocks\nWe can demo this very quickly using our Zymo mocks.\nIf batch effects were negligible, then we’d expect little separation by batch. In other words, we do not expect strong clustering by batch.\nFirst we’ll use the Jaccard dissimilarity measure, which is a binary measure, meaning it considers only presence-absence. And as an ordination method we’ll use principal coordinate analysis (PCoA; also called multidimensional scaling, or MDS).\n\nps_clean %&gt;% \n  subset_samples(., amplicon_type == \"zymo_mock\") %&gt;%\n  filter_taxa(., function(x) sum(x &gt; 0) &gt; 0, prune = TRUE) %&gt;%\n  ordinate(., distance = \"jaccard\", binary = TRUE, method = \"MDS\") %&gt;% \n  plot_ordination(ps_clean, ., type = \"samples\", color = \"instrument_run\")\n\n\n\n\n\n\n\n\nAgain, we see evidence of a batch effect. At this point, we’re not surprised - we’ve seen evidence of this in our bar plots and in our alpha diversity plots.\nWhat happens to the plot above if we change the distance metric to Bray-Curtis (distance = \"bray\"), a quantitative measure, meaning that it takes abundance information into account? (Don’t forget to remove or set to false the binary argument.) Does the pattern change? Does our conclusion change?\nYou may have noticed several distance metrics listed under the function vegan::vegdist() that aren’t available in phyloseq. One of them is the robust Aitchison distance. This metric is a popular choice among microbiome researchers because of the way it handles the compositionality of our data using a centered log-ratio (CLR) transformation. This is especially important when we have relatively few taxa, such as after agglomeration (aggregation) at high taxonomic level (e.g., Phylum or Class). Here’s an example using this distance metric.\n\nlibrary(vegan)\n\n\n# Get the count table (what vegan calls the \"community data matrix\")\nzym_tab &lt;- ps_clean %&gt;% \n  subset_samples(., amplicon_type == \"zymo_mock\") %&gt;%\n  filter_taxa(., function(x) sum(x &gt; 0) &gt; 0, prune = TRUE) %&gt;%\n  otu_table() \n\n# Use vegan's vegdist() to calculate a robust Aitchison distance matrix\nait_dist_zym &lt;- vegdist(x = zym_tab, method = \"robust.aitchison\")\n\n# Input this distance matrix into phyloseq's ordinate() and plot\nps_clean %&gt;%\n  ordinate(., distance = ait_dist_zym, method = \"MDS\") %&gt;% \n  plot_ordination(ps_clean, ., type = \"samples\", color = \"instrument_run\")\n\n\n\n\n\n\n\n\nLooks … familiar.\nNote that plot_ordination() includes an argument justDF that outputs the data underlying the plot rather than the plot itself. This can be helpful if you’d prefer to customize your plot using a different package.\n\n# Highlight `justDF` argument\nps_clean %&gt;%\n  ordinate(., distance = ait_dist_zym, method = \"MDS\") %&gt;% \n  plot_ordination(ps_clean, ., axes = c(1:3), justDF = TRUE) %&gt;% \n  select(Axis.1:reads_out) %&gt;% head(n = 3) %&gt;% pander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\n \nAxis.1\nAxis.2\nAxis.3\namplicon_type\n\n\n\n\nZYMO.DNA.CTRL.P1.run1\n-7.108\n-0.9808\n-0.5162\nzymo_mock\n\n\nZYMO.DNA.CTRL.P2.run1\n-7.97\n-4.122\n0.02995\nzymo_mock\n\n\nZYMO.DNA.CTRL.P3.run1\n-4.194\n-4.718\n0.2392\nzymo_mock\n\n\n\n\n\n\n\n\n\n\n\n \ninstrument_run\nreads_out\n\n\n\n\nZYMO.DNA.CTRL.P1.run1\nrun1\n147436\n\n\nZYMO.DNA.CTRL.P2.run1\nrun1\n144714\n\n\nZYMO.DNA.CTRL.P3.run1\nrun1\n114688\n\n\n\n\n\n\n\nStudy samples\nThe next three plots should give a taste of how clustering patterns may change given, e.g., the choice of distance metric. The first is binary Jaccard, the second is Bray-Curtis, and the third is robust Aitchison.\n\nps_clean %&gt;% \n  subset_samples(., !is.na(sample_type)) %&gt;%\n  filter_taxa(., function(x) sum(x &gt; 0) &gt; 0, prune = TRUE) %&gt;%\n  ordinate(., distance = \"jaccard\", binary = TRUE, method = \"MDS\") %&gt;% \n  plot_ordination(ps_clean, ., type = \"samples\", color = \"sample_type\") +\n  labs(title = \"binary Jaccard\")\n\n\n\n\n\n\n\n\n\nps_clean %&gt;% \n  subset_samples(., !is.na(sample_type)) %&gt;%\n  filter_taxa(., function(x) sum(x &gt; 0) &gt; 0, prune = TRUE) %&gt;%\n  ordinate(., distance = \"bray\", binary = FALSE, method = \"MDS\") %&gt;% \n  plot_ordination(ps_clean, ., type = \"samples\", color = \"sample_type\") +\n  labs(title = \"Bray-Curtis\")\n\n\n\n\n\n\n\n\n\nsam_tab &lt;- ps_clean %&gt;% \n  subset_samples(., !is.na(sample_type)) %&gt;%\n  filter_taxa(., function(x) sum(x &gt; 0) &gt; 0, prune = TRUE) %&gt;%\n  otu_table() \n\nait_dist_sam &lt;- vegdist(x = sam_tab, method = \"robust.aitchison\")\n\nps_clean %&gt;%\n  ordinate(., distance = ait_dist_sam, method = \"MDS\") %&gt;% \n  plot_ordination(ps_clean, ., type = \"samples\", color = \"sample_type\") +\n  labs(title = \"robust Aitchison\")\n\n\n\n\n\n\n\n\nWe see some different shapes. Nonetheless, all plots suggest that the primary source of variation in our data is by body site (gut versus vaginal). However, we can’t rule out that what we see here is driven by a batch effect. So to be sure, we’d need to address that issue – by correcting for the batch effect or by regenerating the data.\nTry re-running the three plots with different taxa filtering or subsetting. Or using different distance measures. Or different ordination methods (e.g., NMDS).",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 4</b>: Exploring High Dimensional Data"
    ]
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/index.html#lets-practice",
    "href": "materials/2-workshop2/4-16s-pt1/index.html#lets-practice",
    "title": "16S Data Analysis Part I",
    "section": "6. Let’s practice!",
    "text": "6. Let’s practice!\nLet’s practice what we’ve learned using some new data. I’ve provided two “practice” phyloseq objects, both based on published data.\nOne is gut (stool) microbiome data from mildly lactose-intolerant adults sampled over time as they eliminated and then reintroduced milk into their diet (dairy.ps).\nThe other is vaginal microbiome data from women sampled soon before and soon after giving birth, some via vaginal delivery and others via c-section (delivery.ps).\n\nManage paths\nShow paths to the data we intend to analyze\n\nfs::dir_ls(here(\"materials/2-workshop2/4-16s-pt1/data\"))\n\n/Users/jelsherbini/dev/durban-data-science-for-biology/materials/2-workshop2/4-16s-pt1/data/instructional\n/Users/jelsherbini/dev/durban-data-science-for-biology/materials/2-workshop2/4-16s-pt1/data/practice\n\n\nCreate an object containing the path to the practice data\n\npt_path &lt;- file.path(here(\"materials/2-workshop2/4-16s-pt1/data/practice/\"))\npt_path\n\n[1] \"/Users/jelsherbini/dev/durban-data-science-for-biology/materials/2-workshop2/4-16s-pt1/data/practice/\"\n\n\nList the files we intend to load\n\nlist.files(pt_path)\n\n[1] \"dairy_phyloseq.rds\"    \"delivery_phyloseq.rds\"\n\n\n\n\nLoad data\nLet’s load the practice data\n\nps_dairy &lt;- readRDS(paste0(pt_path, \"dairy_phyloseq.rds\"))\nps_deliv &lt;- readRDS(paste0(pt_path, \"delivery_phyloseq.rds\"))\n\n\n\nDairy dataset\nProduce a concise summary of ps_dairy\n\nps_dairy\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 3001 taxa and 434 samples ]\nsample_data() Sample Data:       [ 434 samples by 4 sample variables ]\ntax_table()   Taxonomy Table:    [ 3001 taxa by 6 taxonomic ranks ]\n\n\nAccess the sample variables for ps_dairy\n\nsample_variables(ps_dairy)\n\n[1] \"participant_id\" \"previous_diet\"  \"study_day\"      \"study_phase\"   \n\n\nReview the study design by making a plot; use the variables participant_id, study_day, and study_phase\n\nps_dairy %&gt;% \n  sample_data() %&gt;% \n  ggplot(aes(x = study_day, y = participant_id, color = study_phase)) +\n  geom_point() +\n  labs(title = \"dairy study; stool samples\")\n\n\n\n\n\n\n\n\nQuestion: What is the primary source of variation among the samples in ps_dairy? Is it participant_id or study_phase? Hint: Begin with binary Jaccard\n\nps_dairy %&gt;% \n  ordinate(., distance = \"jaccard\", binary = TRUE, method = \"MDS\") %&gt;% \n  plot_ordination(ps_dairy, ., type = \"samples\", color = \"participant_id\") +\n  labs(title = \"binary Jaccard\")\n\n\n\n\n\n\n\nps_dairy %&gt;% \n  ordinate(., distance = \"jaccard\", binary = TRUE, method = \"MDS\") %&gt;% \n  plot_ordination(ps_dairy, ., type = \"samples\", color = \"study_phase\") +\n  labs(title = \"binary Jaccard\")\n\n\n\n\n\n\n\n\nYour answer?\n\n\nDelivery dataset\nProduce a concise summary of ps_deliv\n\nps_deliv\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1551 taxa and 140 samples ]\nsample_data() Sample Data:       [ 140 samples by 7 sample variables ]\ntax_table()   Taxonomy Table:    [ 1551 taxa by 12 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 1551 tips and 1550 internal nodes ]\n\n\nAccess the sample variables for ps_deliv\n\nsample_variables(ps_deliv)\n\n[1] \"pregnancy_id\"       \"day_vs_delivery\"    \"status\"            \n[4] \"delivery_mode\"      \"sp_lacto_crispatus\" \"sp_lacto_gasseri\"  \n[7] \"sp_lacto_iners\"    \n\n\nReview the study design by making a plot; use the variables pregnancy_id, day_vs_delivery, and status; facet by delivery_mode\n\nps_deliv %&gt;% \n  sample_data() %&gt;%\n  ggplot(aes(x = day_vs_delivery, y = pregnancy_id, color = fct_rev(status))) +\n  geom_vline(xintercept = 0, linetype = 2, linewidth = 0.3) +\n  geom_point() +\n  facet_wrap(~delivery_mode, scales = \"free_y\") +\n  labs(title = \"delivery study; vaginal samples\",\n       color = \"status\")\n\n\n\n\n\n\n\n\nQuestion: Does the Shannon diversity index change with delivery? Does this depend on delivery mode?\n\n# Calculate Shannon\nshan_del &lt;- ps_deliv %&gt;% \n  estimate_richness(measures = \"Shannon\") %&gt;% \n  rownames_to_column(var = \"sample_id\")\n \n# Access the sample data and attach Shannon\nsam_shan_del &lt;- ps_deliv %&gt;% \n  sample_data() %&gt;% data.frame() %&gt;% \n  rownames_to_column(var = \"sample_id\") %&gt;% \n  left_join(shan_del, by = \"sample_id\") %&gt;%\n  group_by(pregnancy_id) %&gt;% filter(n() == 2) %&gt;% ungroup() %&gt;% \n  arrange(pregnancy_id, status) \n  \n# Plot\nsam_shan_del %&gt;% \n  ggplot(aes(x = fct_rev(status), y = Shannon)) +\n  geom_boxplot(alpha = 0.4, outlier.shape = NA) +\n  geom_quasirandom(stroke = FALSE, width = 0.3) +\n  facet_wrap(~delivery_mode) +\n  stat_compare_means(method = \"wilcox.test\", paired = TRUE) + \n  labs(x = \"Pregnancy status\",\n       y = \"Shannon diversity index\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nYour answer?\nBonus question: Try ordinating the samples in ps_deliv using Bray-Curtis distances. Color by the relative abundance of Lactobacillus crispatus, which I’ve included as a variable within the sample data (sp_lacto_crispatus), and also facet by status.\nNote that dominant taxa are strong drivers of clustering among vaginal samples. Can we “turn down the volume” on these dominant taxa so we can see more of what’s going on here? Try applying a square-root transformation before the ordination step.\n\nps_deliv %&gt;% \n  transform_sample_counts(., function(x) x^(1/2)) %&gt;%\n  ordinate(., distance = \"bray\", binary = FALSE, method = \"MDS\") %&gt;% \n  plot_ordination(ps_deliv, ., type = \"samples\", color = \"sp_lacto_crispatus\") +\n  facet_wrap(~fct_rev(status))\n\n\n\n\n\n\n\n\nJust as we transferred our alpha diversity measures to a dataframe, one can also transfer any number of taxon relative abundances to a dataframe as well.\nThat’s it for now. Nice work!! 🤓",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 4</b>: Exploring High Dimensional Data"
    ]
  },
  {
    "objectID": "materials/2-workshop2/4-16s-pt1/index.html#reproducibility-receipt",
    "href": "materials/2-workshop2/4-16s-pt1/index.html#reproducibility-receipt",
    "title": "16S Data Analysis Part I",
    "section": "7. Reproducibility receipt",
    "text": "7. Reproducibility receipt\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Africa/Johannesburg\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] vegan_2.6-4      lattice_0.22-6   permute_0.9-7    ggpubr_0.6.0    \n [5] ggbeeswarm_0.7.2 here_1.0.1       pander_0.6.5     rstatix_0.7.2   \n [9] magrittr_2.0.3   lubridate_1.9.3  forcats_1.0.0    stringr_1.5.1   \n[13] dplyr_1.1.4      purrr_1.0.2      readr_2.1.5      tidyr_1.3.1     \n[17] tibble_3.2.1     ggplot2_3.5.1    tidyverse_2.0.0  phyloseq_1.46.0 \n\nloaded via a namespace (and not attached):\n [1] ade4_1.7-22             tidyselect_1.2.1        vipor_0.4.7            \n [4] farver_2.1.1            Biostrings_2.70.3       bitops_1.0-7           \n [7] fastmap_1.1.1           RCurl_1.98-1.14         digest_0.6.35          \n[10] timechange_0.3.0        lifecycle_1.0.4         cluster_2.1.6          \n[13] survival_3.5-8          compiler_4.3.3          rlang_1.1.5            \n[16] tools_4.3.3             igraph_2.0.3            utf8_1.2.4             \n[19] yaml_2.3.8              data.table_1.15.4       ggsignif_0.6.4         \n[22] knitr_1.49              labeling_0.4.3          htmlwidgets_1.6.4      \n[25] plyr_1.8.9              abind_1.4-5             withr_3.0.0            \n[28] BiocGenerics_0.48.1     grid_4.3.3              stats4_4.3.3           \n[31] fansi_1.0.6             multtest_2.58.0         biomformat_1.30.0      \n[34] colorspace_2.1-1        Rhdf5lib_1.24.2         scales_1.3.0           \n[37] iterators_1.0.14        MASS_7.3-60.0.1         cli_3.6.3              \n[40] rmarkdown_2.29          crayon_1.5.2            generics_0.1.3         \n[43] rstudioapi_0.16.0       reshape2_1.4.4          tzdb_0.4.0             \n[46] ape_5.8                 rhdf5_2.46.1            zlibbioc_1.48.2        \n[49] splines_4.3.3           parallel_4.3.3          XVector_0.42.0         \n[52] vctrs_0.6.5             Matrix_1.6-5            carData_3.0-5          \n[55] jsonlite_1.8.8          car_3.1-2               IRanges_2.36.0         \n[58] hms_1.1.3               S4Vectors_0.40.2        beeswarm_0.4.0         \n[61] foreach_1.5.2           glue_1.8.0              codetools_0.2-20       \n[64] stringi_1.8.3           gtable_0.3.4            GenomeInfoDb_1.38.8    \n[67] munsell_0.5.1           pillar_1.9.0            htmltools_0.5.8.1      \n[70] rhdf5filters_1.14.1     GenomeInfoDbData_1.2.11 R6_2.5.1               \n[73] rprojroot_2.0.4         evaluate_0.23           Biobase_2.62.0         \n[76] backports_1.4.1         broom_1.0.5             Rcpp_1.0.12            \n[79] nlme_3.1-164            mgcv_1.9-1              xfun_0.50              \n[82] fs_1.6.5                pkgconfig_2.0.3",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 4</b>: Exploring High Dimensional Data"
    ]
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#section",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#section",
    "title": "Correlations and Differential Abundance Testing",
    "section": "",
    "text": "Workshop materials are at:\nhttps://elsherbini.github.io/durban-data-science-for-biology/"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#section-1",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#section-1",
    "title": "Correlations and Differential Abundance Testing",
    "section": "",
    "text": "Goals for this session\n\n\nLearn more advanced table commands\nLearn about plotting distributions with the tidyverse\n\n\n\n\ndata wrangling (n.) - the art of taking data in one format and filtering, reshaping, and deriving values to make the data format you need."
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#discussions-discord",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#discussions-discord",
    "title": "Correlations and Differential Abundance Testing",
    "section": "Discussions: discord",
    "text": "Discussions: discord\nAsk questions at #workshop-questions on https://discord.gg/UDAsYTzZE."
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#stickies",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#stickies",
    "title": "Correlations and Differential Abundance Testing",
    "section": "Stickies",
    "text": "Stickies\n\n\n\n\n\n\nDuring an activity, place a yellow sticky on your laptop if you’re good to go and a pink sticky if you want help.\n\n\n\n\nImage by Megan Duffy"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#practicalities",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#practicalities",
    "title": "Correlations and Differential Abundance Testing",
    "section": "Practicalities",
    "text": "Practicalities\n\nWiFi:\nNetwork: KTB Free Wifi (no password needed)\nNetwork AHRI Password: @hR1W1F1!17\nNetwork CAPRISA-Corp Password: corp@caprisa17\nBathrooms are out the lobby to your left"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#intro-exploratory-data-analysis-vs-modelling",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#intro-exploratory-data-analysis-vs-modelling",
    "title": "Correlations and Differential Abundance Testing",
    "section": "Intro: Exploratory Data Analysis vs modelling",
    "text": "Intro: Exploratory Data Analysis vs modelling"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#correlations-are-exploratory-observational",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#correlations-are-exploratory-observational",
    "title": "Correlations and Differential Abundance Testing",
    "section": "Correlations are exploratory / observational",
    "text": "Correlations are exploratory / observational"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#correlations-can-be-spurious",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#correlations-can-be-spurious",
    "title": "Correlations and Differential Abundance Testing",
    "section": "Correlations can be “spurious”",
    "text": "Correlations can be “spurious”"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#spurious-example",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#spurious-example",
    "title": "Correlations and Differential Abundance Testing",
    "section": "spurious example",
    "text": "spurious example\nhttps://www.tylervigen.com/spurious/correlation/2592_the-distance-between-uranus-and-earth_correlates-with_asthma-prevalence-in-american-children"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#how-do-we-perform-correlations-on-microbial-taxa",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#how-do-we-perform-correlations-on-microbial-taxa",
    "title": "Correlations and Differential Abundance Testing",
    "section": "How do we perform correlations on microbial taxa?",
    "text": "How do we perform correlations on microbial taxa?"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#one-at-a-time",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#one-at-a-time",
    "title": "Correlations and Differential Abundance Testing",
    "section": "one at a time",
    "text": "one at a time"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#spearman-vs-pearson",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#spearman-vs-pearson",
    "title": "Correlations and Differential Abundance Testing",
    "section": "spearman vs pearson",
    "text": "spearman vs pearson"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#what-to-do-with-zeros",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#what-to-do-with-zeros",
    "title": "Correlations and Differential Abundance Testing",
    "section": "what to do with zeros",
    "text": "what to do with zeros"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#build-a-grid",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#build-a-grid",
    "title": "Correlations and Differential Abundance Testing",
    "section": "build a grid",
    "text": "build a grid"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#heatmap",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#heatmap",
    "title": "Correlations and Differential Abundance Testing",
    "section": "heatmap",
    "text": "heatmap"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#wooclap-of-heatmap-interpretation",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#wooclap-of-heatmap-interpretation",
    "title": "Correlations and Differential Abundance Testing",
    "section": "wooclap of heatmap interpretation",
    "text": "wooclap of heatmap interpretation"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#correlations-of-taxa-with-other-taxa",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#correlations-of-taxa-with-other-taxa",
    "title": "Correlations and Differential Abundance Testing",
    "section": "correlations of taxa with other taxa",
    "text": "correlations of taxa with other taxa"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#interpretation",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#interpretation",
    "title": "Correlations and Differential Abundance Testing",
    "section": "interpretation",
    "text": "interpretation"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#exercise",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#exercise",
    "title": "Correlations and Differential Abundance Testing",
    "section": "exercise",
    "text": "exercise"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#modelling",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#modelling",
    "title": "Correlations and Differential Abundance Testing",
    "section": "modelling",
    "text": "modelling"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#now-we-are-talking-about-potentially-causal-interpretations",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#now-we-are-talking-about-potentially-causal-interpretations",
    "title": "Correlations and Differential Abundance Testing",
    "section": "now we are talking about potentially causal interpretations",
    "text": "now we are talking about potentially causal interpretations"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#what-kinds-of-questions-are-we-asking",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#what-kinds-of-questions-are-we-asking",
    "title": "Correlations and Differential Abundance Testing",
    "section": "what kinds of questions are we asking?",
    "text": "what kinds of questions are we asking?"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#how-do-we-build-a-causal-model",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#how-do-we-build-a-causal-model",
    "title": "Correlations and Differential Abundance Testing",
    "section": "how do we build a causal model?",
    "text": "how do we build a causal model?"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#how-do-we-convert-that-model-into-a-statistical-model",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#how-do-we-convert-that-model-into-a-statistical-model",
    "title": "Correlations and Differential Abundance Testing",
    "section": "how do we convert that model into a statistical model?",
    "text": "how do we convert that model into a statistical model?"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#test-one-taxa-at-a-time",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#test-one-taxa-at-a-time",
    "title": "Correlations and Differential Abundance Testing",
    "section": "Test one taxa at a time",
    "text": "Test one taxa at a time"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#interpret-the-model",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#interpret-the-model",
    "title": "Correlations and Differential Abundance Testing",
    "section": "interpret the model",
    "text": "interpret the model"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#testing-multiple-taxa",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#testing-multiple-taxa",
    "title": "Correlations and Differential Abundance Testing",
    "section": "testing multiple taxa",
    "text": "testing multiple taxa"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#wooclap-interpretting-the-multiple-taxa",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#wooclap-interpretting-the-multiple-taxa",
    "title": "Correlations and Differential Abundance Testing",
    "section": "wooclap interpretting the multiple taxa",
    "text": "wooclap interpretting the multiple taxa"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#showing-results-of-modelling-as-a-tree",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#showing-results-of-modelling-as-a-tree",
    "title": "Correlations and Differential Abundance Testing",
    "section": "showing results of modelling as a tree",
    "text": "showing results of modelling as a tree"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#interpretting-the-tree",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#interpretting-the-tree",
    "title": "Correlations and Differential Abundance Testing",
    "section": "interpretting the tree",
    "text": "interpretting the tree"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#distinction-between-real-phylogenetic-tree-and-this-abomination",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#distinction-between-real-phylogenetic-tree-and-this-abomination",
    "title": "Correlations and Differential Abundance Testing",
    "section": "distinction between real phylogenetic tree and this abomination",
    "text": "distinction between real phylogenetic tree and this abomination"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#wooclap",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#wooclap",
    "title": "Correlations and Differential Abundance Testing",
    "section": "wooclap",
    "text": "wooclap"
  },
  {
    "objectID": "materials/2-workshop2/5-16s-pt2/slides.html#exercise-on",
    "href": "materials/2-workshop2/5-16s-pt2/slides.html#exercise-on",
    "title": "Correlations and Differential Abundance Testing",
    "section": "exercise on",
    "text": "exercise on\n\n\n\nback to module"
  },
  {
    "objectID": "materials/2-workshop2/6-virgo/index.html#working-with-virgo2-output",
    "href": "materials/2-workshop2/6-virgo/index.html#working-with-virgo2-output",
    "title": "VIRGO2 analysis of shotgun metagenomic data",
    "section": "Working with VIRGO2 output",
    "text": "Working with VIRGO2 output\nThe output of VIRGO2 is a data frame of dimensions Genes by Samples and the values are the number of reads that were mapped to that gene. This data frame is generally LARGE and WIDE with up to 1.7 million genes. For the example code I’ve selected a random subset of 100,000 VIRGO2 genes. So the dataframe contains 100,000 rows (features). To expedite things, I’ve already merged a column for the taxonomic and functional annotations for each gene from the VIRGO2 annotation files.\nThis code will go through some basic exploration of the data to a simple analysis of differential taxonomic composition, differential functional composition, and then investigating which taxa are driving the differential abundance.",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 6</b>: Shotgun Metagenomics and VIRGO"
    ]
  },
  {
    "objectID": "materials/2-workshop2/6-virgo/index.html#installing-a-required-package",
    "href": "materials/2-workshop2/6-virgo/index.html#installing-a-required-package",
    "title": "VIRGO2 analysis of shotgun metagenomic data",
    "section": "Installing a required package",
    "text": "Installing a required package\n\nif (!require(\"BiocManager\", quietly = TRUE))\n  install.packages(\"BiocManager\")\nBiocManager::install(\"Heatplus\", ask=FALSE)",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 6</b>: Shotgun Metagenomics and VIRGO"
    ]
  },
  {
    "objectID": "materials/2-workshop2/6-virgo/index.html#loading-required-packages",
    "href": "materials/2-workshop2/6-virgo/index.html#loading-required-packages",
    "title": "VIRGO2 analysis of shotgun metagenomic data",
    "section": "Loading required packages",
    "text": "Loading required packages\nThis block of code loads the packages we will use in this section.\n\nlibrary(dplyr)\nlibrary(gplots)\nlibrary(vegan)\nlibrary(RColorBrewer)\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(Heatplus)\nlibrary(GUniFrac)",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 6</b>: Shotgun Metagenomics and VIRGO"
    ]
  },
  {
    "objectID": "materials/2-workshop2/6-virgo/index.html#reading-in-and-examining-the-metadata-file",
    "href": "materials/2-workshop2/6-virgo/index.html#reading-in-and-examining-the-metadata-file",
    "title": "VIRGO2 analysis of shotgun metagenomic data",
    "section": "Reading in and examining the metadata file",
    "text": "Reading in and examining the metadata file\nThis block of code sets the working directory and reads in the metadata file.\n\n#reading in and examining the metaData file\nmetaData &lt;- read.csv(\"data/instructional/InstructionalDataset_samplesMatch.csv\")\n\nNow lets examine some of the metadata fields and look at how many entries there are.\n\n#counting entries in metaDataFile\n#this function pipes the metaData object to the count function to generation a table of the number of times each variable in the column appears\nmetaData %&gt;% \n  count(UID, name=\"Count\")\n\nmetaData %&gt;% \n  count(CST, name=\"Count\")\n\nmetaData %&gt;% \n  count(pid, name=\"Count\")\n\nmetaData %&gt;% \n  count(arm, name=\"Count\")\n\nmetaData %&gt;% \n  count(timepoint,name=\"Count\")\n\nNow we will read in the VIRGO2 read counts table, this will be followed by a bit of exploration of the dataset to identify and investigate potential problems.\n\n#reading in and examining the read counts file\nreadCounts &lt;- read.csv(\"./data/instructional/InstructionalDataset_VIRGO2.csv\")\n\n#printing the column names\nprint(colnames(readCounts))\n#printing the first couple lines of the dataframe\nprint(head(readCounts))\n\n#printing the number of unique KEGG and Taxa values in the dataframe\nprint(n_distinct(readCounts$KEGG))\nprint(n_distinct(readCounts$Taxa))\n\nAs we can see, the dataframe contains 4 columns of information about the genes (Gene ID, the KEGG category, the taxonomy of the gene, and the length of the gene). This is then followed by one column for each sample where the values are the number of reads from the sample that mapped to each of the genes.\nFirst we will look for differences in the number of reads mapped per metagenome to identify any samples for which there were insufficient reads mapped.\n\n#investigating the total number of reads per MG for outliers\n#this line sums the columns to calculate the number of reads per MG, the first 4 columns are skipped because they contain annotations and not sample data\ntotalReads = colSums(readCounts[,5:ncol(readCounts)])\n#this line returns the log10 transformed median value for the distribution of read counts\nlog10(median(totalReads))\n\n#this line plots a simple histogram of the distribution of read counts per MG\nhist(log10(totalReads),col=\"#000000\")\n\n#identifying the sample with the minimum number of reads\nwhich.min(totalReads)\n####dropping metagenome with minimum reads, from data table and metadata table\ndrops &lt;- c(\"VAG0042\")\nreadCounts &lt;- readCounts[ , !(names(readCounts) %in% drops)]\nmetaData &lt;- metaData[!(metaData$UID %in% drops),]\n\nNext we will just check that the sample was actually dropped and re-examine the distribution and minimum number of reads mapped.\n\n#checking it was dropped\n#again recalculating the number of reads mapped per MG\ntotalReads = colSums(readCounts[,5:ncol(readCounts)])\n\nhist(log10(totalReads),col=\"#000000\")\n\nAnother feature we might want to examine is the number of genes mapped per metagenome. This could help identify differences in the total gene content per metagenome but could also highlight metagenomes where the bulk of the reads mapped to very few genes.\n\n#examining the gene counts per MG\n#converting the abundance data to presence absence data\ngeneCounts &lt;- colSums(readCounts[,5:ncol(readCounts)] != 0)\n\n#breaking this down, this part return true if the value in the cell is not equal to 0 and a False if the value is equal to zero, the column sums then returns the number of TRUE values, whic is the number of genes observed\nreadCounts[,5:ncol(readCounts)] != 0\n\n#generating a histogram of the number of genes per metagenome\n\nhist(geneCounts,breaks=seq(0,25000,by=1000))\n\nWhat do we think? Is this distribution problematic or expected?\n\nmetaData$geneCounts &lt;- geneCounts\nboxplot(geneCounts~CST,data=metaData,notch=FALSE,col=c('#fe0308','#f6d3da','#ff7200','#f8a40e','#221886'),ylab = 'No. of Genes',xlab=\"CST\")\n\nAs you can see, communities which contain more species (CST IV-B) yield more genes per metagenome than those that contain relatively few species (CST I, CST III).\nIn this next section we will begin to calculate the taxonomic composition of each metagenome, using the read counts per gene.\nEssentially, we will divide the read counts for each gene by the length of the gene. This approximates the coverage of the genes.\n\n#calculating taxonomic composition (in relative abundance) of each MG\n#this line divides the read counts by the gene length (NOTE THIS IS AN APPROXIMATION, WHY?)\ngeneCoverages &lt;- readCounts[,5:ncol(readCounts)]/readCounts$Length\n\nprint(head(geneCoverages))\n\nThen to calculate the taxonomic composition, we will sum the coverages of genes per taxa to get a total coverage of the taxa.\n\n#calculating the taxa relative abundances by first summing coverages of genes across taxa\n\n#first we add back the gene metadata columns\ngeneCoverages &lt;- cbind(readCounts[,1:4],geneCoverages)\n#then we drop the metadata values we don't need\ndrops &lt;- c('Gene','KEGG','Length')\ngeneCoverages &lt;- geneCoverages[ , !(names(geneCoverages) %in% drops)]\n\n#then we group the coverage by taxa and apply the sum function\ntaxaCoverages &lt;- data.frame(geneCoverages %&gt;% \n  group_by(Taxa) %&gt;% \n  summarise(across(everything(), sum)))\n\nprint(head(taxaCoverages))\n\nIn the next code block, we are going to drop the coverages of genes which did not have taxonomic annotations, and the genes annotated as originating from multiple genera. Opinions differ on whether or not to consider these proportions in your analyses and plots but for this analysis we will just remove them.\n\n#dropping the coverage contributed by genes without taxonomy and by genes which were annotated as \"MultiGenera\"\ntaxaCoverages = taxaCoverages[!taxaCoverages$Taxa == \"\", ]\ntaxaCoverages = taxaCoverages[!taxaCoverages$Taxa == \"MultiGenera\", ]\n\nWe will then change the row names to the taxa designations and then remove the column to taxa. Following that we will determine relative abundance as the coverage of each taxa divided by the sum of coverage values for the sample. This is akin to dividing the number of reads per taxa (in 16S amplicon data) by the total number of reads with the key difference being that we are considering variation in the gene length.\n\nrownames(taxaCoverages) &lt;- taxaCoverages$Taxa\ndrops &lt;- c('Taxa')\ntaxaCoverages &lt;- taxaCoverages[ , !(names(taxaCoverages) %in% drops)]\n\ntaxaRel &lt;- data.frame(t(t(taxaCoverages)/colSums(taxaCoverages)))\n\n#sorting the taxa by their study wide relative abundances\ntaxaRel &lt;- taxaRel[order(rowSums(taxaRel),decreasing=T),]\n\nprint(head(taxaRel))\n\nIn the next section we will apply some filters on the taxa to drop those that are not very prevalent or abundant in any given community. When we do this, we will keep track of how much “relative abundance” was lost when removing these taxa, to ensure that it is not having a major effect on our estimation of community composition.\n\n#applying a filter to remove any taxa less than 10-5 in average relative abundance (You should try additional filtering thresholds)\n#first we'll look at the sum of each column to demonstrate that, prior to filtering the total relative abundance for each sample was 1!\ncolSums(taxaRel)\n\n#then we apply a rowMeans filter (average relative abundance of the taxa) as 10^-5 or 0.00001\ntaxaRel_filt &lt;-  taxaRel %&gt;% filter(rowMeans(select(., where(is.numeric))) &gt; 0.00001)\n\n#checking what effect this filter had on our relative abundance scores\nmean(colSums(taxaRel_filt))\n#on average, this filter removed &lt;0.001% of the community composition\n1-min(colSums(taxaRel_filt))\n#the sample most impacted by this filter had 0.005% of it's community composition removed\nprint(dim(taxaRel))\nprint(dim(taxaRel_filt))\n\n\n#recomputing relative abundance after dropping those taxa, so samples are back to summing to 1\ntaxaRel &lt;- data.frame(t(t(taxaRel_filt)/colSums(taxaRel_filt)))\ncolSums(taxaRel)\n\nIn the next code block we will generate a heatmap displaying the taxonomic composition of each sample. As part of this process we’ll generate a dendrogram which will arrange samples with similar taxonomic composition together.\n\n#setting the ravel lab heatmap color palette\nheat_cmap &lt;- colorRampPalette(c('#FFFF00','#00EE76','#BBFFFF','#104E8B','#FF1493','#B22222'), space = \"rgb\")(12)\n\n# calculate the Bray-Curtis dissimilarity matrix on the full dataset:\ndata.dist &lt;- vegdist(t(taxaRel), method = \"bray\")\ndata.dist.g &lt;- vegdist(taxaRel, method = \"bray\")\n\n#Do ward linkage to generate dendrograms for hierarchical clustering.\nrow.clus &lt;- hclust(data.dist, \"average\")\ncol.clus &lt;- hclust(data.dist.g, \"average\")\n\n#creating a heatmap, sorting samples by the dendrogram generated in the above step\nheatmap(as.matrix(t(top_n(taxaRel,n=25))),Rowv = as.dendrogram(row.clus),Colv=NA, col = heat_cmap)\n\nIn the next section we will create a stacked bar plot (because I am addicted to them) which also displays the taxonomic composition. Importantly, we will sort the dataframe by the dendrogram so that, like the heatmap, samples which have similar taxonomic composition appear near each other in the plot.\n\n#subsetting out the top 25 taxa\ntaxaRel_trim &lt;- taxaRel[1:25,]\n\n#sorting the dataframe by the dendrogram orders\nsampSort &lt;- row.clus$labels[match(row.clus$order, sort(row.clus$order))]\ntaxaRelBar &lt;- data.frame(t(taxaRel_trim[,sampSort]))\n\n#calculating the \"other\" variable which tracks the proprotion of coverage not contributed by the top 25\ntaxaRelBar$other &lt;- 1-rowSums(taxaRelBar)\ntaxaRelBar$Sample &lt;- row.names(taxaRelBar)\n\n#melting the dataframe from \"wide\" format to \"long\" format\ntaxaRelBarLong = melt(taxaRelBar, id = c(\"Sample\"))\n#ensuring that the samples in the bar graph will maintain their order\ntaxaRelBarLong$Sample &lt;- factor(taxaRelBarLong$Sample,levels=unique(taxaRelBarLong$Sample))\n\n#setting the taxa color scheme\ncolours = c('#ff8c00','#ff0000','#58e0d9','#20b2aa','#409490','#0000cd','#ffeeee','#bbeff2','#99b0af','#1acad6','#333333','#008B45','#cf36b5','#05ebdf','#684a6b','#6accc7','#a16060','#3bb16f','#86bf4d','#146c73','#ff00d9','#ac8fc7','#00494f','#085e5a','#c997cc','#e1e1e1')\n\n#make the stackedplot!\nstackedBarPlot = ggplot(taxaRelBarLong, aes(x = Sample, fill = variable, y = value)) + \n  geom_bar(stat = \"identity\", colour = \"black\") + \n  theme(axis.text.x = element_text(angle = 90, size = 14, colour = \"black\", vjust = 0.5, hjust = 1), \n        axis.title.y = element_text(size = 16, face = \"bold\"), legend.title = element_text(size = 16, face = \"bold\"), \n        legend.text = element_text(size = 12, face = \"bold\", colour = \"black\"),\n        legend.key.size = unit(10, \"pt\"),\n        legend.background = element_rect(fill=\"lightgray\"),\n        axis.text.y = element_text(colour = \"black\", size = 12, face = \"bold\")) + \n  guides(fill=guide_legend(ncol =1)) + \n  scale_y_continuous(expand = c(0,0)) + \n  labs(x = \"\", y = \"Relative Abundance (%)\", fill = \"Taxa\") + \n  scale_fill_manual(values = colours)\n\nstackedBarPlot\n\nIn the final section on taxonomic composition, we’ll perform a differential abundance test to determine if their are taxa which are more abundant in the treatment versus the placebo arm. There are lots of different tools to do this analysis, most of them try to deal with the central problems with microbiome datasets (1: the data are compositional; 2: the data are sparse). For this analysis I’m using ZicoSeq which uses the read counts as inputs and not the relative abundance scores we calculated. Under the hood it’s running a permutation test but if you want to know more I would suggest your read the paper. It seems like every week there is a new tool do this test, generally I recommend trying a couple options and look for consistency between them.\nIn this first part we’ll generate the input expected by ZicoSeq, which is a dataframe of read counts, this will look similar to what we did for the taxonomic composition except we are just summing read counts per taxa.\n\n#generating taxaReadCounts for analysis by ZicoSeq\n\n#dropping gene annotations we don't need\ndrops &lt;- c('Gene','KEGG','Length')\ntaxaCounts &lt;- readCounts[ , !(names(readCounts) %in% drops)]\n\n#summing the read counts per Taxa\ntaxaCounts &lt;- data.frame(taxaCounts %&gt;% \n                           group_by(Taxa) %&gt;% \n                           summarise(across(everything(), sum)))\n\n#filtering out reads mapped to genes without taxonomic annotations or with MultiGenera annotation\ntaxaCounts = taxaCounts[!taxaCounts$Taxa == \"\", ]\ntaxaCounts = taxaCounts[!taxaCounts$Taxa == \"MultiGenera\", ]\n\n#making the row names the taxa\nrownames(taxaCounts) &lt;- taxaCounts$Taxa\ndrops &lt;- c('Taxa')\ntaxaCounts &lt;- taxaCounts[ , !(names(taxaCounts) %in% drops)]\n\n#sorting the dataframe so that the most abundant taxa are at the top of the dataframe\ntaxaCounts &lt;- taxaCounts[order(rowSums(taxaCounts),decreasing=T),]\n\n#dropping taxa which have absolutely no reads, ZicoSeq doesn't like this\ntaxaCounts &lt;- taxaCounts[rowSums(taxaCounts != 0) &gt; 0,]\n\nprint(head(taxaCounts))\nprint(head(metaData))\n\nDifferences in baseline\n\nbaseline_metaData &lt;- metaData[metaData$timepoint == 'baseline',]\nbaseline_taxaCounts &lt;- taxaCounts[ , (names(taxaCounts) %in% baseline_metaData$UID)]\n\nprint(head(baseline_metaData))\nprint(head(baseline_taxaCounts))\ndim(baseline_taxaCounts)\nbaseline_taxaCounts &lt;- as.matrix(baseline_taxaCounts)\nbaseline_taxaCounts &lt;- baseline_taxaCounts[rowSums(baseline_taxaCounts != 0) &gt; 0,]\n\n\nbaseline_taxaZicoOut &lt;- ZicoSeq(meta.dat = baseline_metaData, feature.dat = baseline_taxaCounts                                      ,grp.name =c('arm'),feature.dat.type = \"count\",\n                                     # Filter to remove rare taxa\n                                     prev.filter = 0.15, mean.abund.filter = 0.00001,  \n                                     max.abund.filter = 0.0001, min.prop = 0, \n                                     # Winsorization to replace outliers\n                                     is.winsor = TRUE, outlier.pct = 0.03, winsor.end = 'both',\n                                     # Posterior sampling \n                                     is.post.sample = TRUE, post.sample.no = 100, \n                                     # Use the square-root transformation\n                                     link.func = list(function (x) x^0.5), stats.combine.func = max,\n                                     # Permutation-based multiple testing correction\n                                     perm.no = 99,  strata = NULL, \n                                     # Reference-based multiple stage normalization\n                                     ref.pct = 0.5, stage.no = 6, excl.pct = 0.2,\n                                     is.fwer = TRUE, verbose = TRUE, return.feature.dat = T)\n\n\nZicoSeq.plot(baseline_taxaZicoOut, metaData, pvalue.type = 'p.adj.fdr', cutoff = 0.01, text.size = 10,out.dir = NULL, width = 10, height = 6)\n\nx &lt;- data.frame(baseline_taxaZicoOut$p.adj.fdr)\ny &lt;- data.frame(baseline_taxaZicoOut$R2)\nz &lt;- data.frame(baseline_taxaZicoOut$coef.list)\nz &lt;- t(z)\n\nbaselineTaxaOut &lt;- cbind(x,y,z)\nbaselineTaxaOut &lt;- baselineTaxaOut[order(baselineTaxaOut$baseline_taxaZicoOut.p.adj.fdr,decreasing=FALSE),]\n#print(baselineTaxaOut)\n\nWeek 1\n\nweek1_metaData &lt;- metaData[metaData$timepoint == 'week_1',]\nweek1_taxaCounts &lt;- taxaCounts[ , (names(taxaCounts) %in% week1_metaData$UID)]\n\nprint(head(week1_metaData))\nprint(head(week1_taxaCounts))\ndim(week1_taxaCounts)\nweek1_taxaCounts &lt;- as.matrix(week1_taxaCounts)\nweek1_taxaCounts &lt;- week1_taxaCounts[rowSums(week1_taxaCounts != 0) &gt; 0,]\n\n\nweek1_taxaZicoOut &lt;- ZicoSeq(meta.dat = week1_metaData, feature.dat = week1_taxaCounts, \n                                     grp.name =c('arm'),feature.dat.type = \"count\",\n                                     # Filter to remove rare taxa\n                                     prev.filter = 0.15, mean.abund.filter = 0.00001,  \n                                     max.abund.filter = 0.0001, min.prop = 0, \n                                     # Winsorization to replace outliers\n                                     is.winsor = TRUE, outlier.pct = 0.03, winsor.end = 'both',\n                                     # Posterior sampling \n                                     is.post.sample = TRUE, post.sample.no = 100, \n                                     # Use the square-root transformation\n                                     link.func = list(function (x) x^0.5), stats.combine.func = max,\n                                     # Permutation-based multiple testing correction\n                                     perm.no = 999,  strata = NULL, \n                                     # Reference-based multiple stage normalization\n                                     ref.pct = 0.5, stage.no = 6, excl.pct = 0.2,\n                                     is.fwer = TRUE, verbose = TRUE, return.feature.dat = T)\n\n\nZicoSeq.plot(week1_taxaZicoOut, metaData, pvalue.type = 'p.adj.fdr', cutoff = 0.01, text.size = 10,out.dir = NULL, width = 10, height = 6)\n\nx &lt;- data.frame(week1_taxaZicoOut$p.adj.fdr)\ny &lt;- data.frame(week1_taxaZicoOut$R2)\nz &lt;- data.frame(week1_taxaZicoOut$coef.list)\nz &lt;- t(z)\n\nweek1TaxaOut &lt;- cbind(x,y,z)\nweek1TaxaOut &lt;- week1TaxaOut[order(week1TaxaOut$week1_taxaZicoOut.p.adj.fdr,decreasing=FALSE),]\n#print(week1TaxaOut)\n\nWeek 7\n\nweek7_metaData &lt;- metaData[metaData$timepoint == 'week_7',]\nweek7_taxaCounts &lt;- taxaCounts[ , (names(taxaCounts) %in% week7_metaData$UID)]\n\nprint(head(week7_metaData))\nprint(head(week7_taxaCounts))\ndim(week7_taxaCounts)\nweek7_taxaCounts &lt;- as.matrix(week7_taxaCounts)\nweek7_taxaCounts &lt;- week7_taxaCounts[rowSums(week7_taxaCounts != 0) &gt; 0,]\n\n\nweek7_taxaZicoOut &lt;- ZicoSeq(meta.dat = week7_metaData, feature.dat = week7_taxaCounts, \n                                     grp.name =c('arm'),feature.dat.type = \"count\",\n                                     # Filter to remove rare taxa\n                                     prev.filter = 0.15, mean.abund.filter = 0.00001,  \n                                     max.abund.filter = 0.0001, min.prop = 0, \n                                     # Winsorization to replace outliers\n                                     is.winsor = TRUE, outlier.pct = 0.03, winsor.end = 'both',\n                                     # Posterior sampling \n                                     is.post.sample = TRUE, post.sample.no = 100, \n                                     # Use the square-root transformation\n                                     link.func = list(function (x) x^0.5), stats.combine.func = max,\n                                     # Permutation-based multiple testing correction\n                                     perm.no = 99,  strata = NULL, \n                                     # Reference-based multiple stage normalization\n                                     ref.pct = 0.5, stage.no = 6, excl.pct = 0.2,\n                                     is.fwer = TRUE, verbose = TRUE, return.feature.dat = T)\n\n\nZicoSeq.plot(week7_taxaZicoOut, metaData, pvalue.type = 'p.adj.fdr', cutoff = 0.01, text.size = 10,\n             out.dir = NULL, width = 10, height = 6)\nx &lt;- data.frame(week7_taxaZicoOut$p.adj.fdr)\ny &lt;- data.frame(week7_taxaZicoOut$R2)\nz &lt;- data.frame(week7_taxaZicoOut$coef.list)\nz &lt;- t(z)\n\nweek7TaxaOut &lt;- cbind(x,y,z)\nweek7TaxaOut &lt;- week7TaxaOut[order(week7TaxaOut$week7_taxaZicoOut.p.adj.fdr,decreasing=FALSE),]\n#print(week7TaxaOut)",
    "crumbs": [
      "AWS Instance IPs",
      "<em>2. Working with High Dimensional Data in R - The Human Microbiome</em>",
      "<b>Module 6</b>: Shotgun Metagenomics and VIRGO"
    ]
  },
  {
    "objectID": "materials/2-workshop2/8-group-project/asavela_grp_b_analysis.html",
    "href": "materials/2-workshop2/8-group-project/asavela_grp_b_analysis.html",
    "title": "Group B: Menstruation Data",
    "section": "",
    "text": "Load Libraries\n\nlibrary(phyloseq)\nlibrary(microViz)\n\nmicroViz version 0.12.4 - Copyright (C) 2021-2024 David Barnett\n! Website: https://david-barnett.github.io/microViz\n✔ Useful?  For citation details, run: `citation(\"microViz\")`\n✖ Silence? `suppressPackageStartupMessages(library(microViz))`\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dada2)\n\nLoading required package: Rcpp\n\nlibrary(ggplot2)\nlibrary(ggbeeswarm)\nlibrary(ggpubr)\n\n\nAttaching package: 'ggpubr'\n\nThe following object is masked from 'package:microViz':\n\n    stat_chull\n\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:ggpubr':\n\n    get_legend\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\nlibrary(RColorBrewer)\n\n\n\nLoad data\n\namplicon_ids &lt;- read_csv(\"data/Group B Dataset Menstruation/04_period_amplicon_sample_ids.csv\")\nsamp_id &lt;- read_csv(\"data/Group B Dataset Menstruation/00_sample_ids_period.csv\")\nparticipant_metadata &lt;- read_csv(\"data/Group B Dataset Menstruation/01_participant_metadata_period.csv\")\nflow &lt;- read_csv(\"data/Group B Dataset Menstruation/03_flow_cytometry_period.csv\")\nluminex &lt;- read_csv(\"data/Group B Dataset Menstruation/02_luminex_period.csv\")\n\n\n\nLoad count and tax table\n\nall_samples_count_table &lt;- readRDS(\"data/Group B Dataset Menstruation/gv_seqtab_nobim.rds\")\nall_samples_tax_table &lt;- readRDS(\"data/Group B Dataset Menstruation/gv_spetab_nobim_silva.rds\")\n\n\n\nMake a phyloseq obj\n\n#creating sample data for the phyloseq\nbc_sample_data &lt;- amplicon_ids %&gt;%\n    left_join(samp_id %&gt;% select(-arm), by = c(\"pid\",\"time_point\")) %&gt;% # merges sample ids\n    left_join(participant_metadata %&gt;% select(-arm), by = \"pid\") %&gt;% # merges participant metadata\n    mutate(arm_timepoint = str_c(arm, time_point, sep = \"_\")) %&gt;%#creating a arm and timepoint column for later plotting\n    left_join(flow, by = \"sample_id\") %&gt;% # merges flow\n    left_join(luminex %&gt;% # merges luminex\n        pivot_wider(names_from = cytokine, values_from = c(conc,limits)), by = \"sample_id\") %&gt;% \n    mutate(arm_timepoint = str_c(arm, time_point, sep = \"_\")) %&gt;%\n    column_to_rownames(\"amplicon_sample_id\")\n\n\n\nExtract samples of interest : bc= birth control\n\nbc_ids &lt;- bc_sample_data %&gt;% \n              as.data.frame() %&gt;% \n              rownames_to_column(\"amplicon_sample_id\") %&gt;%\n              pull(amplicon_sample_id)\n\n\n\nRemove unused ASVs from the count table\n\ncount_table &lt;- all_samples_count_table %&gt;%\n                as.data.frame() %&gt;%\n                rownames_to_column(\"amplicon_sample_id\") %&gt;%\n                filter(amplicon_sample_id %in% bc_ids) %&gt;% #filtering the count data by bc ids\n                mutate_at(vars(-amplicon_sample_id), as.numeric) %&gt;%\n                column_to_rownames(\"amplicon_sample_id\") %&gt;%\n                select(where(~sum(.) != 0)) # removing unused ASVs - when the sum of a column is 0\n\n\n\nSparsity of my count table\n\na&lt;-as.matrix(count_table)\nclass(a)\n\n[1] \"matrix\" \"array\" \n\nsum(a == 0) / length(a)\n\n[1] 0.9678092\n\n##0.97\n\n\ndata.frame(asv_prev = colSums(a &gt; 0)) %&gt;% \n  ggplot(aes(x = asv_prev)) +\n  geom_histogram(bins = 50) +\n  labs(x = \"Number of samples in which the ASV was detected\",\n       y = \"Number of ASVs\")\n\n\n\n\n\n\n\nsum(is.na(a))\n\n[1] 0\n\n#0\n\nMost ASVs are absent from most samples. Sparsity is a common characteristic of microbiome data due to several reasons:\nBiological Diversity: Microbiome samples often contain a large number of microbial taxa, but each individual taxa may be present in only a subset of samples. Sequencing Depth: The depth of sequencing may not be sufficient to capture all microbial taxa present in a sample, leading to many undetected taxa. Experimental Conditions: Factors such as sample handling, DNA extraction methods, and PCR amplification biases can influence which taxa are detected and at what abundance.\n\n\nExplore the distribution of ASV lengths.\nAre there any you’d like to remove?\n\nLength\nusing getSequences from the dada2 package\n\ntable(nchar(getSequences(a)))\n\n\n260 270 271 272 273 274 275 276 277 278 279 280 282 283 288 300 337 339 346 373 \n  1   7   9   7  11  10   7   6   2   1   1   1   1   1   1   1   1   1   1   1 \n378 381 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 \n  1   1   1  32  78 194  46  54  68   9  86  48  53  22   4   2   4   7  10   5 \n419 420 421 422 423 424 425 426 427 428 429 430 431 448 473 \n 11   3  13   7  63 269  49   4   6  53 491  69   1   2   1 \n\n\nnchar shows you a table of each length and the number of ASVs which have that length We can visualize this using hist\n\nhist(nchar(getSequences(a)),\n     main=\"Distribution of sequence lengths\")\n\n\n\n\n\n\n\n\nYou may want to remove the outliers. I specified my desired length to be between 400 and 440\n\nseqtab &lt;- a[,nchar(colnames(a)) %in% seq(400, 440)]\n\n# % of reads with desired length\nsum(seqtab)/sum(a)\n\n[1] 0.9998825\n\n\nThis is not bad because we still retain 99% of our reads. Let us visualize them after filtering 🧐\n\nhist(nchar(getSequences(seqtab)),\n     main=\"Distribution of sequence lengths\")\n\n\n\n\n\n\n\n\n\n\n\nFilter the taxa table to only keep birth contr ASVs\n\n#asvs &lt;- colnames(count_table)\nasvs &lt;- colnames(seqtab)\n\n\n\nMake a taxa table\n\n#filter the taxa table so there is only asvs for the bc samples\ntax_table &lt;- all_samples_tax_table %&gt;%\n              as.data.frame() %&gt;%\n              rownames_to_column(\"tax_table_asv\") %&gt;%\n              filter(tax_table_asv %in% asvs) %&gt;% # filters the tax table by the asvs we want\n              column_to_rownames(\"tax_table_asv\") %&gt;%\n              as.matrix()\n\n\n\nAre there NAs in the tax table?\n\nclass(tax_table)\n\n[1] \"matrix\" \"array\" \n\nsum(is.na(tax_table))\n\n[1] 1869\n\n#2112 NAs\n\nMany! What could be the source of NAs in the taxa table?\n\n\nCreate a birth contr specific phyloseq object\n\n#creating phyloseq object\nbc_ps &lt;- phyloseq(otu_table(seqtab,#count_table,\n                            taxa_are_rows=FALSE), \n               sample_data(bc_sample_data), \n               tax_table(tax_table))\n\n\n\nPhyloseq cleaning and diagnosis\nDescribe the structure of the experimental design\nAim To investigate whether taking birth control is associated with vaginal inflammation throughout the menstrual cycle.\nStudy Description This is an observational study to evaluate the relationship between birth control and vaginal inflammation in response to menstruation. 16S rRNA sequencing was done to characterize patient microbiome composition. Cytokine levels (in ug/mL of vaginal fluid) were also measured by Luminex. We also looked at the number and type of immune cells in the vagina using Flow Cytometry. Data was collected at four timepoints; before, at the start, the end, and after menstruation.\n\n\nMake a long phyloseq data frame\npsmelt will provide us with a data frame with all the variables within the phyloseq object\n\nps_df&lt;-psmelt(bc_ps)\nnames(ps_df)\n\n [1] \"OTU\"                \"Sample\"             \"Abundance\"         \n [4] \"amplicon_type\"      \"pid\"                \"arm\"               \n [7] \"time_point\"         \"sample_id\"          \"age\"               \n[10] \"pcos_status\"        \"period_product\"     \"sex\"               \n[13] \"arm_timepoint\"      \"live_cd19_negative\" \"cd45_negative\"     \n[16] \"cd45_positive\"      \"neutrophils\"        \"non_neutrophils\"   \n[19] \"cd3_negative\"       \"cd3_positive\"       \"cd4_t_cells\"       \n[22] \"cd8_t_cells\"        \"conc_IL.1a\"         \"conc_IL.1b\"        \n[25] \"conc_IL.6\"          \"conc_TNFa\"          \"conc_MIG\"          \n[28] \"conc_IFNg\"          \"conc_IP.10\"         \"conc_MIP.3a\"       \n[31] \"limits_IL.1a\"       \"limits_IL.1b\"       \"limits_IL.6\"       \n[34] \"limits_TNFa\"        \"limits_MIG\"         \"limits_IFNg\"       \n[37] \"limits_IP.10\"       \"limits_MIP.3a\"      \"Kingdom\"           \n[40] \"Phylum\"             \"Class\"              \"Order\"             \n[43] \"Family\"             \"Genus\"              \"Species\"           \n\n\n##Research question Does Birth control #arm change inflammation #8 cells and 8 cytokines during menstruation? # time_point relative to menstruation period # Assess the study design To do this, I extracted variables of interest from the melted phyloseq object\n\nmy_variables&lt;-ps_df %&gt;% \n  select(arm, pid, Abundance, time_point,\n        conc_IL.1a:conc_MIP.3a  # columns with cytokines\n         ) %&gt;% distinct()\n\nnames(my_variables)\n\n [1] \"arm\"         \"pid\"         \"Abundance\"   \"time_point\"  \"conc_IL.1a\" \n [6] \"conc_IL.1b\"  \"conc_IL.6\"   \"conc_TNFa\"   \"conc_MIG\"    \"conc_IFNg\"  \n[11] \"conc_IP.10\"  \"conc_MIP.3a\"\n\n\nTo plot the variables, I used pivot_longer() so that my cytokines and concentrations can be in long format, and in two distinct columns\n\nmy_variables&lt;-my_variables %&gt;% \n  pivot_longer(cols= conc_IL.1a:conc_MIP.3a,\n               names_to = 'cytokines',\n               values_to='concentration')\nnames(my_variables)\n\n[1] \"arm\"           \"pid\"           \"Abundance\"     \"time_point\"   \n[5] \"cytokines\"     \"concentration\"\n\nmy_variables %&gt;% \n  # checking if there is 16S data available for each individual\n  mutate(detected = concentration &gt; 1) %&gt;%  \n  ggplot(aes(x=time_point,y=pid, \n             color= detected))+ # color indicates detectebility of cytokine conc\n  geom_point()+  # point indicates availability of cytokine value for each individual \n  facet_grid(rows = vars(arm), cols = vars(cytokines), scales = \"free_y\")+\n  theme(axis.text.x = element_text(angle = 90))+ \n  labs(title = \"Study Design\", y = \"Participant ID\")\n\n\n\n\n\n\n\n\nHere we can observe that in both arms, birth control and no birth control, cytokines such as IFNg , IL1_b, IL.6, MIP3 and TNFa have low concentrations a week prior and a week post menstruation. This could be an indicationg of a subsiding inflammatory response. In contrast , the cytokine MIP.3 is showing decrease in cytokine concentrations at onset and at the end of bleeding.\n\n\nRemoval of low yield samples\n\n\nCalculate total read counts per sample\n\nsample_sums(bc_ps)\n\nVAG0078 VAG0079 VAG0080 VAG0085 VAG0089 VAG0090 VAG0091 VAG0092 VAG0095 VAG0096 \n  56536   62605   74002   56260   58478   60649   23433   23079   63737   47081 \nVAG0101 VAG0104 VAG0107 VAG0113 VAG0116 VAG0119 VAG0120 VAG0123 VAG0127 VAG0161 \n  63996   57621   71918   58265   53864   54495   58633   61982   26395   31214 \nVAG0163 VAG0165 VAG0168 VAG0170 VAG0171 VAG0172 VAG0173 VAG0174 VAG0175 VAG0178 \n  70264   77552   59582   56702   43721   34335   58279   59759   58214   61400 \nVAG0181 VAG0182 VAG0183 VAG0185 VAG0188 VAG0190 VAG0196 VAG0197 VAG0198 VAG0200 \n  55600   86583   49733   57785   46704   16965   64876   65542   56368   57925 \nVAG0201 VAG0204 VAG0205 VAG0206 VAG0209 VAG0210 VAG0214 VAG0215 VAG0216 VAG0218 \n  46685   78531   10134   88943   60411   36643   32043   24929   49231   37102 \nVAG0228 VAG0230 VAG0231 VAG0233 VAG0235 VAG0296 VAG0298 VAG0300 VAG0304 VAG0306 \n  13733   48176   18031   44033    2756   96522    6265   83317   62892   74907 \nVAG0309 VAG0315 VAG0321 VAG0322 VAG0324 VAG0329 VAG0331 VAG0337 VAG0338 VAG0341 \n  61432   89541   12043   47712   34078   49699   98857   37939   56331   73460 \nVAG0342 VAG0348 VAG0349 VAG0350 VAG0351 VAG0352 VAG0356 VAG0360 VAG0363 VAG0366 \n  89980   40167   22079   71767   39806   42423  103025   91946    9793   62216 \nVAG0369 VAG0370 VAG0371 VAG0373 VAG0374 VAG0377 VAG0379 VAG0380 VAG0383 VAG0385 \n  80157   43982   64887   55703    7133   69681   55707   62280   72843   31136 \nVAG0386 VAG0390 VAG0394 VAG0395 VAG0396 VAG0398 VAG0399 VAG0400 VAG0402 VAG0403 \n  35617   41420    7548   73105   41220   47167   43424   58858   33705   57690 \nVAG0410 VAG0411 VAG0417 VAG0418 VAG0419 VAG0420 VAG0423 VAG0428 \n  85993   41437   69371   70672   54413   83228   31344   89748 \n\nsample_sums &lt;- sample_sums(bc_ps)\n\nI look at the summary statistics to see assess the distribution of read counts for most samples\n\nsummary(sample_sums)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2756   40077   56452   53344   65051  103025 \n\n## Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#    2766   40084   56452   53351   65059  103025\n\nYou want to exclude samples with exremely low sequecing depth but still retai majority of your samples.\nThese samples have a relatively good amount of reads, looking at the first quartile, 25% of these samples have about 40084 reads. To balance out the sequencing depth, I will remove samples that have half the Q2 value. This is because I want to retain a sufficient number of samples for downstream analysis while still using samples with a relatively high sequencing depth. This is totally dependent on how bad your yield is. if you lose a lot of samples you might have to repeat your sequencing and revise your library prep.\n\n# Create a dataframe for plotting\nsample_sums_df &lt;- data.frame(Sample = names(sample_sums), Total_Reads = sample_sums)\n# Plot \nggplot(sample_sums_df, aes(y = Total_Reads, x=Sample)) +\n  geom_point()+\n  labs(x = \"Sample\", y = \"Total reads\", title = \"Distribution of Total Reads per Sample\") +\n  theme_minimal()+\n  theme(axis.text.x = element_text(angle=90))+\n  geom_hline(aes(yintercept =20000,colour = 'red')) # shows my specified threshold \n\n\n\n\n\n\n\n\nWhen I set a threshold of 20000 I will lose 10 samples.\n\n# Plot \nggplot(sample_sums_df, aes(x = Total_Reads)) +\n  #geom_point()+\n  geom_histogram(binwidth = 1000, fill = \"skyblue\", color = \"black\") +\n  labs(x = \"Total Reads\", y = \"Frequency\", title = \"Distribution of Total Reads per Sample\", color= 'Threshold') +\n  theme_minimal()+\n  theme(axis.text.x = element_text(angle=90))+\n  geom_vline(aes(xintercept =20000,colour = 'red'))\n\n\n\n\n\n\n\n\n\n\nDefine a threshold for minimum total read count and remove empty ASVs\n\nmin_total_reads &lt;- 20000  # change to your desired threshold \n\n# Filter samples based on total read count\nfiltered_physeq &lt;-\n  prune_samples(sample_sums &gt;= min_total_reads, bc_ps) %&gt;%\n  # Drop empty ASVs\n  filter_taxa(., function(x) sum(x &gt; 0) &gt; 0, prune = TRUE) %&gt;% \n  # # Drop sporadic families\n   filter_taxa(., function(x) sum(x &gt; 0.01) &gt; 2, prune = TRUE) \n  \n  \n  \n# Check the number of samples before and after filtering\nprint(paste(\"Number of samples before filtering:\", length(sample_names( bc_ps))))\n\n[1] \"Number of samples before filtering: 108\"\n\nprint(paste(\"Number of samples after filtering:\", length(sample_names(filtered_physeq))))\n\n[1] \"Number of samples after filtering: 98\"\n\n# Now 'filtered_physeq' contains only the samples with total read counts above the threshold\n\n\nmin(taxa_sums(filtered_physeq))\n\n[1] 13\n\n\n\n#sample_variables(bc_ps)\n\n\n\nTaxa fix\n\nps_manual_taxonomy &lt;- filtered_physeq %&gt;%\n  tax_fix() %&gt;%\n  tax_mutate(Species = case_when(\n    Species ==  \"acidophilus/casei/crispatus/gallinarum\" ~ \"crispatus\",\n    Species == \"crispatus/gasseri/helveticus/johnsonii/kefiranofaciens\" ~ \"crispatus\",\n    Species == \"animalis/apodemi/crispatus/murinus\" ~ \"crispatus\",\n    .default = Species)) %&gt;%\n    #also remake genus_species to fix those taxa\n    tax_mutate(genus_species = str_c(Genus, Species, sep = \" \")) %&gt;%\n    tax_rename(rank = \"genus_species\")\n\nGenerate plots to compare relative abudances across arms and time points What are some noticable differences throughout menstruation? #Does birth control change inflammation during menstruation?\nOptional questions (Examine your sample_data to see what other questions you can ask!):\n\n\nWhat cytokines correlated with Lactobaccilus? #What cytokines correlate with certain taxa? How do the absolute abundance of bacteria and #Lactobacillus species change throughout menstruation?\n\n ps_manual_taxonomy %&gt;%\n  tax_fix() %&gt;%\n  tax_agg(\"genus_species\") %&gt;%\n  ps_seriate() %&gt;% # this changes the order of the samples to be sorted by similarity\n  comp_barplot(tax_level = \"genus_species\", sample_order = \"asis\", n_taxa = 10) +\n  facet_wrap(vars(arm, time_point), scales=\"free_x\")+\n  theme( axis.text.x = element_blank(), \n         axis.ticks.x = element_blank(), # removed x axis labels because they are ineligible \n          legend.position = \"bottom\") \n\nRegistered S3 method overwritten by 'seriation':\n  method         from \n  reorder.hclust vegan\n\n\n\n\n\n\n\n\n\n\n# Reorder the appearance of time_point\nsample_data(ps_manual_taxonomy)$time_point &lt;- factor(\n  sample_data(ps_manual_taxonomy)$time_point, \n  levels = c(\"week_prior\", \"onset\", \"end_bleeding\", \"week_post\")\n)\n\nps_manual_taxonomy %&gt;%\n  tax_fix() %&gt;%\n  tax_agg(\"genus_species\") %&gt;%\n  ps_seriate() %&gt;%\n  comp_barplot(tax_level = \"genus_species\", sample_order = \"asis\", n_taxa = 15) +\n  facet_wrap(vars(arm, time_point), scales = \"free_x\") +\n  theme(axis.text.x = element_blank(), \n        axis.ticks.x = element_blank(),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nThese plots are showing us that there is an increased diversity of microbes at onset until the end of the period. a diverse community of microbes can be detected at onset in the birth control group, in contrast, the group without birth control starts of with a shift of dominance from lacto to Atopobium, Gardnerella and Prevotella ,increased diversity is only observed later, at the end of bleeding.\nWhat else can you derive from the plot?\n\n#ps_manual_taxonomy %&gt;% ord_explore()\n\n\n\nDoes birth control change inflammation during menstruation?\n\nps_manual_taxonomy %&gt;% \n  subset_samples(., time_point %in% c(\"onset\")) %&gt;% \n  transform_sample_counts(., function(x) x/sum(x)) %&gt;% \n  subset_taxa(., Genus == \"Lactobacillus\") %&gt;% \n  plot_bar(x = \"pid\", fill = \"Species\") +\n  facet_wrap(.~arm, scales = \"free_x\") +\n  theme(axis.text.x = element_blank(),\n        legend.position = \"bottom\")+\n    labs(title = \"Lactobacillus at the onset of periods\")\n\n\n\n\n\n\n\n\n\n ps_manual_taxonomy %&gt;% \n  #subset_samples(., time_point %in% c(\"onset\")) %&gt;% \n  transform_sample_counts(., function(x) x/sum(x)) %&gt;% \n  subset_taxa(., Genus == \"Lactobacillus\") %&gt;% \n  plot_bar(x = \"pid\", fill = \"Species\") +\n  facet_wrap(vars(arm, time_point), scales = \"free_x\")+\n  #facet_wrap(.~arm, scales = \"free_x\") +\n  theme(axis.text.x = element_blank(),\n         legend.position = \"bottom\")+\n    labs(title = \"Lactobacillus throughout periods\")\n\n\n\n\n\n\n\n\n\nps_df&lt;-ps_manual_taxonomy %&gt;%\n    microViz::ps_melt() %&gt;% \n    group_by(Sample, pid, arm, time_point, sample_id, arm_timepoint, Kingdom, Phylum, Class, Order, Family, Genus, Species, genus_species) %&gt;%\n    summarise(count = sum(Abundance)) %&gt;%\n    group_by(Sample) %&gt;%\n    mutate(rel_abundance = count / sum(count)) %&gt;%\n    ungroup()\n\n`summarise()` has grouped output by 'Sample', 'pid', 'arm', 'time_point',\n'sample_id', 'arm_timepoint', 'Kingdom', 'Phylum', 'Class', 'Order', 'Family',\n'Genus', 'Species'. You can override using the `.groups` argument.\n\ncytokine_data &lt;- ps_manual_taxonomy %&gt;% \n  samdat_tbl() %&gt;%\n  select(.sample_name, sample_id, arm, time_point, arm_timepoint, starts_with(\"conc\"))\n\n\n\nCorrelation plots\n\nps_df%&gt;% # melted phyloseq\n  filter(Genus == \"Lactobacillus\") %&gt;%\n  left_join(cytokine_data, by = c(\"Sample\" = \".sample_name\", \"arm_timepoint\")) %&gt;%\n  pivot_longer(c(starts_with(\"conc\")), names_to = \"analyte\", values_to = \"conc\") %&gt;%\n  filter(!str_detect(analyte, \"log\")) %&gt;%\n  mutate(analyte = str_replace(analyte, \"conc_\", \"\")) %&gt;%\n  group_by(analyte) %&gt;%\n  mutate(cor = cor(rel_abundance, conc, method=\"spearman\") %&gt;% scales::label_number(accuracy = 0.01)()) %&gt;%\n  mutate(facet_label = str_c(analyte, \" *(ρ = \", cor, \")*\")) %&gt;%\n  ggplot(aes(x = rel_abundance, y = conc)) +\n  geom_point(aes(color = arm_timepoint)) +\n  scale_color_brewer(palette = \"Set2\", guide = NULL) +\n  scale_y_log10(labels=scales::label_log()) +\n  facet_wrap(vars(facet_label), scales=\"free_y\") +\n  theme_cowplot() + \n  background_grid() + \n  theme(axis.title.x = ggtext::element_markdown(), strip.text =  ggtext::element_markdown())\n\n\n\n\n\n\n\n\nHere, we learn that there is no correlation between the analytes (cytokines) and the abundance of lactobacillus. However, this might not be true at different time points, for example if we had grouped by analyte and time_point prior to calculating the correlation , then facet the plot by both analyte and time_point. Try it and see if theres any correlation at different time points.\n\n\nExploring within-sample diversity (Alpha Diversity)\nDoes your data contain singletons? Calculate alpha diversity using three different diversity measures Is there a difference in alpha diversity between the birth control and no birth control arms? How about across menstruation? Is the difference between arms statistically significant?\n\n\nSingletons\n\nsum(otu_table(filtered_physeq) == 1)\n\n[1] 2\n\n# not enough to calculate chao1 whichh requires unfiltered data and more single ASVs\n\n\nps_df&lt;-ps_manual_taxonomy %&gt;%\n    microViz::ps_melt() %&gt;% \n    group_by(Sample, pid, arm, time_point, sample_id, arm_timepoint, Kingdom, Phylum, Class, Order, Family, Genus, Species, genus_species) %&gt;%\n    summarise(count = sum(Abundance)) %&gt;%\n    group_by(Sample) %&gt;%\n    mutate(rel_abundance = count / sum(count)) %&gt;%\n    ungroup()\n\n`summarise()` has grouped output by 'Sample', 'pid', 'arm', 'time_point',\n'sample_id', 'arm_timepoint', 'Kingdom', 'Phylum', 'Class', 'Order', 'Family',\n'Genus', 'Species'. You can override using the `.groups` argument.\n\nmetadata&lt;- ps_manual_taxonomy %&gt;% \n  samdat_tbl() %&gt;%\n  select(.sample_name, sample_id, arm, time_point, arm_timepoint, starts_with(\"conc\")) %&gt;% \n  mutate(Sample=.sample_name)\n\n# merged_data&lt;-ps_df%&gt;% # melted phyloseq\n#   #filter(Genus == \"Lactobacillus\") %&gt;%\n#   left_join(metadata, by = c(\"Sample\", \"arm_timepoint\")) %&gt;%\n#   pivot_longer(c(starts_with(\"conc\")), names_to = \"analyte\", values_to = \"conc\") \n# \n\n\n# data &lt;- psmelt(ps_manual_taxonomy) %&gt;% \n#   select(\"pid\",\"Sample\",\"Genus\",\"Species\",\"Abundance\",\n#          \"genus_species\" , \"conc_IL.1a\":\"conc_MIP.3a\",\n#          \"time_point\", \"arm\", \"arm_timepoint\")\n# \n# # Group by pid and calculate the sum of abundance for each pid\n# grouped_data &lt;- data %&gt;%\n#   group_by(pid) %&gt;%\n#   summarise(total_abundance = sum(Abundance))\n# \n# # Merge back the total abundance with the original data\n# merged_data &lt;- data %&gt;%\n#   left_join(grouped_data, by = \"pid\") %&gt;%\n#   mutate(rel_abundance = Abundance / total_abundance) %&gt;% \n#   pivot_longer( \"conc_IL.1a\":\"conc_MIP.3a\",\n#                 names_to='cytokines', \n#                 values_to='concentration')\n\n\nmeasures &lt;- c(\"Observed\", \"Shannon\", \"Simpson\")\n\n# Calculate alpha diversity metrics\nalpha_div&lt;- estimate_richness(ps_manual_taxonomy, measures = measures)\n\n# create a Sample col using rownames\nalpha_div$Sample&lt;-rownames(alpha_div) \n\n# add my ps_melt object for additional variables \nalpha_div_df&lt;-alpha_div %&gt;% left_join(metadata) %&gt;% \n  pivot_longer(\n    Observed:Simpson,\n    names_to='metric',\n    values_to = 'score') \n\nJoining with `by = join_by(Sample)`\n\n# Plot alpha diversity metrics\nalpha_div_df %&gt;% distinct() %&gt;% \n  filter(time_point %in% c('onset', 'week_post') ) %&gt;% \nggplot(aes(x = arm, y = score)) +\n  geom_boxplot() +  \n  geom_quasirandom(stroke = FALSE, width = 0.3) + \n  labs(x = \"Alpha diversty metric\", y = \"Score\") +\n  #facet_wrap(~arm, scales = \"free_y\") +\n   facet_wrap(time_point~metric, scales = \"free_y\") +\n   stat_compare_means(method = \"wilcox.test\", paired = FALSE, label=\"p.signif\")+\n  theme(axis.text.x = element_text(angle= 90))\n\n\n\n\n\n\n\n\n\n\nHow about across menstruation?\n\n# Plot alpha diversity metrics\nalpha_div_df %&gt;% distinct() %&gt;% \n  #filter(time_point=='onset') %&gt;% \nggplot(aes(x = arm, y = as.numeric(score))) +\n  geom_boxplot() +  \n  labs(x = \"Alpha diversty metric\", y = \"Score\") +\n  #facet_wrap(~arm, scales = \"free_y\") +\n   facet_wrap(time_point~metric, scales = \"free_y\") +\n  stat_compare_means(aes(group=arm), method = \"wilcox.test\", paired = FALSE, label=\"p.signif\")+\n  theme(axis.text.x = element_text(angle= 90))\n\n\n\n\n\n\n\n\nHere we can observe that there is no significant difference in Alpha diversity of individuals who were on birth control and individuals who were not on birth control. This implies that changes that we seee in diversity may not be attributed to the use of contraceptives but those changes may be a result of other factors such as the the time relative to the menstruation period ( a week prior, onset,the end and a week post). # Beta diversity\n\nps_manual_taxonomy %&gt;%\n  tax_transform(trans = \"compositional\", rank = \"genus_species\") %&gt;%\n  dist_calc(dist = \"bray\") %&gt;%\n  ord_calc(method = \"PCoA\") %&gt;%\n  ord_plot(alpha = 0.6, size = 2) +\n  theme_classic(12) +\n  coord_fixed(0.7)\n\n\n\n\n\n\n\n\n\nps_manual_taxonomy %&gt;% \n  ordinate(., distance = \"bray\", binary = FALSE, method = \"MDS\") %&gt;% \n  plot_ordination(ps_manual_taxonomy , ., type = \"Sample\", color = \"time_point\") +\n  labs(title = \"Bray-Curtis\")\n\n\n\n\n\n\n\n\nSamples from the onset and end of bleeding have clustered together on the left, indicating that the relative abundancesn of taxa in these samples are similar compared to the rest of the samples. Samples from a week prior and week post bleeding have also clustered together. What could that indicate?\nAfter the completion of periods, the microbiome reverts back to the abundances of taxa it had prior to the onset of periods. #Create Microbiome heatmaps ##Annotate the heatmap with arms and/or timepoints\n\nps_manual_taxonomy %&gt;%\n    tax_sort(by = sum, at = \"Genus\", trans = \"compositional\", tree_warn = FALSE) %&gt;%\n    tax_transform(trans = \"compositional\", rank = \"Genus\") %&gt;%\n      # tax_transform(trans = \"clr\", zero_replace = \"halfmin\", chain = TRUE, rank = \"Genus_Species\") %&gt;%\n    comp_heatmap(samples = 1:51, taxa = 1:30, name = \"Proportions\", tax_seriation = \"Identity\")\n\n\n\n\n\n\n\n\n\ncols &lt;- distinct_palette(n = 2, add = NA)\nnames(cols) &lt;- unique(samdat_tbl(ps_manual_taxonomy)$arm)\ncols2&lt;- distinct_palette(n = 4, add = NA)\nnames(cols2)&lt;-unique(samdat_tbl(ps_manual_taxonomy)$time_point)\n\nps_manual_taxonomy %&gt;%\n  tax_transform(\"compositional\", rank = \"Class\") %&gt;%\n  comp_heatmap(\n    tax_anno = taxAnnotation(\n      Prev. = anno_tax_prev(bar_width = 0.3, size = grid::unit(1, \"cm\"))\n    ),\n    sample_anno = sampleAnnotation(\n      Arm = anno_sample(\"arm\"),\n      col = list(Arm = cols\n                 ), border = FALSE#,\n      #State2 = anno_sample_cat(\"time_point\", col = cols2)\n    )\n  )\n\n\n\n\n\n\n\n\n\ncols &lt;- distinct_palette(n = 2, add = NA)\nnames(cols) &lt;- unique(samdat_tbl(ps_manual_taxonomy)$arm)\n\nps_manual_taxonomy %&gt;%\n  # sort all samples by similarity\n  ps_seriate(rank = \"Class\", tax_transform = \"compositional\", dist = \"bray\") %&gt;% \n  # arrange the samples into arm groups\n  ps_arrange(arm) %&gt;% \n  tax_transform(\"compositional\", rank = \"Class\") %&gt;%\n  comp_heatmap(\n    tax_anno = taxAnnotation(\n      Prev. = anno_tax_prev(bar_width = 0.3, size = grid::unit(1, \"cm\"))\n    ),\n    sample_anno = sampleAnnotation(\n     Arm= anno_sample(\"arm\"),\n      col = list(Arm = cols), border = FALSE#,\n      #State2 = anno_sample_cat(\"DiseaseState\", col = cols)\n    ),\n    sample_seriation = \"Identity\" # suppress sample reordering\n  )\n\n\n\n\n\n\n\n\nHere we can observe that there is no apparent difference in the relative abundances of the top classes of species in individuals who were on birth control and individuals who were not on birth control. This is consistent with what we observed from the alpha diversity plots earlier.\n\n# cols &lt;- distinct_palette(n = 4, add = NA)\n# names(cols) &lt;- unique(samdat_tbl(ps_manual_taxonomy)$time_point)\n# \n# ps_manual_taxonomy %&gt;%\n#   # sort all samples by similarity\n#   ps_seriate(rank = \"Class\", tax_transform = \"compositional\", dist = \"bray\") %&gt;% \n#   # arrange the samples \n#   ps_arrange(time_point) %&gt;% \n#   tax_transform(\"compositional\", rank = \"Class\") %&gt;%\n#   comp_heatmap(\n#     tax_anno = taxAnnotation(\n#       Prev. = anno_tax_prev(bar_width = 0.3, size = grid::unit(1, \"cm\"))\n#     ),\n#     sample_anno = sampleAnnotation(\n#      Time= anno_sample(\"time_point\"),\n#       col = list(Time = cols), border = FALSE#,\n#       #State2 = anno_sample_cat(\"DiseaseState\", col = cols)\n#     ),\n#     sample_seriation = \"Identity\" # suppress sample reordering\n#   )\n\n\n# cols &lt;- distinct_palette(n = 4, add = NA)\n# names(cols) &lt;- unique(samdat_tbl(ps_manual_taxonomy)$time_point)\n# \n# ps_manual_taxonomy %&gt;%\n#   # sort all samples by similarity\n#   ps_seriate(rank = \"Class\", tax_transform = \"compositional\", dist = \"bray\") %&gt;%\n#   # arrange the samples into time_point groups\n#   ps_arrange(time_point) %&gt;%\n#   tax_transform(\"compositional\", rank = \"Class\") %&gt;%\n#   comp_heatmap(\n#     tax_anno = taxAnnotation(\n#       Prev. = anno_tax_prev(bar_width = 0.3, size = grid::unit(1, \"cm\"))\n#     ),\n#     sample_anno = sampleAnnotation(\n#       State1 = anno_sample(\"time_point\"),\n#       col = list(State1 = cols), border = FALSE\n#     ),\n#     sample_seriation = \"Identity\" # suppress sample reordering\n#   )\n\n\nnames(cytokine_data)\n\n [1] \".sample_name\"  \"sample_id\"     \"arm\"           \"time_point\"   \n [5] \"arm_timepoint\" \"conc_IL.1a\"    \"conc_IL.1b\"    \"conc_IL.6\"    \n [9] \"conc_TNFa\"     \"conc_MIG\"      \"conc_IFNg\"     \"conc_IP.10\"   \n[13] \"conc_MIP.3a\"  \n\nps_manual_taxonomy%&gt;%\n  tax_agg(\"Genus\") %&gt;%\n  cor_heatmap(taxa = tax_top(ps_manual_taxonomy, 15, by = max, rank= \"Genus\"),\n    vars = c(\n      \"conc_IL.1a\", \n      \"conc_IL.1b\",\n      \"conc_IL.6\",\n      \"conc_TNFa\",\n      \"conc_MIG\",\n      \"conc_IFNg\",\n      \"conc_IP.10\",\n      \"conc_MIP.3a\"))\n\n\n\n\n\n\n\n\nNegative correlation between lactobacillus and IL1_a as seen by the blue color. From Escherichia to Veillonella there is a trend of positive correlation with certain inflammatory markers,from ifn_g to il_6. In contrast , Lactobacillus has a negative correlation with the same set of markers. What other trends can you observe?"
  },
  {
    "objectID": "materials/2-workshop2/8-group-project/group_activity_1_yogurt_starter.html#goals",
    "href": "materials/2-workshop2/8-group-project/group_activity_1_yogurt_starter.html#goals",
    "title": "Yogurt Group Activity I starter code",
    "section": "Goals",
    "text": "Goals\n\nCreate a phyloseq object\nCreate relative abundance bar plots\nExplore within-sample diversity (Alpha Diversity)\n\n\n\n\nLoading in libraries and data 📚\nLoading in libraries. You may need to add more!\n\n\nlibrary(phyloseq)\nlibrary(microViz)\nlibrary(tidyverse)\n\nIdentify your project file paths, and read them in! Depending on where you created your Quarto project, this may vary. Make sure you change the following file paths to match your directory structure.\n\namplicon_ids &lt;- read_csv(\"data/Group A Dataset Yogurt/04_yogurt_amplicon_sample_ids.csv\")\nsample_id &lt;- read_csv(\"data/Group A Dataset Yogurt/00_sample_ids_yogurt.csv\")\nparticipant_metadata &lt;- read_csv(\"data/Group A Dataset Yogurt/01_participant_metadata_yogurt.csv\")\nqpcr &lt;- read_csv(\"data/Group A Dataset Yogurt/02_qpcr_results_yogurt.csv\")\nluminex &lt;- read_csv(\"data/Group A Dataset Yogurt/03_luminex_results_yogurt.csv\")\n\nThis loads in the count and tax table for the whole sequencing run, so there may be samples you don’t need!\n\nall_samples_count_table &lt;- readRDS(\"data/gv_seqtab_nobim.rds\")\nall_samples_tax_table &lt;- readRDS(\"data/gv_spetab_nobim_silva.rds\")\n\n\n\nCreating the phyloseq 🅿️\nTo speed up some data manipulation steps to get you to your analysis, I’ve provided some code to help you merge all the csvs together and create your phyloseq.\nIn a phyloseq object, we can put qPCR, cytokine data, and other sample metadata into the sample_data. We can create a yogurt_sample_data dataframe that will become the sample_data.\n\nyogurt_sample_data &lt;- amplicon_ids %&gt;%\n    left_join(sample_id %&gt;% select(-arm), by = c(\"pid\",\"time_point\")) %&gt;% #merges sample ids\n    left_join(participant_metadata %&gt;% select(-arm), by = \"pid\") %&gt;% #merges participant metadata\n    mutate(arm_timepoint = str_c(arm, time_point, sep = \"_\")) %&gt;%  #creating a arm and timepoint column for later plotting\n    mutate(arm_timepoint = fct_relevel(arm_timepoint, \"unchanged_diet_baseline\", \"unchanged_diet_after_antibiotic\",\"yogurt_baseline\",\"yogurt_after_antibiotic\")) %&gt;%  # sets the factor order of this column for plotting\n    mutate(time_point = fct_relevel(time_point, \"baseline\", \"after_antibiotic\")) %&gt;% # sets the factor order of this column for plotting\n    left_join(qpcr, by = \"sample_id\") %&gt;% # merges qpcr data\n    left_join(luminex %&gt;%  # merges luminex data \n        pivot_wider(names_from = cytokine, values_from = c(conc,limits)), by = \"sample_id\") %&gt;%      #will need to pivot the cyotokine data to longer to merge with sample data\n    column_to_rownames(\"amplicon_sample_id\")\n\n\n\n\n\n\n\nWhy did I pivot_wider?\n\n\n\n\n\n💭 Because the luminex csv was in long format, pivoting it to wider helps set each analyte as a column and makes each row an unique sample.\n\n\n\n\n\n\n\n\n\nWhich columns do I merge by?\n\n\n\n\n\n💭 How did I know which columns to merge by? This depends on the structure of your data and the task you’re performing. Usually you can choose columns that uniquely identify each row, and you can merge based on these columns. Some times, you need more than one!\n\n\n\nEarlier, I mentioned that there are samples from other projects in the count and tax table! Here we need to update the count and tax table to to reflect the correct ASVs and samples.\nExtracting a vector of yogurt dataset sample ids\n\nyogurt_ids &lt;- yogurt_sample_data %&gt;% \n              as.data.frame() %&gt;% \n              rownames_to_column(\"amplicon_sample_id\") %&gt;%\n              pull(amplicon_sample_id)\n\nRemoving unused ASVs from the count table\n\ncount_table &lt;- all_samples_count_table %&gt;%\n                as.data.frame() %&gt;%\n                rownames_to_column(\"amplicon_sample_id\") %&gt;%\n                filter(amplicon_sample_id %in% yogurt_ids) %&gt;% #filtering the count data by the yogurt ids\n                mutate_at(vars(-amplicon_sample_id), as.numeric) %&gt;%\n                column_to_rownames(\"amplicon_sample_id\") %&gt;%\n                select(where(~sum(.) != 0)) # removing unused ASVs - when the sum of a column is 0\n\nFiltering the taxa table so there are only ASVs for the yogurt sample\n\n#getting all asvs for the yogurt data\nasvs &lt;- colnames(count_table)\n#filtering the taxa table so there is only asvs for the yogurt samples\ntax_table &lt;- all_samples_tax_table %&gt;%\n              as.data.frame() %&gt;%\n              rownames_to_column(\"tax_table_asv\") %&gt;%\n              filter(tax_table_asv %in% asvs) %&gt;% # filters the tax table by the asvs we want\n              column_to_rownames(\"tax_table_asv\") %&gt;%\n              as.matrix()\n\nNow, we can create our phyloseq!\n\n#creating phyloseq object\nyogurt_ps &lt;- phyloseq(otu_table(count_table, taxa_are_rows=FALSE), \n               sample_data(yogurt_sample_data), \n               tax_table(tax_table))\n\nWarning: From here on out, there will be less helpful code!\n\n\nThese are questions that can be used as a guide. Looking at the data, think about what questions you can address.\n\n\nPhyloseq cleaning and diagnosis 🔍\n\nDescribe the structure of the experimental design\nRemove low-yield samples\nHow sparse is your count table?\nExplore the distribution of ASV lengths. Are there any you’d like to remove?\nAre there NAs in the tax table?\n\n\n\nPlot the relative abundance bar plots 📊\n\nGenerate plots to compare relative abudances across arms and time points\nWhat are some noticable differences after antibiotic treatment?\nDoes consumption of yogurt influence a patient’s microbiome?\n\nOptional questions (Examine your sample_data to see what other questions you can ask!):\n\nWhat cytokines correlated with Lactobaccilus?\nWhat cytokines correlate with certain taxa?\nHow do the absolute abundance of bacteria and Lactobacillus species change after antibiotic administration?\n\n\n\n\n\n\n\nIs your plot hard to see?\n\n\n\n\n\n💭 Consider adjusting the code chunk options!\n#| fig-width: 10\n#| fig-height: 10\nOr a number depending on your window size may help.\n\n\n\n\n\n\n\n\n\nChecking your plots or if you’re stuck!\n\n\n\n\n\n💭 Remember, ord_explore() is a very useful function within the microViz package that can build ordinations and relative abundance plots!\n\n\n\n\n\nExploring within-sample diversity (Alpha Diversity) 🎏\n\nDoes your data contain singletons?\nCalculate alpha diversity using three different diversity measures\nIs there a difference in alpha diversity between the yogurt and normal diet arms? How about before and after antibiotics?\nIs the difference between arms statistically significant?\n\n\n\nOrdination and Correlation\n\nCreate Microbiome heatmaps\nAnnotate the heatmap with arms and/or timepoints\nUsing ord_explore() or with code, generate ordination plots\nCreate a correlation heatmap of taxa and cytokines (optional)\n\n\n\n\n\n\n\nReferences\n\n\n\n\n\n💭 The microViz documentation can be helpful.\n\n\n\n\n\nShotgun Metagenomics\n\nCreate a phyloseq object using the VIRGO dataset\nTry applying Michael’s code!"
  },
  {
    "objectID": "materials/2-workshop2/8-group-project/liz_yogurt.html",
    "href": "materials/2-workshop2/8-group-project/liz_yogurt.html",
    "title": "Yogurt study data analysis",
    "section": "",
    "text": "Research Project A - Does yogurt consumption change the vaginal microbiome?\nAim To investigate whether the consumption of yogurt influences microbiome composition after antibiotics treatment.\nStudy Description This is a randomized controlled trial to study whether yogurt consumption has an effect on the vaginal microbiome post antibiotic treatment. 16S rRNA gene sequencing was done to characterize patient microbiome composition. Absolute abundance of bacteria (in gene copies / mL) was measured by 3 qPCR assays (for total, L. crispatus, L. iners). Cytokine levels (in ug/mL of vaginal fluid) were also measured by Luminex. Data was collected at two timepoints, pre- and post antibiotic treatment.\n\nLoad packages\n\n\nCode\nlibrary(here)\nlibrary(tidyverse)\nlibrary(phyloseq)\nlibrary(rstatix)\nlibrary(microViz)\nlibrary(ggpubr)\nlibrary(ggbeeswarm)\nlibrary(ggside)\nlibrary(vegan)\nlibrary(reshape2)\nlibrary(pander)\n\n\n\n\nManage paths\nShow path to the current project\n\nhere::here()\n\nCreate object containing the path to the data we intend to analyze\n\nyo_path &lt;- file.path(here(\"data/\"))\nyo_path\n\nList the files we intend to load\n\nlist.files(yo_path)\n\n\n\nLoad data\nLoad the sample-associated data\n\nsample_ids &lt;- read.csv(paste0(yo_path, \"00_sample_ids_yogurt.csv\"))\nparticipant_metadata &lt;- read.csv(paste0(yo_path, \"01_participant_metadata_yogurt.csv\"))\nqpcr &lt;- read.csv(paste0(yo_path, \"02_qpcr_results_yogurt.csv\"))\nluminex &lt;- read.csv(paste0(yo_path, \"03_luminex_results_yogurt.csv\"))\namplicon_ids &lt;- read.csv(paste0(yo_path, \"04_yogurt_amplicon_sample_ids.csv\"))\n\nLoad the count table and taxonomy table. These are the outputs of dada2.\n\nall_samples_count_table &lt;- readRDS(paste0(yo_path, \"gv_seqtab_nobim.rds\"))\nall_samples_tax_table &lt;- readRDS(paste0(yo_path, \"gv_spetab_nobim_silva.rds\"))\n\n\n\nPrepare sample data\nPrepare the sample-associated data by joining (adding) the sample ids, participant metadata, qpcr data, and luminex data to the amplicon ids. Adjust the factor levels for several variables.\n\nyogurt_sample_data &lt;- amplicon_ids %&gt;%\n  \n  left_join(sample_ids %&gt;% select(-arm), by = c(\"pid\", \"time_point\")) %&gt;% # joins sample ids\n  \n  left_join(participant_metadata %&gt;% select(-arm), by = \"pid\") %&gt;% # joins participant metadata\n  \n  left_join(qpcr, by = \"sample_id\") %&gt;% # joins qpcr data\n  \n  left_join(luminex %&gt;%  # pivots wider then joins the luminex data\n              pivot_wider(names_from = cytokine,\n                          values_from = c(conc,limits)), by = \"sample_id\") %&gt;% \n  \n  mutate(time_point = fct_relevel(time_point, \"baseline\", \"after_antibiotic\")) %&gt;% \n  \n  mutate(arm_timepoint = str_c(arm, time_point, sep = \"_\")) %&gt;%\n  mutate(arm_timepoint = fct_relevel(arm_timepoint,\n                                     \"unchanged_diet_baseline\",\n                                     \"unchanged_diet_after_antibiotic\",\n                                     \"yogurt_baseline\",\n                                     \"yogurt_after_antibiotic\"))\n\n\n\nQuick check sample data\n\n# What are the variables?\ncolnames(yogurt_sample_data)\n\n [1] \"amplicon_sample_id\"  \"amplicon_type\"       \"pid\"                \n [4] \"arm\"                 \"time_point\"          \"sample_id\"          \n [7] \"days_since_last_sex\" \"birth_control\"       \"age\"                \n[10] \"education\"           \"sex\"                 \"qpcr_bacteria\"      \n[13] \"qpcr_crispatus\"      \"qpcr_iners\"          \"conc_IL-1a\"         \n[16] \"conc_IL-10\"          \"conc_IL-1b\"          \"conc_IL-8\"          \n[19] \"conc_IL-6\"           \"conc_TNFa\"           \"conc_IP-10\"         \n[22] \"conc_MIG\"            \"conc_IFN-Y\"          \"conc_MIP-3a\"        \n[25] \"limits_IL-1a\"        \"limits_IL-10\"        \"limits_IL-1b\"       \n[28] \"limits_IL-8\"         \"limits_IL-6\"         \"limits_TNFa\"        \n[31] \"limits_IP-10\"        \"limits_MIG\"          \"limits_IFN-Y\"       \n[34] \"limits_MIP-3a\"       \"arm_timepoint\"      \n\n\n\n# Any missing data?\nsum(is.na(yogurt_sample_data))\n\n[1] 0\n\n\n\n# All IDs unique?\nlength(unique(yogurt_sample_data$amplicon_sample_id)) == nrow(yogurt_sample_data)\n\n[1] TRUE\n\n\n\n# Any incomplete cases?\nyogurt_sample_data %&gt;% \n  count(pid) %&gt;% filter(n != 2)\n\n[1] pid n  \n&lt;0 rows&gt; (or 0-length row.names)\n\n\n\n\nViz study design\n\nyogurt_sample_data %&gt;% \n  ggplot(aes(x = time_point, y = pid, color = age, shape = sex)) +\n  geom_point(size = 3, stroke = FALSE) +\n  facet_wrap(~arm, scales = \"free_y\") +\n  labs(title = \"VMB before & after ABX\")\n\n\n\n\n\n\n\n\n\n\nMake phyloseq object\n\nyogurt_ps &lt;- phyloseq(\n  \n  # Make the phyloseq object\n  sample_data(yogurt_sample_data %&gt;% column_to_rownames(var = \"amplicon_sample_id\")),\n  otu_table(all_samples_count_table, taxa_are_rows = FALSE),\n  tax_table(all_samples_tax_table)) %&gt;% \n  \n  # Remove empty ASVs\n  filter_taxa(., function(x) sum(x &gt; 0) &gt; 0, prune = TRUE)\n\n# Show a concise summary of the phyloseq object\nyogurt_ps\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1588 taxa and 102 samples ]\nsample_data() Sample Data:       [ 102 samples by 34 sample variables ]\ntax_table()   Taxonomy Table:    [ 1588 taxa by 7 taxonomic ranks ]\n\n\n\nmin(sample_sums(yogurt_ps))\n\n[1] 19\n\nmin(taxa_sums(yogurt_ps))\n\n[1] 1\n\n\n\n\nQuick check yields, ASVs\nDo we have any samples that returned very few reads?\n\nyield_df &lt;- data.frame(yield = sample_sums(yogurt_ps)) %&gt;% \n  rownames_to_column(var = \"amplicon_sample_id\") %&gt;% \n  arrange(yield)\n\nhead(yield_df, n = 3) %&gt;% pander()\n\n\n\n\n\n\n\n\namplicon_sample_id\nyield\n\n\n\n\nVAG0191\n19\n\n\nVAG0405\n35\n\n\nVAG0208\n7712\n\n\n\n\n\nYes, two samples returned very few reads.\nDo we see any arm- or time-point-associated bias in sequencing yield?\n\nyogurt_sample_data %&gt;% \n  left_join(yield_df, by = \"amplicon_sample_id\") %&gt;% \n  mutate(lo_yield = yield &lt; 100) %&gt;% \n  ggplot(aes(x = yield, fill = lo_yield)) +\n  geom_histogram(alpha = 0.6, bins = 10) +\n  facet_grid(arm ~ time_point)\n\n\n\n\n\n\n\n\nThese look fine. Two low-yield samples are in the unchanged_diet arm, after abx.\nAre there any ASVs we’d consider setting aside for now?\n\nprev_df &lt;- data.frame(\n  asv_sum = colSums(otu_table(yogurt_ps)),\n  asv_prev = colSums(otu_table(yogurt_ps) &gt; 0)) %&gt;% \n  rownames_to_column(var = \"asv_seq\")\n\nasv_df &lt;- tax_table(yogurt_ps) %&gt;% \n  data.frame() %&gt;% \n  rownames_to_column(var = \"asv_seq\") %&gt;% \n  left_join(prev_df, by = \"asv_seq\") %&gt;% \n  mutate(asv_len = nchar(asv_seq)) %&gt;% \n  arrange(desc(asv_sum))\n\nrm(prev_df)\n\n\nasv_df %&gt;% \n  ggplot(aes(x = asv_len, y = asv_prev, color = Kingdom)) +\n  geom_point(alpha = 0.5, size = 3, stroke = FALSE) +\n  labs(title = \"How prevalent are any length outliers?\",\n       x = \"ASV length\",\n       y = \"ASV prevalence\") + \n  scale_y_log10()\n\n\n\n\n\n\n\n\nBLASTed a few of the Euk ASVs. Other than Trichomomas vaginalis, which was observed in one sample, they look like host contamination. Let’s keep ASVs that fit the following criteria:\n\nkeep &lt;- asv_df %&gt;% \n  filter(asv_len %in% 390:450) %&gt;% \n  filter(Kingdom == \"Bacteria\") %&gt;% \n  filter(!is.na(Phylum)) %&gt;% \n  pull(asv_seq)\n\nnrow(asv_df) - length(keep)\n\n[1] 71\n\n\n\n\nClean phyloseq object\nSetting aside 71 length outliers, or non-Bacteria, or Bacteria with Phylum NA.\n\nyo_ps_clean &lt;- yogurt_ps %&gt;% \n  prune_taxa(keep, .) %&gt;%\n  prune_samples(sample_sums(.) &gt; 100, .) %&gt;% \n  filter_taxa(., function(x) sum(x &gt; 0) &gt; 0, prune = TRUE) # Removes empty ASVs (if any exist)\n\nyo_ps_clean\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1517 taxa and 100 samples ]\nsample_data() Sample Data:       [ 100 samples by 34 sample variables ]\ntax_table()   Taxonomy Table:    [ 1517 taxa by 7 taxonomic ranks ]\n\n\n\n\nPotential confounders?\nWe have two variables in our sample data that stand out (to me) as potential influencers of the vaginal microbiota - days_since_last_sex & birth_control. Are they well-balanced between arms; between timepoints?\nLet’s check the variable days_since_last_sex\n\nyogurt_sample_data %&gt;% \n  ggplot(aes(x = days_since_last_sex, fill = arm)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~time_point) +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\nNote: We have the same value for baseline and after_antibiotic. Was days_since_last_sex assessed only at baseline? Will need to follow-up with team as to how the study was conducted. Were the participants asked to abstain from sex during the study? For now I will assume that days_since_last_sex is relative to the baseline timepoint.\n\nyogurt_sample_data %&gt;% \n  filter(time_point == \"baseline\") %&gt;% \n  group_by(arm) %&gt;% \n  get_summary_stats(days_since_last_sex, type = \"five_number\") %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\narm\nvariable\nn\nmin\nmax\nq1\nmedian\nq3\n\n\n\n\nunchanged_diet\ndays_since_last_sex\n25\n1\n30\n3\n6\n10\n\n\nyogurt\ndays_since_last_sex\n26\n1\n48\n3\n5\n11\n\n\n\n\n\nThis variable, days_since_last_sex, seems reasonably well-balanced between arms.\nLet’s check the variable birth_control\n\nyogurt_sample_data %&gt;% \n  filter(time_point == \"after_antibiotic\") %&gt;% \n  count(arm, birth_control) %&gt;% pander()\n\n\n\n\n\n\n\n\n\narm\nbirth_control\nn\n\n\n\n\nunchanged_diet\nDepoprovera\n8\n\n\nunchanged_diet\nno hormonal birth control\n17\n\n\nyogurt\nDepoprovera\n17\n\n\nyogurt\nno hormonal birth control\n9\n\n\n\n\n\nThings are a bit unbalanced here. In the unchanged_diet arm, around 1/3 of participants use Depo, but in the yogurt arm around 2/3 of them do. We’ll keep this in mind as we perform our analysis. This variable could confound a simple comparison of unchanged_diet vs yogurt at timepoint after_antibiotic.\nAt the level of Genus, is there much difference in composition between participants using Depo vs Not using Depo?\n\nyo_ps_clean %&gt;%\n  ps_filter(time_point == \"baseline\") %&gt;%\n  tax_fix() %&gt;% \n  # tax_name(prefix = \"asv\", rank = \"Genus\") %&gt;% \n  # tax_names2rank(colname = \"Label\") %&gt;% \n  comp_barplot(tax_level = \"Genus\", n_taxa = 12) +\n  coord_flip() +\n  facet_wrap(.~birth_control, scales = \"free_y\")\n\n\n\n\n\n\n\n\nNothing too dramatic, it seems. Nonetheless, we’ll try to compare participants to themselves.\n\n\nqPCR results\nLet’s glimpse the qPCR data.\n\nyogurt_sample_data %&gt;% \n  \n  pivot_longer(qpcr_bacteria:qpcr_iners,\n               names_to = \"qpcr_tax\", values_to = \"qpcr_value\") %&gt;% \n  arrange(pid, arm, time_point) %&gt;% \n  \n  ggplot(aes(x = time_point, y = qpcr_value+1)) +\n  \n  geom_boxplot(alpha = 0.5, outlier.shape = NA) +\n  geom_line(aes(group = pid), alpha = 0.2) +\n  geom_point(alpha = 0.5, stroke = FALSE) +\n  \n  facet_grid(cols = vars(qpcr_tax), rows = vars(arm), scales = \"free\") +\n  scale_y_log10() + \n  stat_compare_means(paired = TRUE)\n\n\n\n\n\n\n\n\nPost antibiotics, bacterial load is down; L. crispatus is up (from none to some; more sporadic); and L. iners is up (from some to lots; more consistent). Maybe L. crispatus up more (often) in yogurt than in no-yogurt?\n\n\nPlot bars, especially Lactos\nLet’s fix L. crispatus as demonstrated in class\n\n# How many ways do we see crispatus?\ntax_table(yo_ps_clean) %&gt;% \n  data.frame() %&gt;% \n  filter(grepl(\"crispatus\", Species)) %&gt;% \n  pull(Species) %&gt;% unique()\n\n[1] \"crispatus\"                                             \n[2] \"acidophilus/casei/crispatus/gallinarum\"                \n[3] \"crispatus/gasseri/helveticus/johnsonii/kefiranofaciens\"\n[4] \"animalis/apodemi/crispatus/murinus\"                    \n\n\n\n# Make consistent\nps_yo_tax &lt;- yo_ps_clean %&gt;%\n  tax_fix() %&gt;%\n  tax_mutate(Species = case_when(\n    Species ==  \"acidophilus/casei/crispatus/gallinarum\" ~ \"crispatus\",\n    Species == \"crispatus/gasseri/helveticus/johnsonii/kefiranofaciens\" ~ \"crispatus\",\n    Species == \"animalis/apodemi/crispatus/murinus\" ~ \"crispatus\",\n    .default = Species)) %&gt;%\n    tax_mutate(Genus_species = str_c(Genus, Species, sep = \" \")) %&gt;%\n    tax_rename(rank = \"Genus_species\")\n\n\n# Plot bars Genus\nps_yo_tax %&gt;%\n  tax_fix() %&gt;%\n  tax_agg(\"Genus\") %&gt;%\n  ps_seriate() %&gt;% # This changes the order of the samples to be sorted by similarity\n  comp_barplot(tax_level = \"Genus\", sample_order = \"asis\", n_taxa = 11) +\n  facet_wrap(arm ~ time_point, scales=\"free_x\") +\n  theme(axis.text.x = element_text(size = 5, angle = 45, vjust = 1, hjust = 1))\n\n\n\n\n\n\n\n\nLooks like increase in prevalence of Lactobacillus dominance post antibiotic; little difference by diet\n\n# Plot bars Genus_species\nps_yo_tax %&gt;%\n  tax_fix() %&gt;%\n  tax_agg(\"Genus_species\") %&gt;%\n  ps_seriate() %&gt;% # This changes the order of the samples to be sorted by similarity\n  comp_barplot(tax_level = \"Genus_species\", sample_order = \"asis\", n_taxa = 11) +\n  facet_wrap(arm ~ time_point, scales=\"free_x\") +\n  theme(axis.text.x = element_text(size = 5, angle = 45, vjust = 1, hjust = 1))\n\n\n\n\n\n\n\n\nIt would be worth spending some time to refine and make consistent all of the Lactobacillus species-level calls. In the past I’ve done this via clustering against a reference. Put it on the to-do list.\n\n# Viz & test genus Lactobacillus\nlacto_df &lt;- yo_ps_clean %&gt;% \n  transform_sample_counts(., function(x) x/sum(x)) %&gt;% \n  tax_glom(., taxrank = \"Genus\") %&gt;% \n  subset_taxa(., Genus == \"Lactobacillus\") %&gt;% \n  otu_table() %&gt;% \n  data.frame() %&gt;% \n  rownames_to_column(var = \"amplicon_sample_id\")\n\ncolnames(lacto_df) &lt;- c(\"amplicon_sample_id\", \"Lactobacillus\")\n\nlacto_df %&gt;% \n  left_join(yogurt_sample_data, by = \"amplicon_sample_id\") %&gt;% \n  group_by(pid) %&gt;% filter(n() == 2) %&gt;% ungroup() %&gt;% \n  arrange(pid, arm, time_point) %&gt;% \n  ggplot(aes(x = time_point, y = Lactobacillus)) +\n  geom_boxplot(alpha = 0.5, outlier.shape = NA) +\n  geom_line(aes(group = pid), alpha = 0.2) +\n  geom_point(alpha = 0.5, stroke = FALSE) +\n  facet_wrap(.~arm, scales = \"free_y\") + \n  theme(aspect.ratio = 1) +\n  stat_compare_means(paired = TRUE)\n\n\n\n\n\n\n\n\nIn both arms, the relative abundance of the genus Lactobacillus was significantly higher post antibiotics.\n\n# Viz & test L. iners\nlin_df &lt;- yo_ps_clean %&gt;% \n  transform_sample_counts(., function(x) x/sum(x)) %&gt;% \n  tax_glom(., taxrank = \"Species\") %&gt;% \n  subset_taxa(., Species == \"iners\") %&gt;% \n  otu_table() %&gt;% \n  data.frame() %&gt;% \n  rownames_to_column(var = \"amplicon_sample_id\")\n\ncolnames(lin_df) &lt;- c(\"amplicon_sample_id\", \"Lactobacillus_iners\")\n\nlin_df %&gt;% \n  left_join(yogurt_sample_data, by = \"amplicon_sample_id\") %&gt;% \n  group_by(pid) %&gt;% filter(n() == 2) %&gt;% ungroup() %&gt;% \n  arrange(pid, arm, time_point) %&gt;% \n  ggplot(aes(x = time_point, y = Lactobacillus_iners)) +\n  geom_boxplot(alpha = 0.5, outlier.shape = NA) +\n  geom_line(aes(group = pid), alpha = 0.2) +\n  geom_point(alpha = 0.5, stroke = FALSE) +\n  facet_wrap(.~arm, scales = \"free_y\") + \n  theme(aspect.ratio = 1) +\n  stat_compare_means(paired = TRUE)\n\n\n\n\n\n\n\n\nThis seems largely driven by increases in Lactobacillus iners\n\n# Viz & test L. crispatus\nlcr_df &lt;- yo_ps_clean %&gt;% \n  transform_sample_counts(., function(x) x/sum(x)) %&gt;% \n  tax_glom(., taxrank = \"Species\") %&gt;% \n  subset_taxa(., grepl(\"crispatus\", Species)) %&gt;% \n  otu_table() %&gt;% \n  data.frame() %&gt;% \n  rownames_to_column(var = \"amplicon_sample_id\") %&gt;% \n  rowwise() %&gt;% \n  mutate(Lactobacillus_crispatus = sum(c_across(2:5))) %&gt;% \n  select(amplicon_sample_id, Lactobacillus_crispatus)\n\nlcr_df %&gt;% \n  drop_na() %&gt;% \n  left_join(yogurt_sample_data, by = \"amplicon_sample_id\") %&gt;% \n  group_by(pid) %&gt;% filter(n() == 2) %&gt;% ungroup() %&gt;% \n  arrange(pid, arm, time_point) %&gt;% \n  ggplot(aes(x = time_point, y = Lactobacillus_crispatus)) +\n  geom_boxplot(alpha = 0.5, outlier.shape = NA) +\n  geom_line(aes(group = pid), alpha = 0.2) +\n  geom_point(alpha = 0.5, stroke = FALSE) +\n  facet_wrap(.~arm, scales = \"free_y\") + \n  theme(aspect.ratio = 1) +\n  stat_compare_means(paired = TRUE)\n\n\n\n\n\n\n\n\nAnd less so by increases in Lactobacillus crispatus\n\n\nCytokines\nIdentify cytokines with most measurements within limits\n\nyogurt_sample_data %&gt;% \n  select(starts_with(\"limits\")) %&gt;%\n  summarise(across(starts_with(\"limits\"), ~ sum(.x == \"within limits\", na.rm = TRUE))) %&gt;% \n  t() %&gt;% data.frame() %&gt;% \n  rownames_to_column(var = \"cytokine\") %&gt;% \n  rename(samples_within_limits = \".\") %&gt;% \n  arrange(desc(samples_within_limits)) %&gt;% pander()\n\n\n\n\n\n\n\n\ncytokine\nsamples_within_limits\n\n\n\n\nlimits_IL-1a\n102\n\n\nlimits_IP-10\n99\n\n\nlimits_IL-8\n98\n\n\nlimits_MIG\n89\n\n\nlimits_MIP-3a\n78\n\n\nlimits_IL-1b\n28\n\n\nlimits_IL-6\n25\n\n\nlimits_TNFa\n2\n\n\nlimits_IFN-Y\n1\n\n\nlimits_IL-10\n0\n\n\n\n\n\nLet’s go with these; at least 75% of measurements were “within the limits” of detection\n\nkines &lt;- paste0(\"conc_\", c(\"IL-1a\", \"IP-10\", \"IL-8\", \"MIG\", \"MIP-3a\"))\n\n\nyogurt_sample_data %&gt;% \n  pivot_longer(all_of(kines), names_to = \"kine\", values_to = \"conc\") %&gt;% \n  ggplot(aes(x = conc, fill = time_point)) + \n  geom_histogram(bins = 30) +\n  facet_wrap(~kine) +\n  scale_x_log10()\n\n\n\n\n\n\n\n\nNoticing a subtle shift with abx: Down for IL-1a and IL-8; up for IP-10 and MIG\n\n\nAlpha diversity\n\n# Any singletons?\nsum(otu_table(yo_ps_clean) == 1)\n\n[1] 3\n\n\nOwing to very few singletons, we’ll exclude Chao1 and ACE\nCalculate measures & add sample data\n\nmeasures &lt;- c(\"Shannon\", \"Simpson\", \"InvSimpson\", \"Fisher\")\n\nalpha_df &lt;- yo_ps_clean %&gt;%\n  estimate_richness(measures = measures) %&gt;% \n  rownames_to_column(var = \"amplicon_sample_id\") %&gt;% \n  left_join(yogurt_sample_data, by = \"amplicon_sample_id\")\n\nalpha_df %&gt;% \n  select(1:9) %&gt;% \n  head(n = 3) %&gt;% pander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\n\namplicon_sample_id\nShannon\nSimpson\nInvSimpson\nFisher\namplicon_type\n\n\n\n\nVAG0049\n1.738\n0.7194\n3.564\n5.664\nvaginal\n\n\nVAG0052\n2.889\n0.9084\n10.92\n7.494\nvaginal\n\n\nVAG0053\n1.013\n0.5593\n2.269\n1.637\nvaginal\n\n\n\n\n\n\n\n\n\n\n\npid\narm\ntime_point\n\n\n\n\npid_24\nunchanged_diet\nbaseline\n\n\npid_36\nunchanged_diet\nbaseline\n\n\npid_02\nunchanged_diet\nbaseline\n\n\n\n\n\nHow many complete cases?\n\nalpha_df %&gt;% \n  group_by(pid) %&gt;% filter(n() == 2) %&gt;% ungroup() %&gt;%\n  count(time_point)\n\n# A tibble: 2 × 2\n  time_point           n\n  &lt;fct&gt;            &lt;int&gt;\n1 baseline            49\n2 after_antibiotic    49\n\n\nPlot and test for antibiotic-associated change\n\nalpha_df %&gt;% \n  group_by(pid) %&gt;% filter(n() == 2) %&gt;% ungroup() %&gt;%\n  arrange(pid, time_point) %&gt;% \n  pivot_longer(cols = all_of(measures),\n               names_to = \"adiv_measure\",\n               values_to = \"adiv_value\") %&gt;% \n  ggplot(aes(x = time_point, y = adiv_value)) +\n  geom_boxplot(alpha = 0.5, outlier.shape = NA) +\n  geom_line(aes(group = pid), alpha = 0.2) +\n  geom_point(alpha = 0.5, stroke = FALSE) +\n  facet_wrap(.~adiv_measure, scales = \"free_y\") + \n  theme(aspect.ratio = 1) +\n  stat_compare_means(paired = TRUE)\n\n\n\n\n\n\n\n\nResult: Antibiotic treatment was associated with a modest decrease in vaginal bacterial diversity (Shannon diversity index, Wilcoxon signed rank test, n = 49 participants, P = 0.006). This result was robust to choice of alpha diversity metric (Supplemental Figure 1).\nHow many complete cases per arm?\n\nalpha_df %&gt;% \n  group_by(pid) %&gt;% filter(n() == 2) %&gt;% ungroup() %&gt;%\n  count(arm, time_point) %&gt;% pander()\n\n\n\n\n\n\n\n\n\narm\ntime_point\nn\n\n\n\n\nunchanged_diet\nbaseline\n23\n\n\nunchanged_diet\nafter_antibiotic\n23\n\n\nyogurt\nbaseline\n26\n\n\nyogurt\nafter_antibiotic\n26\n\n\n\n\n\nPlot and test for yogurt modulation of antibiotic-associated change\n\nalpha_df %&gt;% \n  group_by(pid) %&gt;% filter(n() == 2) %&gt;% ungroup() %&gt;%\n  pivot_longer(all_of(measures), names_to=\"metric\", values_to=\"value\") %&gt;%\n  select(pid, time_point, arm, metric, value) %&gt;% \n  pivot_wider(names_from = time_point, values_from = value) %&gt;% \n  mutate(diff_alpha = after_antibiotic - baseline) %&gt;% \n  ggplot(aes(x = arm, y = diff_alpha)) +\n  geom_boxplot(alpha = 0.4, outlier.shape = NA) +\n  geom_quasirandom(stroke = FALSE, width = 0.2) +\n  facet_wrap(.~metric, scales = \"free_y\") + \n  theme(aspect.ratio = 1) +\n  stat_compare_means(paired = FALSE)\n\n\n\n\n\n\n\n\nResult: Antibiotic-associated changes in vaginal bacterial diversity were similar between participants who consumed yogurt and those who did not (change in Shannon diversity index, Wilcoxon rank sum test, N = 26 yogurt consumers vs 23 with unchanged diet, P = 0.82). This result was robust to choice of alpha diversity metric (Supplemental Figure 2).\n\n\nBeta diversity\nLet’s ordinate using some microViz recipes\n\n# Using robust.aitchison distance, MDS (PCoA) ordination, and displaying side panel boxplots\nset.seed(123)\n\nps_yo_tax %&gt;%\n  tax_fix() %&gt;% \n  tax_transform(\"identity\", rank = \"Genus\") %&gt;%\n  dist_calc(dist = \"robust.aitchison\") %&gt;%\n  ord_calc(\"PCoA\") %&gt;%\n  ord_plot(color = \"time_point\", shape = \"arm\", size = 2) +\n  scale_colour_brewer(palette = \"Dark2\", aesthetics = c(\"fill\", \"colour\")) +\n  theme_bw() +\n  ggside::geom_xsideboxplot(aes(fill = time_point, y = time_point), orientation = \"y\") +\n  ggside::geom_ysideboxplot(aes(fill = time_point, x = time_point), orientation = \"x\") +\n  ggside::scale_xsidey_discrete(labels = NULL) +\n  ggside::scale_ysidex_discrete(labels = NULL) +\n  ggside::theme_ggside_void()\n\n\n\n\n\n\n\n\n\n# Using bray-curtis distance, MDS (PCoA) ordination, and displaying side panel density plots\nset.seed(123)\n\nps_yo_tax %&gt;%\n  tax_fix() %&gt;% \n  tax_transform(\"identity\", rank = \"Genus\") %&gt;%\n  dist_calc(dist = \"bray\") %&gt;%\n  ord_calc(\"PCoA\") %&gt;%\n  ord_plot(color = \"time_point\", shape = \"arm\", size = 2) +\n  scale_colour_brewer(palette = \"Dark2\", aesthetics = c(\"fill\", \"colour\")) +\n  theme_bw() + \n  ggside::geom_xsidedensity(aes(fill = time_point), alpha = 0.5, show.legend = FALSE) +\n  ggside::geom_ysidedensity(aes(fill = time_point), alpha = 0.5, show.legend = FALSE) +\n  ggside::theme_ggside_void()\n\n\n\n\n\n\n\n\nAlong axis 1, there’s a subtle shift (in the density of samples) associated with antibiotic treatment, with more “after” samples clustering on left.\nNow let’s answer our question about whether diet (yogurt) modulates response of vaginal microbiota to antibiotics.\nGet a distance for each person’s pair of samples; we’ll ask: are distances different for those who ate yogurt vs those who didn’t?\n\n# Pick some distance metrics\ndistances &lt;- c(\"bray\", \"canberra\", \"euclidean\", \"hellinger\", \"kulczynski\", \"manhattan\", \"morisita\", \"robust.aitchison\")\n\n# Get counts\nyo_tab &lt;- yo_ps_clean %&gt;% otu_table()\n\n# Prepare sample data\nsam1 &lt;- yogurt_sample_data %&gt;% select(amplicon_sample_id, pid, arm) %&gt;%\n  dplyr::rename(sam1 = amplicon_sample_id, pid1 = pid, arm1 = arm)\n\nsam2 &lt;- yogurt_sample_data %&gt;% select(amplicon_sample_id, pid, arm) %&gt;%\n  dplyr::rename(sam2 = amplicon_sample_id, pid2 = pid, arm2 = arm)\n\n\n# Get distances\nresult &lt;- vector(\"list\")\n\nfor (d in distances) {\n  \n# Get distance matrix\n# This contains a distance for every pair of samples\ndist &lt;- vegdist(x = yo_tab, method = d)  \n\n# Convert to regular matrix\nd_mat &lt;- as(dist, \"matrix\")\n\n# Replace upper tri and diag w/ NA\nd_mat[upper.tri(d_mat, diag = TRUE)] &lt;- NA\n\n# Melt to columns and drop NA\nd_col &lt;- setNames(melt(d_mat, as.is = TRUE), c(\"sam1\", \"sam2\", \"dist\")) %&gt;% \n  drop_na() %&gt;% mutate(metric = d)\n\n# Add sample data\nd_col &lt;- left_join(d_col, sam1, by = \"sam1\") %&gt;% left_join(sam2, by = \"sam2\")\n\n# Filter to same-participant distances\n# This contains a distance for only those pairs we're interested in\n# For each person, the distance between their pre- and post-abx\nd_wi &lt;- d_col %&gt;% filter(pid1 == pid2)\n\nresult[[d]] &lt;- d_wi\n\n}\n\n\n# Plot and test\nbind_rows(result) %&gt;% \n  ggplot(aes(x = arm1, y = dist)) +\n  geom_boxplot(alpha = 0.4, outlier.shape = NA) +\n  geom_quasirandom(stroke = FALSE, width = 0.2) +\n  facet_wrap(.~metric, nrow = 2, scales = \"free_y\") + \n  stat_compare_means(paired = FALSE) +\n  labs(x = \"\", y = \"Distance between pre-abx and post-abx sample\")\n\n\n\n\n\n\n\n\nResult: Turnover in bacterial community composition was similar between antibiotic-treated participants who consumed yogurt and those who did not (Bray-Curtis distance, Wilcoxon rank sum test, N = 26 yogurt consumers vs 23 with unchanged diet, P = 0.9). This result was robust to choice of distance metric (Supplemental Figure 3).\n\n\nRDA\nLet’s explore this microViz recipe for redundancy analysis (RDA).\nRDA is a constrained ordination method. “Constrained” means that the ordination (in this case, PCA) only shows the community variation that can be explained by/associated with the external environmental/clinical variables (these are called the “constraints”).\nFirst let’s constrain by time_point and arm\n\nyo_ps_clean %&gt;%\n  ps_mutate(\n    after_abx = as.numeric(time_point == \"after_antibiotic\"),\n    yogurt = as.numeric(arm == \"yogurt\")\n  ) %&gt;%\n  tax_fix() %&gt;% \n  tax_transform(\"clr\", rank = \"Genus\") %&gt;%\n  ord_calc(\n    constraints = c(\"after_abx\", \"yogurt\"),\n    method = \"RDA\",\n    scale_cc = FALSE\n  ) %&gt;%\n  ord_plot(\n    colour = \"time_point\", size = 2, alpha = 0.5, shape = \"arm\",\n    plot_taxa = 1:10\n  )\n\n\n\n\n\n\n\n\nGood, makes sense given other findings. Look at the axes - how much variation explained? Less than in our unconstrained analyses shown above. So we might say that the variation associated with antibiotic exposure is just a (small?) part of the total variation observed among samples.\nNow let’s examine birth control and recent sex\n\nyo_ps_clean %&gt;%\n  ps_mutate(\n    depo = as.numeric(birth_control == \"Depoprovera\"),\n    recent_sex = as.numeric(days_since_last_sex &lt; 7)\n  ) %&gt;%\n  tax_fix() %&gt;% \n  tax_transform(\"clr\", rank = \"Genus\") %&gt;%\n  ord_calc(\n    constraints = c(\"depo\", \"recent_sex\"),\n    method = \"RDA\",\n    scale_cc = FALSE\n  ) %&gt;%\n  ord_plot(\n    colour = \"time_point\", size = 2, alpha = 0.5, shape = \"arm\",\n    plot_taxa = 1:10\n  )\n\n\n\n\n\n\n\n\nMaybe not a lot of signal here.\nLet’s try with cytokines as constraints\n\nyo_ps_clean %&gt;%\n  # ps_mutate(\n  #   conc_IL.1a = log10(conc_IL.1a),\n  #   conc_IP.10 = log10(conc_IP.10),\n  #   conc_IL.8 = log10(conc_IL.8),\n  #   conc_MIG = log10(conc_MIG)\n  # ) %&gt;%\n  tax_fix() %&gt;% \n  tax_transform(\"clr\", rank = \"Genus\") %&gt;%\n  ord_calc(\n    constraints = c(\"conc_IL.1a\", \"conc_IP.10\", \"conc_IL.8\", \"conc_MIG\"),\n    method = \"RDA\",\n    scale_cc = FALSE\n  ) %&gt;%\n  ord_plot(\n    colour = \"time_point\", size = 2, alpha = 0.5, shape = \"arm\",\n    plot_taxa = 1:12\n  )\n\n\n\n\n\n\n\n\nInteresting, IL-1a and IL-8 associated with diverse anaerobes.\nFinally, let’s look at antibiotic, IL-1a, IP-10 & IL-8\n\nyo_ps_clean %&gt;%\n  ps_mutate(\n    after_abx = as.numeric(time_point == \"after_antibiotic\"),\n  ) %&gt;%\n  tax_fix() %&gt;% \n  tax_transform(\"clr\", rank = \"Genus\") %&gt;%\n  ord_calc(\n    constraints = c(\"after_abx\", \"conc_IL.1a\", \"conc_IP.10\", \"conc_IL.8\"),\n    method = \"RDA\",\n    scale_cc = FALSE\n  ) %&gt;%\n  ord_plot(\n    colour = \"time_point\", size = 2, alpha = 0.5, shape = \"arm\",\n    plot_taxa = 1:15\n  )\n\n\n\n\n\n\n\n\nSo we’re beginning to build a sense of how vaginal microbiota, vaginal cytokines, and antibiotic treatment relate to each other. We might suggest that antibiotic treatment was associated with an increase in the prevalence of Lactobacillus dominance, possibly accompanied by a reduction (or shift) in the levels (or type) of inflammation.\nThat’s it for now! 🤓\n\n\nOther\nCorrelation heatmaps in microViz\n\nps_manual_taxonomy &lt;- yo_ps_clean %&gt;%\n  tax_fix() %&gt;%\n  tax_mutate(Species = case_when(\n    Species ==  \"acidophilus/casei/crispatus/gallinarum\" ~ \"crispatus\",\n    Species == \"crispatus/gasseri/helveticus/johnsonii/kefiranofaciens\" ~ \"crispatus\",\n    Species == \"animalis/apodemi/crispatus/murinus\" ~ \"crispatus\",\n    .default = Species)) %&gt;%\n    #also remake genus_species to fix those taxa\n    tax_mutate(genus_species = str_c(Genus, Species, sep = \" \")) %&gt;%\n    tax_rename(rank = \"genus_species\")\n\n\nyo_ps_clean %&gt;%\n  tax_filter(min_prevalence = 3, min_total_abundance = 3) %&gt;%\n  tax_fix() %&gt;%\n  tax_agg(\"Phylum\") %&gt;%\n  cor_heatmap(taxa = tax_top(yo_ps_clean, 8, by = max, rank = \"Phylum\"),\n              vars = c(\"conc_IL.1a\", \"conc_IP.10\", \"conc_IL.8\", \"conc_MIG\", \"conc_MIP.3a\"),\n              cor = \"spearman\")\n\n\n\n\n\n\n\n\n\nlibrary(\"ComplexHeatmap\")\n\nLoading required package: grid\n\n\n========================================\nComplexHeatmap version 2.18.0\nBioconductor page: http://bioconductor.org/packages/ComplexHeatmap/\nGithub page: https://github.com/jokergoo/ComplexHeatmap\nDocumentation: http://jokergoo.github.io/ComplexHeatmap-reference\n\nIf you use it in published research, please cite either one:\n- Gu, Z. Complex Heatmap Visualization. iMeta 2022.\n- Gu, Z. Complex heatmaps reveal patterns and correlations in multidimensional \n    genomic data. Bioinformatics 2016.\n\n\nThe new InteractiveComplexHeatmap package can directly export static \ncomplex heatmaps into an interactive Shiny app with zero effort. Have a try!\n\nThis message can be suppressed by:\n  suppressPackageStartupMessages(library(ComplexHeatmap))\n========================================\n\n# data(\"ibd\", package = \"microViz\")\npsq &lt;- tax_filter(yo_ps_clean, min_prevalence = 5)\npsq &lt;- tax_mutate(psq, Species = NULL)\npsq &lt;- tax_fix(psq)\npsq &lt;- tax_agg(psq, rank = \"Phylum\")\ntaxa &lt;- tax_top(psq, n = 15, rank = \"Phylum\")\nsamples &lt;- phyloseq::sample_names(psq)\n\nset.seed(42) # random colours used in first example\n# sampleAnnotation returns a function that takes data, samples, and which\nfun &lt;- sampleAnnotation(\n  gap = grid::unit(2.5, \"mm\"),\n  Time = anno_sample_cat(var = \"time_point\", col = 1:2),\n  Arm = anno_sample_cat(var = \"arm\", col = 3:4),\n  Bc = anno_sample_cat(var = \"birth_control\", col = 5:6)\n)\n\n# manually specify the sample annotation function by giving it data etc.\nheatmapAnnoFunction &lt;- fun(.data = psq, .side = \"top\", .samples = samples)\n\nWarning in (function (x, which, renamer = identity, col = distinct_palette(), :\ncoercing non-character anno_cat annotation data to character\n\n# draw the annotation without a heatmap, you will never normally do this!\ngrid.newpage()\nvp &lt;- viewport(width = 0.65, height = 0.75)\npushViewport(vp)\ndraw(heatmapAnnoFunction)\npushViewport(viewport(x = 0.7, y = 0.6))\ndraw(attr(heatmapAnnoFunction, \"Legends\"))\n\n\n\n\n\n\n\n\n\nyo_ps_clean %&gt;%\n  ps_arrange(time_point) %&gt;% \n  tax_transform(\"compositional\", rank = \"Phylum\") %&gt;%\n  comp_heatmap(\n    tax_anno = taxAnnotation(\n      Prev. = anno_tax_prev(bar_width = 0.3, size = grid::unit(1, \"cm\"))),\n    sample_anno = fun, sample_seriation = \"Identity\")\n\nWarning in (function (x, which, renamer = identity, col = distinct_palette(), :\ncoercing non-character anno_cat annotation data to character"
  },
  {
    "objectID": "materials/3-workshop3/0-welcome/index.html",
    "href": "materials/3-workshop3/0-welcome/index.html",
    "title": "Welcome to the workshop",
    "section": "",
    "text": "Get to know your neighbors and instructors and learn what to expect from this workshop",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 0:</b> Welcome to the workshop"
    ]
  },
  {
    "objectID": "materials/3-workshop3/0-welcome/index.html#slides",
    "href": "materials/3-workshop3/0-welcome/index.html#slides",
    "title": "Welcome to the workshop",
    "section": "Slides",
    "text": "Slides\nMake slides full screen",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 0:</b> Welcome to the workshop"
    ]
  },
  {
    "objectID": "materials/3-workshop3/1-viral-sequencing/index.html",
    "href": "materials/3-workshop3/1-viral-sequencing/index.html",
    "title": "Building Sucessful Sequencing Libraries",
    "section": "",
    "text": "Download Slides",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 1</b>: Preparing libraries for viromics"
    ]
  },
  {
    "objectID": "materials/3-workshop3/1-viral-sequencing/index.html#slides",
    "href": "materials/3-workshop3/1-viral-sequencing/index.html#slides",
    "title": "Building Sucessful Sequencing Libraries",
    "section": "",
    "text": "Download Slides",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 1</b>: Preparing libraries for viromics"
    ]
  },
  {
    "objectID": "materials/3-workshop3/3-sequence-files-quality-adapters/index.html",
    "href": "materials/3-workshop3/3-sequence-files-quality-adapters/index.html",
    "title": "Quality filtering of Reads",
    "section": "",
    "text": "Download Manual\n\n\n#################################################################################################################################\nAll the commands from the tutorial, organized by section. You can execute these commands sequentially in your terminal\n#################################################################################################################################\n#Common Command-Line Tools for FASTQ Files\n#Ensure all necessary tools are installed on system. You can also install them using package managers like apt, brew,\n\n#    grep: Search for patterns/sudo apt-get install grep\n#    awk: Perform text processing/sudo apt-get install gawk\n#    sed: Edit text in a stream/sudo apt-get install sed\n#    seqkit: A versatile toolkit for FASTA/FASTQ files/sudo apt-get install seqkit\n#    fastp: Quality control and adapter trimming/sudo apt-get install fastp\n#    cutadapt: Adapter trimming/sudo apt-get install cutadapt/ pip install cutadapt\n#    seqtk: FASTQ manipulation/sudo apt-get install seqtk\n\necho \"Running this script will make your life easier... unless you forgot to install seqkit. Then it's just pain. 😆\"\n\n\n            ########################################################################################################\n            #                                           &lt;&lt;&lt; PART A &gt;&gt;&gt;                                           #\n            ########################################################################################################\n\n# -----------------------------\n# Step 1: Understanding the FASTQ File Format\n# -----------------------------\n#Find the size of FASTQ files \nls -lh 1001.fastq\n\n# View the first 8 lines (2 records)\nhead -8 1001.fastq\n\n# Count the total number of records\nwc -l 1001.fastq | awk '{print $1/4}'\n\n# Search for sequences containing \"GATTTG\"\ngrep -B 1 -A 2 \"GATTTG\" 1001.fastq\n#| echo: true\n#| eval: false\n\n# -----------------------------\n# Step 2: Filtering FASTQ Records\n# -----------------------------\n# Filter sequences with average quality &gt; 30\nseqtk seq -q 30 -Q 33 1001.fastq &gt; filtered.fastq\n#| echo: true\n#| eval: false\n\n# -----------------------------\n# Step 3: Trimming Adapters\n# -----------------------------\n# Search for adapter sequence \"GGCCTCAGTGAAGGTCTCCTGCAAG\"\ngrep -A 1 \"GGCCTCAGTGAAGGTCTCCTGCAAG\" 1001.fastq | less\n\n# Trim adapters using cutadapt\ncutadapt -a grep -A 1 GGCCTCAGTGAAGGTCTCCTGCAAG 1001.fastq | less -o trimmed.fastq 1001.fastq\n\n#For paired Reads\ncutadapt -a GGCCTCAGTGAAGGTCTCCTGCAAG -A REVERSE_ADAPTER -o trimmed_R1.fastq -p trimmed_R2.fastq file_R1.fastq file_R2.fastq\n\n# Trim adapters and perform quality control using fastp\nfastp -i 1001.fastq -o trimmed.fastq -a GGCCTCAGTGAAGGTCTCCTGCAAG\n#| echo: true\n#| eval: false\n\n# -----------------------------\n# Step 4: Advanced Adapter Identification\n# -----------------------------\n\n# De novo adapter detection using FastQC\nfastqc 1001.fastq\n\n# Plot adapter content using fastp\nfastp -i 1001.fastq -o output.fastq --detect_adapter_for_pe --html adapter_report.html\n#| echo: true\n#| eval: false\n\n# -----------------------------\n# Step 4: Manipulating FASTQ Files\n# -----------------------------\n\n# Convert FASTQ to FASTA\nseqtk seq -A 1001.fastq &gt; 1001.fasta\n\n# Split a large FASTQ file into smaller files (e.g., 1 million reads each)\nseqkit split2 -p 10 1001.fastq\n\n# Combine two FASTQ files (e.g., paired-end reads)\ncat file1.fastq file2.fastq &gt; merged.fastq\n\n# -----------------------------\n# Step 4: Extract Subsets of Reads\n# -----------------------------\n\n# Extract reads longer than 50 bp using awk\nawk 'NR%4==2 && length($0) &gt; 50 {print $0}' 1001.fastq &gt; long_reads.fastq\n\n# Randomly select 10,000 reads using seqtk\nseqtk sample -s100 1001.fastq 10000 &gt; subsample.fastq\n\n# -----------------------------\n# Step 5: Compressing and Decompressing FASTQ Files\n# -----------------------------\n\n# Compress a FASTQ file\ngzip 1001.fastq\n\n# Decompress a FASTQ file\ngunzip 1001.fastq.gz\n\n# Work with compressed files directly\nzcat 1001.fastq.gz | head -8\n#| echo: true\n#| eval: false\n\n# -----------------------------\n# Step 6: Troubleshooting Tips\n# -----------------------------\n# Ensure FASTQ integrity by checking line count\nwc -l 1001.fastq\n\n# Check for duplicate sequence IDs\ngrep \"^@\" 1001.fastq | sort | uniq -c | sort -nr | head\n\n# Repair corrupted paired-end files using seqkit\nseqkit pair file1.fastq file2.fastq\n#| echo: true\n#| eval: false\n\n            ########################################################################################################\n            #                                           &lt;&lt;&lt; PART B &gt;&gt;&gt;                                           #\n            ########################################################################################################\n#B.1 Understanding For loops, using Fruits examples\n#########################################################\n#!/bin/bash\n# Demonstrating a FOR loop with fruits\necho \"FOR loop example:\"\nfor fruit in Apple Banana Cherry Date Elderberry; do\n    echo \"Fruit: $fruit\"\ndone\n\n\nqc_pipeline.sh\n\n#| echo: true\n#| eval: false\n\n#B.2 Applying it to QC \n#########################################################\n#!/bin/bash\nINPUT_FASTQ_DIR=\"/home/genomics/workshop_materials/quality_control\"\n\nRAW_DATA_DIR=\"/home/genomics/workshop_materials/quality_control\"\nQC_REPORTS_DIR=\"/home/genomics/workshop_materials/quality_control/qc_report\"\n\nmkdir -p $QC_REPORTS_DIR\necho \"FASTQC running... If your quality scores look bad, just remember: even the best scientists make ‘random errors’ too\"\nfor fastq_file in $RAW_DATA_DIR/*.fastq; do\n    echo \"Processing $fastq_file ...\"\n    fastqc \"$fastq_file\" -o \"$QC_REPORTS_DIR\"\n    echo \"Done with $fastq_file\"\ndone\n\nNow to run the script:\n#| echo: true\n#| eval: false\n\necho \"Running MultiQC ...\"\nmultiqc $QC_REPORTS_DIR -o $QC_REPORTS_DIR\necho \"Processing FASTQ files… because Excel said ‘File too large.’ 💀\"\necho \"QC pipeline completed.\"\n\nchmod +x qc_pipeline.sh\n./qc_pipeline.sh\n            ########################################################################################################\n            #                                           &lt;&lt;&lt; PART C &gt;&gt;&gt;                                           #\n            ########################################################################################################\n#!/bin/bash\necho \"Bioinformatics: Where we spend 90% of our time debugging, and the other 10% pretending it’s not our fault. 🧑‍💻🧐\"\n# Define input and output directories\nINPUT_FASTQ_DIR=\"/home/genomics/workshop_materials/quality_control\"\n\nINPUT_FASTQ_DIR=\"/home/genomics/workshop_materials/quality_control/output\"\n\n# Create output directory if it doesn't exist\nmkdir -p \"$OUTPUT_DIR\"\n\n# -----------------------------\n# Step 1: Understand FASTQ Files\n# -----------------------------\necho \"Step 1: Inspecting the FASTQ file format...\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    echo \"Processing file: $FILE\"\n    head -8 \"$FILE\"\n    echo \"Total records in $FILE:\"\n    wc -l \"$FILE\" | awk '{print $1/4}'\ndone\n\n# -----------------------------\n# Step 2: Search for Patterns\n# -----------------------------\necho \"Step 2: Searching for a specific sequence (e.g., GATTTG)...\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    echo \"Searching in: $FILE\"\n    grep -B 1 -A 2 \"GATTTG\" \"$FILE\" | head -10\ndone\n\n# -----------------------------\n# Step 3: Filter FASTQ Records by Quality\n# -----------------------------\necho \"Step 3: Filtering FASTQ records with quality scores above 30...\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    OUTPUT_FILE=\"$OUTPUT_DIR/$(basename \"$FILE\" .fastq)_filtered.fastq\"\n    seqtk seq -q 30 -Q 33 \"$FILE\" &gt; \"$OUTPUT_FILE\"\ndone\necho \"ERROR: Just kidding. Everything is fine. Probably. Maybe. Who knows?\"\n# -----------------------------\n# Step 4: Extract Records by ID\n# -----------------------------\necho \"Step 4: Extracting specific records by IDs...\"\nIDS_FILE=\"$OUTPUT_DIR/ids.txt\"\necho -e \"SEQ_ID_1\\nSEQ_ID_2\" &gt; \"$IDS_FILE\"\n\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    OUTPUT_FILE=\"$OUTPUT_DIR/$(basename \"$FILE\" .fastq)_extracted.fastq\"\n    seqtk subseq \"$FILE\" \"$IDS_FILE\" &gt; \"$OUTPUT_FILE\"\ndone\n\n# -----------------------------\n# Step 5: Trim Adapters\n# -----------------------------\necho \"Step 5: Trimming adapters (e.g., GGCCTCAGTGAAGGTCTCCTGCAAG)...\"\nADAPTER=\"GGCCTCAGTGAAGGTCTCCTGCAAG\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    OUTPUT_FILE=\"$OUTPUT_DIR/$(basename \"$FILE\" .fastq)_trimmed.fastq\"\n    cutadapt -a \"$ADAPTER\" -o \"$OUTPUT_FILE\" \"$FILE\"\ndone\necho \"Adapter trimming complete! Now your reads are like my social skills—shorter, but still functional. 😆\"\n# -----------------------------\n# Step 6: Perform Quality Control\n# -----------------------------\necho \"Step 6: Generating quality control reports...\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    fastqc -o \"$OUTPUT_DIR\" \"$FILE\"\ndone\n\n# -----------------------------\n# Step 7: Convert FASTQ to FASTA\n# -----------------------------\necho \"Step 7: Converting FASTQ to FASTA format...\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    OUTPUT_FILE=\"$OUTPUT_DIR/$(basename \"$FILE\" .fastq).fasta\"\n    seqtk seq -A \"$FILE\" &gt; \"$OUTPUT_FILE\"\ndone\n\n# -----------------------------\n# Step 8: Subsample Reads\n# -----------------------------\necho \"Step 8: Randomly subsampling 10,000 reads...\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    OUTPUT_FILE=\"$OUTPUT_DIR/$(basename \"$FILE\" .fastq)_subsampled.fastq\"\n    seqtk sample -s100 \"$FILE\" 10000 &gt; \"$OUTPUT_FILE\"\ndone\n\n# -----------------------------\n# Step 9: Extract Reads by Length\n# -----------------------------\necho \"Step 9: Extracting reads longer than 50bp...\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    OUTPUT_FILE=\"$OUTPUT_DIR/$(basename \"$FILE\" .fastq)_longreads.fastq\"\n    awk 'NR%4==2 && length($0) &gt; 50' \"$FILE\" &gt; \"$OUTPUT_FILE\"\ndone\necho \"If this script fails, remember: it's not a bug, it's an *undocumented feature*\"\n# -----------------------------\n# Step 10: Split FASTQ Files\n# -----------------------------\necho \"Step 10: Splitting FASTQ file into smaller chunks...\"\nSPLIT_DIR=\"$OUTPUT_DIR/split\"\nmkdir -p \"$SPLIT_DIR\"\n\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    seqkit split2 -p 5 -O \"$SPLIT_DIR\" \"$FILE\"\ndone\n\n# -----------------------------\n# Step 11: Compress FASTQ Files\n# -----------------------------\necho \"Step 11: Compressing FASTQ files...\"\necho \"This might take a while. Go grab a coffee ☕\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    OUTPUT_FILE=\"$OUTPUT_DIR/$(basename \"$FILE\" .fastq)_compressed.fastq.gz\"\n    gzip -c \"$FILE\" &gt; \"$OUTPUT_FILE\"\ndone\n\n# -----------------------------\n# Step 12: Check FASTQ Integrity\n# -----------------------------\necho \"Step 12: Checking FASTQ file integrity...\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    echo \"Checking integrity of: $FILE\"\n    awk 'NR%4==1' \"$FILE\" | wc -l\n    awk 'NR%4==0' \"$FILE\" | wc -l\ndone\n\n# -----------------------------\n# Completion Message\n# -----------------------------\necho \"FASTQ processing completed. Results are stored in '$OUTPUT_DIR'.\"\necho \"Script complete! If everything worked, congrats! 🎉\"",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 3</b>:Intro to Bioinformatics I"
    ]
  },
  {
    "objectID": "materials/3-workshop3/3-sequence-files-quality-adapters/index.html#slides",
    "href": "materials/3-workshop3/3-sequence-files-quality-adapters/index.html#slides",
    "title": "Quality filtering of Reads",
    "section": "",
    "text": "Download Manual\n\n\n#################################################################################################################################\nAll the commands from the tutorial, organized by section. You can execute these commands sequentially in your terminal\n#################################################################################################################################\n#Common Command-Line Tools for FASTQ Files\n#Ensure all necessary tools are installed on system. You can also install them using package managers like apt, brew,\n\n#    grep: Search for patterns/sudo apt-get install grep\n#    awk: Perform text processing/sudo apt-get install gawk\n#    sed: Edit text in a stream/sudo apt-get install sed\n#    seqkit: A versatile toolkit for FASTA/FASTQ files/sudo apt-get install seqkit\n#    fastp: Quality control and adapter trimming/sudo apt-get install fastp\n#    cutadapt: Adapter trimming/sudo apt-get install cutadapt/ pip install cutadapt\n#    seqtk: FASTQ manipulation/sudo apt-get install seqtk\n\necho \"Running this script will make your life easier... unless you forgot to install seqkit. Then it's just pain. 😆\"\n\n\n            ########################################################################################################\n            #                                           &lt;&lt;&lt; PART A &gt;&gt;&gt;                                           #\n            ########################################################################################################\n\n# -----------------------------\n# Step 1: Understanding the FASTQ File Format\n# -----------------------------\n#Find the size of FASTQ files \nls -lh 1001.fastq\n\n# View the first 8 lines (2 records)\nhead -8 1001.fastq\n\n# Count the total number of records\nwc -l 1001.fastq | awk '{print $1/4}'\n\n# Search for sequences containing \"GATTTG\"\ngrep -B 1 -A 2 \"GATTTG\" 1001.fastq\n#| echo: true\n#| eval: false\n\n# -----------------------------\n# Step 2: Filtering FASTQ Records\n# -----------------------------\n# Filter sequences with average quality &gt; 30\nseqtk seq -q 30 -Q 33 1001.fastq &gt; filtered.fastq\n#| echo: true\n#| eval: false\n\n# -----------------------------\n# Step 3: Trimming Adapters\n# -----------------------------\n# Search for adapter sequence \"GGCCTCAGTGAAGGTCTCCTGCAAG\"\ngrep -A 1 \"GGCCTCAGTGAAGGTCTCCTGCAAG\" 1001.fastq | less\n\n# Trim adapters using cutadapt\ncutadapt -a grep -A 1 GGCCTCAGTGAAGGTCTCCTGCAAG 1001.fastq | less -o trimmed.fastq 1001.fastq\n\n#For paired Reads\ncutadapt -a GGCCTCAGTGAAGGTCTCCTGCAAG -A REVERSE_ADAPTER -o trimmed_R1.fastq -p trimmed_R2.fastq file_R1.fastq file_R2.fastq\n\n# Trim adapters and perform quality control using fastp\nfastp -i 1001.fastq -o trimmed.fastq -a GGCCTCAGTGAAGGTCTCCTGCAAG\n#| echo: true\n#| eval: false\n\n# -----------------------------\n# Step 4: Advanced Adapter Identification\n# -----------------------------\n\n# De novo adapter detection using FastQC\nfastqc 1001.fastq\n\n# Plot adapter content using fastp\nfastp -i 1001.fastq -o output.fastq --detect_adapter_for_pe --html adapter_report.html\n#| echo: true\n#| eval: false\n\n# -----------------------------\n# Step 4: Manipulating FASTQ Files\n# -----------------------------\n\n# Convert FASTQ to FASTA\nseqtk seq -A 1001.fastq &gt; 1001.fasta\n\n# Split a large FASTQ file into smaller files (e.g., 1 million reads each)\nseqkit split2 -p 10 1001.fastq\n\n# Combine two FASTQ files (e.g., paired-end reads)\ncat file1.fastq file2.fastq &gt; merged.fastq\n\n# -----------------------------\n# Step 4: Extract Subsets of Reads\n# -----------------------------\n\n# Extract reads longer than 50 bp using awk\nawk 'NR%4==2 && length($0) &gt; 50 {print $0}' 1001.fastq &gt; long_reads.fastq\n\n# Randomly select 10,000 reads using seqtk\nseqtk sample -s100 1001.fastq 10000 &gt; subsample.fastq\n\n# -----------------------------\n# Step 5: Compressing and Decompressing FASTQ Files\n# -----------------------------\n\n# Compress a FASTQ file\ngzip 1001.fastq\n\n# Decompress a FASTQ file\ngunzip 1001.fastq.gz\n\n# Work with compressed files directly\nzcat 1001.fastq.gz | head -8\n#| echo: true\n#| eval: false\n\n# -----------------------------\n# Step 6: Troubleshooting Tips\n# -----------------------------\n# Ensure FASTQ integrity by checking line count\nwc -l 1001.fastq\n\n# Check for duplicate sequence IDs\ngrep \"^@\" 1001.fastq | sort | uniq -c | sort -nr | head\n\n# Repair corrupted paired-end files using seqkit\nseqkit pair file1.fastq file2.fastq\n#| echo: true\n#| eval: false\n\n            ########################################################################################################\n            #                                           &lt;&lt;&lt; PART B &gt;&gt;&gt;                                           #\n            ########################################################################################################\n#B.1 Understanding For loops, using Fruits examples\n#########################################################\n#!/bin/bash\n# Demonstrating a FOR loop with fruits\necho \"FOR loop example:\"\nfor fruit in Apple Banana Cherry Date Elderberry; do\n    echo \"Fruit: $fruit\"\ndone\n\n\nqc_pipeline.sh\n\n#| echo: true\n#| eval: false\n\n#B.2 Applying it to QC \n#########################################################\n#!/bin/bash\nINPUT_FASTQ_DIR=\"/home/genomics/workshop_materials/quality_control\"\n\nRAW_DATA_DIR=\"/home/genomics/workshop_materials/quality_control\"\nQC_REPORTS_DIR=\"/home/genomics/workshop_materials/quality_control/qc_report\"\n\nmkdir -p $QC_REPORTS_DIR\necho \"FASTQC running... If your quality scores look bad, just remember: even the best scientists make ‘random errors’ too\"\nfor fastq_file in $RAW_DATA_DIR/*.fastq; do\n    echo \"Processing $fastq_file ...\"\n    fastqc \"$fastq_file\" -o \"$QC_REPORTS_DIR\"\n    echo \"Done with $fastq_file\"\ndone\n\nNow to run the script:\n#| echo: true\n#| eval: false\n\necho \"Running MultiQC ...\"\nmultiqc $QC_REPORTS_DIR -o $QC_REPORTS_DIR\necho \"Processing FASTQ files… because Excel said ‘File too large.’ 💀\"\necho \"QC pipeline completed.\"\n\nchmod +x qc_pipeline.sh\n./qc_pipeline.sh\n            ########################################################################################################\n            #                                           &lt;&lt;&lt; PART C &gt;&gt;&gt;                                           #\n            ########################################################################################################\n#!/bin/bash\necho \"Bioinformatics: Where we spend 90% of our time debugging, and the other 10% pretending it’s not our fault. 🧑‍💻🧐\"\n# Define input and output directories\nINPUT_FASTQ_DIR=\"/home/genomics/workshop_materials/quality_control\"\n\nINPUT_FASTQ_DIR=\"/home/genomics/workshop_materials/quality_control/output\"\n\n# Create output directory if it doesn't exist\nmkdir -p \"$OUTPUT_DIR\"\n\n# -----------------------------\n# Step 1: Understand FASTQ Files\n# -----------------------------\necho \"Step 1: Inspecting the FASTQ file format...\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    echo \"Processing file: $FILE\"\n    head -8 \"$FILE\"\n    echo \"Total records in $FILE:\"\n    wc -l \"$FILE\" | awk '{print $1/4}'\ndone\n\n# -----------------------------\n# Step 2: Search for Patterns\n# -----------------------------\necho \"Step 2: Searching for a specific sequence (e.g., GATTTG)...\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    echo \"Searching in: $FILE\"\n    grep -B 1 -A 2 \"GATTTG\" \"$FILE\" | head -10\ndone\n\n# -----------------------------\n# Step 3: Filter FASTQ Records by Quality\n# -----------------------------\necho \"Step 3: Filtering FASTQ records with quality scores above 30...\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    OUTPUT_FILE=\"$OUTPUT_DIR/$(basename \"$FILE\" .fastq)_filtered.fastq\"\n    seqtk seq -q 30 -Q 33 \"$FILE\" &gt; \"$OUTPUT_FILE\"\ndone\necho \"ERROR: Just kidding. Everything is fine. Probably. Maybe. Who knows?\"\n# -----------------------------\n# Step 4: Extract Records by ID\n# -----------------------------\necho \"Step 4: Extracting specific records by IDs...\"\nIDS_FILE=\"$OUTPUT_DIR/ids.txt\"\necho -e \"SEQ_ID_1\\nSEQ_ID_2\" &gt; \"$IDS_FILE\"\n\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    OUTPUT_FILE=\"$OUTPUT_DIR/$(basename \"$FILE\" .fastq)_extracted.fastq\"\n    seqtk subseq \"$FILE\" \"$IDS_FILE\" &gt; \"$OUTPUT_FILE\"\ndone\n\n# -----------------------------\n# Step 5: Trim Adapters\n# -----------------------------\necho \"Step 5: Trimming adapters (e.g., GGCCTCAGTGAAGGTCTCCTGCAAG)...\"\nADAPTER=\"GGCCTCAGTGAAGGTCTCCTGCAAG\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    OUTPUT_FILE=\"$OUTPUT_DIR/$(basename \"$FILE\" .fastq)_trimmed.fastq\"\n    cutadapt -a \"$ADAPTER\" -o \"$OUTPUT_FILE\" \"$FILE\"\ndone\necho \"Adapter trimming complete! Now your reads are like my social skills—shorter, but still functional. 😆\"\n# -----------------------------\n# Step 6: Perform Quality Control\n# -----------------------------\necho \"Step 6: Generating quality control reports...\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    fastqc -o \"$OUTPUT_DIR\" \"$FILE\"\ndone\n\n# -----------------------------\n# Step 7: Convert FASTQ to FASTA\n# -----------------------------\necho \"Step 7: Converting FASTQ to FASTA format...\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    OUTPUT_FILE=\"$OUTPUT_DIR/$(basename \"$FILE\" .fastq).fasta\"\n    seqtk seq -A \"$FILE\" &gt; \"$OUTPUT_FILE\"\ndone\n\n# -----------------------------\n# Step 8: Subsample Reads\n# -----------------------------\necho \"Step 8: Randomly subsampling 10,000 reads...\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    OUTPUT_FILE=\"$OUTPUT_DIR/$(basename \"$FILE\" .fastq)_subsampled.fastq\"\n    seqtk sample -s100 \"$FILE\" 10000 &gt; \"$OUTPUT_FILE\"\ndone\n\n# -----------------------------\n# Step 9: Extract Reads by Length\n# -----------------------------\necho \"Step 9: Extracting reads longer than 50bp...\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    OUTPUT_FILE=\"$OUTPUT_DIR/$(basename \"$FILE\" .fastq)_longreads.fastq\"\n    awk 'NR%4==2 && length($0) &gt; 50' \"$FILE\" &gt; \"$OUTPUT_FILE\"\ndone\necho \"If this script fails, remember: it's not a bug, it's an *undocumented feature*\"\n# -----------------------------\n# Step 10: Split FASTQ Files\n# -----------------------------\necho \"Step 10: Splitting FASTQ file into smaller chunks...\"\nSPLIT_DIR=\"$OUTPUT_DIR/split\"\nmkdir -p \"$SPLIT_DIR\"\n\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    seqkit split2 -p 5 -O \"$SPLIT_DIR\" \"$FILE\"\ndone\n\n# -----------------------------\n# Step 11: Compress FASTQ Files\n# -----------------------------\necho \"Step 11: Compressing FASTQ files...\"\necho \"This might take a while. Go grab a coffee ☕\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    OUTPUT_FILE=\"$OUTPUT_DIR/$(basename \"$FILE\" .fastq)_compressed.fastq.gz\"\n    gzip -c \"$FILE\" &gt; \"$OUTPUT_FILE\"\ndone\n\n# -----------------------------\n# Step 12: Check FASTQ Integrity\n# -----------------------------\necho \"Step 12: Checking FASTQ file integrity...\"\nfor FILE in \"$INPUT_FASTQ_DIR\"/*.fastq; do\n    echo \"Checking integrity of: $FILE\"\n    awk 'NR%4==1' \"$FILE\" | wc -l\n    awk 'NR%4==0' \"$FILE\" | wc -l\ndone\n\n# -----------------------------\n# Completion Message\n# -----------------------------\necho \"FASTQ processing completed. Results are stored in '$OUTPUT_DIR'.\"\necho \"Script complete! If everything worked, congrats! 🎉\"",
    "crumbs": [
      "AWS Instance IPs",
      "<b>3. Bioinformatics and Viral Genomics - February 22-28 2025</b>",
      "<b>Module 3</b>:Intro to Bioinformatics I"
    ]
  },
  {
    "objectID": "materials/3-workshop3/4-mapping-and-assembly/slides.html#practicalities",
    "href": "materials/3-workshop3/4-mapping-and-assembly/slides.html#practicalities",
    "title": "Wifi details",
    "section": "Practicalities",
    "text": "Practicalities\n\nWiFi:\nNetwork: Southernsunconference\nNetwork Password: S0uthernsun1\n\n\n\n\nback to module"
  },
  {
    "objectID": "materials/3-workshop3/6-untargetted-viromics/UntargetedViromics.html",
    "href": "materials/3-workshop3/6-untargetted-viromics/UntargetedViromics.html",
    "title": "Workshop III – Bioinformatics and Viral Genomics",
    "section": "",
    "text": "The tables used for this exercise are the main outputs of Hecatomb (https://github.com/shandley/hecatomb), a software aimed to increase virus discovery on complex samples (Metagenomic WGS or VLP prep WGS)."
  },
  {
    "objectID": "materials/3-workshop3/6-untargetted-viromics/UntargetedViromics.html#background",
    "href": "materials/3-workshop3/6-untargetted-viromics/UntargetedViromics.html#background",
    "title": "Workshop III – Bioinformatics and Viral Genomics",
    "section": "",
    "text": "The tables used for this exercise are the main outputs of Hecatomb (https://github.com/shandley/hecatomb), a software aimed to increase virus discovery on complex samples (Metagenomic WGS or VLP prep WGS)."
  },
  {
    "objectID": "materials/3-workshop3/6-untargetted-viromics/UntargetedViromics.html#dataset",
    "href": "materials/3-workshop3/6-untargetted-viromics/UntargetedViromics.html#dataset",
    "title": "Workshop III – Bioinformatics and Viral Genomics",
    "section": "Dataset",
    "text": "Dataset\nPrimarily, we will use a file called bigtable.tsv. This file has the taxonomic classification of each read that was used as input to Hecatomb. It also has the alignment statistics and the number of equal reads found for each sample.\nWe will also use the file called metadata.tsv. The samples in the test dataset are samples taken from deceased Macaques from the study “SIV Infection-Mediated Changes in Gastrointestinal Bacterial Microbiome and Virome Are Associated with Immunodeficiency and Prevented by Vaccination” (https://www.sciencedirect.com/science/article/pii/S1931312816300518). The metadata contains the individuals’ gender and the vaccine that was administered."
  },
  {
    "objectID": "materials/3-workshop3/6-untargetted-viromics/UntargetedViromics.html#setting",
    "href": "materials/3-workshop3/6-untargetted-viromics/UntargetedViromics.html#setting",
    "title": "Workshop III – Bioinformatics and Viral Genomics",
    "section": "Setting",
    "text": "Setting\nWe are going to connect to the Rstudio server that is running on our machines\n\nhttp://44.202.27.9:8787 #IMPORTANT: Change 44.202.27.9 by your IP\n#Paste it in your browser\nuser: genomics\npassword:evomics2025\n\nAlso, we are going to connect to using ssh within the terminal. Then, we can go to workshop_materials and create a new directory called untargetedViromis. Then, go to that directory. Finally, we are going to download our dataset.\nssh genomics@serverIP #connect to the server\ncd ~/workshop_materials/\nmkdir untargetedViromics #create a new working directory\ncd untargetedViromics\ngit clone https://github.com/luisalbertoc95/UV_data-Workshop-III-Bioinformatics-SA-2025.git  \nmv UV_data-Workshop-III-Bioinformatics-SA-2025 uv_data #Change the name to work with a shorter one."
  },
  {
    "objectID": "materials/3-workshop3/6-untargetted-viromics/UntargetedViromics.html#analysis",
    "href": "materials/3-workshop3/6-untargetted-viromics/UntargetedViromics.html#analysis",
    "title": "Workshop III – Bioinformatics and Viral Genomics",
    "section": "Analysis",
    "text": "Analysis\n\nStep 1: Set up a new RMarkdwon/Quarto document\n\n\nStep 2: Initiate your environment\nFirst, We are going to set our working directory for all the chunks\n\nknitr::opts_knit$set(root.dir = \"/Users/luischica/Desktop/uv_data\") #IMPORTANT: change /Users/luischica/Desktop/Workshop_SA/data by ~/workshop_materials/untargetedViromics/uv_data\n\nOptionally, if you are using a normal R script, you can use: setwd(pathToYourWorkingDirectory)\nFor this exercise we will use 4 packages: ggplot2, dplyr and tidyr from the tidyverse suite, but also we will need the rstatix package.\n\n# Check and install tidyverse if needed, then load it\nif (!requireNamespace(\"tidyverse\", quietly = TRUE)) {\n  install.packages(\"tidyverse\")\n}\nlibrary(tidyverse)\n\n# Check and install rstatix if needed, then load it\nif (!requireNamespace(\"rstatix\", quietly = TRUE)) {\n  install.packages(\"rstatix\")\n}\nlibrary(rstatix)\n\n# Check and install DECIPHER if needed, then load it\nif (!requireNamespace(\"DECIPHER\", quietly = TRUE)) {\n  if (!requireNamespace(\"BiocManager\", quietly = TRUE)) {\n    install.packages(\"BiocManager\")\n  }\n  BiocManager::install(\"DECIPHER\")\n}\nlibrary(DECIPHER)\n\n\n\nStep 3: Set the location and load our input files\n\ndata &lt;- read.delim('bigtable.tsv',header=T,sep='\\t')\nmeta &lt;- read.csv('metadata.tsv',header=T,sep='\\t')\n\nInspect the dataframes\n\nhead(data)\n\n                         seqID              sampleID count normCount alnType\n1 A13-258-124-06_CGTACG:1:6006 A13-258-124-06_CGTACG     1  1.012666      aa\n2 A13-258-124-06_CGTACG:1:6007 A13-258-124-06_CGTACG     1  1.012666      aa\n3 A13-258-124-06_CGTACG:1:6019 A13-258-124-06_CGTACG     1  1.012666      aa\n4 A13-258-124-06_CGTACG:1:6020 A13-258-124-06_CGTACG     1  1.012666      aa\n5 A13-258-124-06_CGTACG:1:6030 A13-258-124-06_CGTACG     1  1.012666      aa\n6 A13-258-124-06_CGTACG:1:6031 A13-258-124-06_CGTACG     1  1.012666      aa\n    targetID    evalue pident fident nident mismatches  qcov  tcov qstart qend\n1 A0A1W5PTE0 2.240e-46   97.3  0.973     73          2 0.962 0.166    234   10\n2     E0NZW5 1.941e-23   74.1  0.741     46         16 0.795 0.785     18  203\n3 A0A349YS28 4.343e-17   51.9  0.519     40         37 0.991 0.193      3  233\n4 A0A2N5ZDN1 3.061e-35   81.8  0.818     63         14 0.987 0.279      4  234\n5 A0A1Q6JQ74 4.923e-17   56.5  0.565     39         30 0.885 0.107     15  221\n6 A0A345MUW0 7.251e-24   85.0  0.850     51          9 0.933 0.163     12  191\n  qlen tstart tend tlen alnlen bits\n1  234    372  446  452    225  167\n2  234     16   77   79    186  101\n3  233    155  231  399    231   83\n4  234    161  237  276    231  135\n5  234    422  490  643    207   83\n6  193    294  353  369    180  102\n                                            targetName taxMethod  kingdom\n1                                       VP1 (Fragment)    TopHit  Viruses\n2 Toxin-antitoxin system, toxin component, HicA family    TopHit Bacteria\n3                              Uncharacterized protein       LCA Bacteria\n4                       Lysine--tRNA ligase (Fragment)    TopHit Bacteria\n5                      Phage tail tape measure protein    TopHit Bacteria\n6             A0A345MUW0_9VIRU Uncharacterized protein       LCA  Viruses\n                        phylum                       class\n1                Cossaviricota             Quintoviricetes\n2                   Firmicutes               Negativicutes\n3 unclassified Bacteria phylum unclassified Bacteria class\n4                Bacteroidetes                 Bacteroidia\n5                   Firmicutes                  Clostridia\n6               Hofneiviricota              Faserviricetes\n                        order                               family\n1                Piccovirales                         Parvoviridae\n2             Selenomonadales                     Selenomonadaceae\n3 unclassified Bacteria order         unclassified Bacteria family\n4            Marinilabiliales unclassified Marinilabiliales family\n5               Clostridiales    unclassified Clostridiales family\n6               Tubulavirales                           Inoviridae\n                                genus                        species\n1                     Protoparvovirus               Simian bufavirus\n2                         Selenomonas Selenomonas sp. oral taxon 149\n3         unclassified Bacteria genus  unclassified Bacteria species\n4 unclassified Marinilabiliales genus     Marinilabiliales bacterium\n5    unclassified Clostridiales genus  Clostridiales bacterium 59_14\n6       unclassified Inoviridae genus                 Inoviridae sp.\n  baltimoreType baltimoreGroup\n1         ssDNA             II\n2          &lt;NA&gt;           &lt;NA&gt;\n3          &lt;NA&gt;           &lt;NA&gt;\n4          &lt;NA&gt;           &lt;NA&gt;\n5          &lt;NA&gt;           &lt;NA&gt;\n6         ssDNA             II\n\n\n\nhead(meta)\n\n               sampleID  vaccine sex MacGuffinGroup\n1  A13-04-182-06_TAGCTT     sham   F              B\n2  A13-12-250-06_GGCTAC     sham   F              B\n3 A13-135-177-06_AGTTCC Ad_alone   F              A\n4 A13-151-169-06_ATGTCA Ad_alone   F              A\n5 A13-252-114-06_CCGTCC     sham   M              A\n6 A13-253-140-06_GTCCGC     sham   M              B\n\n\n\n\nStep 4: Merging our metadata with our data table\nThe merge function with perform an inner join by default, or you can specify outer, and left- and right-outer. This shouldn’t matter if you have metadata for all of your samples.\n\ndataMeta &lt;-  merge(data, meta, by='sampleID')\n\n\n\nStep 5: Preliminary bigtable plots\nFirst, we will plot the alignment length against identity, and facet by viral family. We show the different alignment types by color, and we can scale the point size by the cluster number. The alpha=0.1 will help us to set it to 10% opacity and the points will overlap a lot at this scale.\nBefore making our plot, we will filter our dataMeta in order to remove all non viral taxonomic annotation. The function filter() will take all the hits to the word Viruses in our Kingdom column\n\nviruses &lt;- dataMeta %&gt;% \n    filter(kingdom==\"Viruses\")\n\nNow we can make our first plot\n\nggplot(viruses) + \n    geom_point(\n        aes(x=alnlen,y=pident,color=alnType,size=count),\n        alpha=0.1) + \n    facet_wrap(~family)\n\n\n\n\n\n\n\n\nWe can immediately see that a handful of viral families make up a majority of the viral hits. You can use these plots to help guide filtering strategies. We can divide the alignments into ‘quadrants’ by adding alignment length and percent identity thresholds, for instance alignment length of 150 and percent identity of 75.\nWe can also add some threshold lines to divide our plot into quadrants. This quadrants will be useful to see how much can we trust in each alignment.\n\nggplot(viruses) +\n    geom_point(\n        aes(x=alnlen,y=pident,color=alnType,size=count),\n        alpha=0.1) +\n    facet_wrap(~family) +\n    geom_vline(xintercept=150,colour='red',linetype='longdash') +\n    geom_hline(yintercept=75,colour='red',linetype='longdash')\n\n\n\n\n\n\n\n\nWe can see that for Adenoviridae and Parvoviridae the majority of hits occupy the top two quadrants, and we can be reasonably confident about these alignments. For Podoviridae and Circoviridae, the majority of hits occupy the bottom two quadrants. This could indicate that the viruses are only distantly related to the reference genomes in these families.\nTask: plot the Bacterial hits faceted by phylum\n\n\nStep 6: Filtering Strategies\nHecatomb is not intended to be a black box of predetermined filtering cutoffs that returns an immutable table of hits. Instead, it delivers as much information as possible to empower the user to decide which hits they want to keep and which hits to purge. Let’s take our raw viral hits data frame viruses and filter them to only keep the ones we are confident about.\nThe e-value is one of the most common metrics to use for filtering alignments. Let’s see what hits would be removed if we used a fairly stringent cutoff of 1e-20. We will create a new data set that contains only the hits remaining after applying the p-value threshold.\nWe are going to see two strategies for filtering our table. In the first strategy, we will add an additional column to our data set. This column will have 2 possible values: “filter” and “pass”. whether a read is tagged as filter or pass will depend of the the threshold added in the function ifelse()\n\nvirusesFiltered &lt;- viruses %&gt;% \n    mutate(filter=ifelse(evalue&lt;1e-20,'pass','filter'))\n\nWe can plot our new table\n\nggplot(virusesFiltered) +\n    geom_point(\n        aes(x=alnlen,y=pident,color=filter),\n        alpha=0.2) +\n    facet_wrap(~family)\n\n\n\n\n\n\n\n\nThe red sequences are destined to be removed, while the blue sequences will be kept. Some viral families will be removed altogether, which is probably a good thing if they only have low quality hits.\nThe second strategy, is a more straightforward method. We only need to use the function filter() and the desired e-value threshold. No additional column will be added.\n\nvirusesFiltered &lt;- viruses %&gt;% \n    filter(evalue&lt;1e-20)\n\nGoing back to the quadrant concept, you might only want to keep sequences above a certain length and percent identity:\n\nvirusesFiltered = virusesFiltered %&gt;% \n    filter(alnlen&gt;150 & pident&gt;75)\n\nNow, our filtered table has an alignment filter, an identity % filter and our previous p-value filter.\nThere are many alignment metrics included in the bigtable for you to choose from.\nTask: Filter your raw viral hits to only keep protein hits with an evalue &lt; 1e-10\n\n\nStep 7: Analyse taxon counts\n\nMake Per family plots First, we will sum the normalized count of each read to a family level using the columns sampleID and family\n\n\nviralFamCounts &lt;- virusesFiltered %&gt;% \n  group_by(family) %&gt;% \n  summarise(normCount=sum(normCount)) %&gt;% \n  arrange(desc(normCount))\n\nviralFamCounts$family &lt;- factor(viralFamCounts$family,levels=viralFamCounts$family)\n\nhead(viralFamCounts)\n\n# A tibble: 6 × 2\n  family                      normCount\n  &lt;fct&gt;                           &lt;dbl&gt;\n1 unclassified Viruses family     2188.\n2 Adenoviridae                    1906.\n3 Microviridae                    1831.\n4 Parvoviridae                    1274.\n5 Picornaviridae                   741.\n6 Siphoviridae                     720.\n\n\nAfter having our new dataframe with the counts per Family, we can create our abundance plot.\n\nggplot(viralFamCounts) +\n  geom_bar(aes(x=family,y=normCount),stat='identity') +\n  coord_flip()  \n\n\n\n\n\n\n\n\n\nDiscriminate our family plots by sample ID\n\nThe previous plot are a good way to understand the overall abundance of each family in all our samples. However, with that plot is impossible to differentiate between samples and therefore, between different treatments. We can sum our normalized counts in a similar way we did before. Now we are going to add the variable “sampleID” to the function group_by()\n\nviralFamCounts &lt;- virusesFiltered %&gt;% \n  group_by(sampleID,family) %&gt;% \n  summarise(normCount=sum(normCount)) %&gt;% \n  arrange(desc(normCount))\n\nhead(viralFamCounts)\n\n# A tibble: 6 × 3\n# Groups:   sampleID [4]\n  sampleID              family                      normCount\n  &lt;chr&gt;                 &lt;chr&gt;                           &lt;dbl&gt;\n1 A13-151-169-06_ATGTCA Adenoviridae                    1777.\n2 A13-151-169-06_ATGTCA Parvoviridae                     993.\n3 A13-135-177-06_AGTTCC unclassified Viruses family      956.\n4 A13-256-115-06_GTTTCG Microviridae                     620.\n5 A13-253-140-06_GTCCGC Myoviridae                       490.\n6 A13-253-140-06_GTCCGC Picornaviridae                   486.\n\n\nand then, make the new plot, filling by family\n\nggplot(viralFamCounts) +\n  geom_bar(aes(x=sampleID,y=normCount, fill =family ),stat='identity') +\n  coord_flip()  \n\n\n\n\n\n\n\n\nTask: Make a stacked bar chart of the viral families for the Male and Female monkeys\n\nVisualizing groups We have a few viral families that are very prominent in our samples. For the purposes of the tutorial we have a completely made up sample group category called MacGuffinGroup. Let’s see if there is a difference in viral loads according to our MacGuffinGroup groups. Collect sample counts for Microviridae. Include the metadata group in group_by() so you can use it in the plot.\n\nFor our first plot we are going to focus exclusively in Microviridae family\n\npodoCounts &lt;- virusesFiltered %&gt;% \n    group_by(family,sampleID,MacGuffinGroup) %&gt;% \n    filter(family=='Podoviridae') %&gt;% \n    summarise(n = sum(normCount))\n\nThen, we can plot using jitter plots, box plots or violin plots\n\nggplot(podoCounts) +\n    geom_jitter(aes(x=MacGuffinGroup,y=n),width = 0.1) +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\nStep 8: Statistical tests\nIn Step 6, we compared the viral counts between the two sample groups for Podoviridae, and it appeared as though group B had more viral sequence hits on average than group A. We can compare the normalized counts for these two groups to see if they’re significantly different.\n\nStudent’s T-test\n\nLet’s check out the data frame we made earlier that we’ll be using for the test\n\nhead(podoCounts)\n\n# A tibble: 6 × 4\n# Groups:   family, sampleID [6]\n  family      sampleID              MacGuffinGroup      n\n  &lt;chr&gt;       &lt;chr&gt;                 &lt;chr&gt;           &lt;dbl&gt;\n1 Podoviridae A13-04-182-06_TAGCTT  B                4.56\n2 Podoviridae A13-12-250-06_GGCTAC  B              203.  \n3 Podoviridae A13-135-177-06_AGTTCC A                1.91\n4 Podoviridae A13-151-169-06_ATGTCA A               12.7 \n5 Podoviridae A13-252-114-06_CCGTCC A                8.22\n6 Podoviridae A13-253-140-06_GTCCGC B               67.9 \n\n\nWe will use the base-r function t.test(), which takes two vectors. One vector has the group A counts and the other has the group B counts. We can use the filter() and pull() functions within the t.test() function. The function pull will take the column n, which has our abundance values.\n\nt.test(\n    podoCounts %&gt;% \n        filter(MacGuffinGroup=='A') %&gt;% \n        pull(n),\n    podoCounts %&gt;% \n        filter(MacGuffinGroup=='B') %&gt;% \n        pull(n),\n    alternative='two.sided',\n    paired=F,\n    var.equal=T)  \n\n\n    Two Sample t-test\n\ndata:  podoCounts %&gt;% filter(MacGuffinGroup == \"A\") %&gt;% pull(n) and podoCounts %&gt;% filter(MacGuffinGroup == \"B\") %&gt;% pull(n)\nt = -2.0096, df = 8, p-value = 0.07933\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -145.561036    9.996944\nsample estimates:\nmean of x mean of y \n 7.701613 75.483659 \n\n\n\nWilcoxon test\n\nThe Wilcoxon test is analogue to the t-test, however is used when our values do not follow a normal distribution. The syntax for this test is very similar to the t-test\n\nwilcox.test(\n    podoCounts %&gt;% \n        filter(MacGuffinGroup=='A') %&gt;% \n        pull(n),\n    podoCounts %&gt;% \n        filter(MacGuffinGroup=='B') %&gt;% \n        pull(n),\n    alternative='t',\n    paired=F)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  podoCounts %&gt;% filter(MacGuffinGroup == \"A\") %&gt;% pull(n) and podoCounts %&gt;% filter(MacGuffinGroup == \"B\") %&gt;% pull(n)\nW = 4, p-value = 0.09524\nalternative hypothesis: true location shift is not equal to 0\n\n\nIn some cases, the viral loads have minor importance for answering a research question, instead we might be just interested in comparing the presence or absence of viruses. For this you could use a Fisher’s exact test. To perform this test you need to assign a presence ‘1’ or absence ‘0’ for each viral family/genus/etc for each sample.\nFirst we are going to apply and even more stringent filter to be sure about the alignments\n\nvirusesStringent &lt;- viruses %&gt;% \n    filter(evalue&lt;1e-30,alnlen&gt;150,pident&gt;75,alnType=='aa') \n\nThen we will assign anything with any hits as ‘present’ for that an specific viral family (It can be done for all of them). For this example we are going to use Myoviridae.\nOur chunk has two lines. The first one will extract the counts for Myoviridae and the second will merge our new table with our metadata file\n\nmyovirPresAbs &lt;- virusesStringent %&gt;% \n    filter(family=='Myoviridae') %&gt;%\n    group_by(sampleID) %&gt;% \n    summarise(n=sum(normCount)) %&gt;%\n    mutate(present=ifelse(n&gt;0,1,0))\n\nmyovirPresAbs &lt;- merge(myovirPresAbs,meta,by='sampleID',all=T)\n\nIf we visualise our “myovirPresAbs” table, we will see some values missing or NA. Those are the samples in which no presence of Myoviridae was found. We can convert the NAs to zeros.\n\nmyovirPresAbs[is.na(myovirPresAbs)] = 0\n\nTo do the Fisher’s exact test we need to specify a 2x2 grid; The first column will be the number with Myoviridae for each group. The second column will be the numbers without for each group.\n\n# matrix rows\nmtxGroupA = c(\n    myovirPresAbs %&gt;% \n        filter(MacGuffinGroup=='A',present==1) %&gt;% \n        summarise(n=n()) %&gt;% \n        pull(n),\n    myovirPresAbs %&gt;% \n        filter(MacGuffinGroup=='A',present==0) %&gt;% \n        summarise(n=n()) %&gt;% \n        pull(n))\nmtxGroupB = c(\n    myovirPresAbs %&gt;% \n        filter(MacGuffinGroup=='B',present==1) %&gt;% \n        summarise(n=n()) %&gt;% \n        pull(n),\n    myovirPresAbs %&gt;% \n        filter(MacGuffinGroup=='B',present==0) %&gt;% \n        summarise(n=n()) %&gt;% \n        pull(n))\n\n# create the 2x2 matrix\nmyovirFishMtx = matrix(c(mtxGroupA,mtxGroupB),nrow = 2)\n\n# this bit is not necessary, but lets add row and col names to illustrate the matrix layout\ncolnames(myovirFishMtx) = c('GroupA','GroupB')\nrow.names(myovirFishMtx) = c('present','absent')\n\n# view\nmyovirFishMtx\n\n        GroupA GroupB\npresent      1      5\nabsent       4      0\n\n\nWe can run our t-test using our new matrix as input\n\nfisher.test(myovirFishMtx)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  myovirFishMtx\np-value = 0.04762\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.000000 0.975779\nsample estimates:\nodds ratio \n         0 \n\n\n\n\nStep 9: Contig based analysis\nContig analysis is a very important approach when identifying viruses from a metagenomic or VLP prep sample. Because of the length of the contigs, new genomic features can be identified and used for generating a taxonomic classification, functional analysis and more.\n\nFist, we will load our taxonomy table generated from hecatomb. This table relates every contig with their best hit in the data base.\n\nFor the purpose of this exercise we will filter our table for contigs classified at family level as Caudovirales. and with a sequence length between 2000 and 3000 base pairs.\n\ncontig_data &lt;- read.delim('MMseqsTax.txt',header=T,sep='\\t')\n\ncaudovirales_data &lt;- contig_data %&gt;%\n  filter(Order == \"Caudovirales\" & qlen &gt; 2000 & qlen &lt; 3000) %&gt;%\n  select(contigID) \n\n\nWe will load our fasta file generated after the assembly using the package Biostrings, included in the package DECIPHER. We can also use the package [] for selecting our target contigs from the fasta file\n\n\ncontigs &lt;- readDNAStringSet(\"assembly.fasta\", format=\"fasta\")\n\ncaudo_contigs &lt;- contigs[caudovirales_data$contigID] \n\n\nWe can use our set of contigs to make a multiple sequence alignment (MSA) with the final purpose of generating a phylogeny tree.\n\nThe steps to make our MSA are going to be based on the decision diagram from the DEIPHER package\n Steps:\n\nCreate the multiple sequences alignement\nRemove ambiguous regions from the alignment\n\n\nCreate the phylogenetic tree\n\n\nPlot the dendrogram\n\n\n#A\naln_contig &lt;- AlignSeqs(caudo_contigs)\n\nDetermining distance matrix based on shared 10-mers:\n================================================================================\n\nTime difference of 0.04 secs\n\nClustering into groups by similarity:\n================================================================================\n\nTime difference of 0 secs\n\nAligning Sequences:\n================================================================================\n\nTime difference of 2.76 secs\n\nIteration 1 of 2:\n\nDetermining distance matrix based on alignment:\n================================================================================\n\nTime difference of 0 secs\n\nReclustering into groups by similarity:\n================================================================================\n\nTime difference of 0 secs\n\nRealigning Sequences:\n================================================================================\n\nTime difference of 1.99 secs\n\nIteration 2 of 2:\n\nDetermining distance matrix based on alignment:\n================================================================================\n\nTime difference of 0 secs\n\nReclustering into groups by similarity:\n================================================================================\n\nTime difference of 0 secs\n\nRealigning Sequences:\n================================================================================\n\nTime difference of 1.07 secs\n\nRefining the alignment:\n================================================================================\n\nTime difference of 3.22 secs\n\n#B\nstagger_aln_contig&lt;- StaggerAlignment(aln_contig)\n\nCalculating distance matrix:\n================================================================================\n\nTime difference of 0 secs\n\nConstructing neighbor-joining tree:\n================================================================================\n\nTime difference of 0 secs\n\nStaggering insertions and deletions:\n================================================================================\n\nTime difference of 0.65 secs\n\nBrowseSeqs(stagger_aln_contig, highlight=1)\n#C\ntree &lt;- TreeLine(stagger_aln_contig, reconstruct=TRUE, maxTime=0.05)\n\nOptimizing model parameters:\nJC69     -ln(L) = 62850, AICc = 125775, BIC = 125989\nJC69+G4  -ln(L) = 62814, AICc = 125705, BIC = 125924\nK80      -ln(L) = 62843, AICc = 125763, BIC = 125982\nK80+G4   -ln(L) = 62806, AICc = 125692, BIC = 125916\nF81      -ln(L) = 62406, AICc = 124893, BIC = 125123\nF81+G4   -ln(L) = 62374, AICc = 124832, BIC = 125068\nHKY85    -ln(L) = 62402, AICc = 124887, BIC = 125123\nHKY85+G4 -ln(L) = 62372, AICc = 124830, BIC = 125072\nT92      -ln(L) = 62416, AICc = 124911, BIC = 125135\nT92+G4   -ln(L) = 62384, AICc = 124850, BIC = 125080\nTN93     -ln(L) = 62401, AICc = 124888, BIC = 125130\nTN93+G4  -ln(L) = 62369, AICc = 124826, BIC = 125074\nSYM      -ln(L) = 62755, AICc = 125596, BIC = 125838\nSYM+G4   -ln(L) = 62706, AICc = 125499, BIC = 125747\nGTR      -ln(L) = 62386, AICc = 124863, BIC = 125122\nGTR+G4   -ln(L) = 62352, AICc = 124798, BIC = 125063\n\nThe selected model was:  GTR+G4\n\nPHASE 1 OF 3: INITIAL TREES\n\n1/3. Optimizing initial tree #1 of 10 to 100:\n-ln(L) = 62350.7 (-0.002%), 2 Climbs  \n1/3. Optimizing initial tree #2 of 10 to 100:\n-ln(L) = 62351.2 (+0.001%), 3 Climbs  \n1/3. Optimizing initial tree #3 of 10 to 100:\n-ln(L) = 62346.3 (-0.007%), 4 Climbs  \n\nPHASE 2 OF 3: REGROW GENERATION 1 OF 10 TO 20\n\n2/3. Optimizing regrown tree #1 of 10 to 100:\n-ln(L) = 62350.6 (+0.007%), 2 Climbs  \n2/3. Optimizing regrown tree #2 of 10 to 100:\n-ln(L) = 62345.7 (-0.001%), 1 Climb  \n2/3. Optimizing regrown tree #3 of 12 to 100:\n-ln(L) = 62344.4 (-0.002%), 2 Climbs  \n2/3. Optimizing regrown tree #4 of 13 to 100:\n-ln(L) = 62352.7 (+0.013%), 1 Climb  \n2/3. Optimizing regrown tree #5 of 13 to 100:\n-ln(L) = 62348.1 (+0.006%), 2 Climbs  \n2/3. Optimizing regrown tree #6 of 13 to 100:\n-ln(L) = 62345.4 (+0.002%), 2 Climbs  \n2/3. Optimizing regrown tree #7 of 13 to 100:\n-ln(L) = 62347.2 (+0.004%), 1 Climb  \n2/3. Optimizing regrown tree #8 of 13 to 100:\n-ln(L) = 62346.2 (+0.003%), 1 Climb  \n2/3. Optimizing regrown tree #9 of 13 to 100:\n-ln(L) = 62345.3 (+0.001%), 2 Climbs  \n\nPHASE 3 OF 3: SHAKEN TREES\n\nGrafting 1 tree to the best tree:\n-ln(L) = 62344.4 (0.000%), 0 Grafts of 4  \n\n3/3. Optimizing shaken tree #1 of 3 to 1000:\n-ln(L) = 62352.9 (+0.014%), 11 Climbs  \n3/3. Optimizing shaken tree #2 of 3 to 1000:\n-ln(L) = 62345.4 (+0.002%), 11 Climbs  \n3/3. Optimizing shaken tree #3 of 3 to 1000:\n-ln(L) = 62368.2 (+0.038%), 10 Climbs  \n\nGrafting 3 trees to the best tree:\n-ln(L) = 62344.4 (0.000%), 0 Grafts of 7  \n\nModel parameters:\nFrequency(A) = 0.284\nFrequency(C) = 0.204\nFrequency(G) = 0.200\nFrequency(T) = 0.313\nRate A &lt;-&gt; C = 1.181\nRate A &lt;-&gt; G = 1.367\nRate A &lt;-&gt; T = 1.093\nRate C &lt;-&gt; G = 1.453\nRate C &lt;-&gt; T = 1.172\nRate G &lt;-&gt; T = 1.000\nAlpha = 11.198\n\nTime difference of 240.55 secs\n\n#D\nplot(dendrapply(tree,\n         function(x) {\n                 s &lt;- attr(x, \"probability\") \n                 if (!is.null(s) && !is.na(s)) {\n                         s &lt;- formatC(as.numeric(s), digits=2, format=\"f\")\n                         attr(x, \"edgetext\") &lt;- paste(s, \"\\n\")\n                 }\n                 attr(x, \"edgePar\") &lt;- list(p.col=NA, p.lwd=1e-5, t.col=\"#CC55AA\")\n                 if (is.leaf(x))\n                   \nattr(x, \"nodePar\") &lt;- list(lab.font=3, pch=NA)\nx \n}),\nhoriz=TRUE,yaxt='n')"
  },
  {
    "objectID": "materials/3-workshop3/7-best-practices-and-send-off/exercise_1.html#activity",
    "href": "materials/3-workshop3/7-best-practices-and-send-off/exercise_1.html#activity",
    "title": "Exploring Data for Patterns",
    "section": "Activity",
    "text": "Activity\n\nPatterns are the essence of data exploration and our eyes’ ability to pick them out is integral to data understanding. Much of the data we work with, however, do not have a natural form and we need to make decisions about how they are to be represented. Try different ways to visualize the datasets so meaningful patterns may be found.\n\n\nGenetic profiles of cancer\nThese datasets contains 10 cancer samples. Table 1 describes the mutational status for a set of genes (A-E) and whether a mutation if absent (0) or present (1). Table 2 summarizes the expression levels of those genes, ranging from no expression (0) to high expression (3).\n\n\n\nTable 1: Mutational status for a set of genes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample -&gt;\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nGene A\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nGene B\n0\n0\n0\n0\n1\n1\n1\n0\n1\n1\n\n\nGene C\n0\n0\n1\n0\n0\n0\n1\n1\n1\n1\n\n\nGene D\n1\n1\n0\n0\n1\n1\n0\n0\n0\n0\n\n\nGene E\n0\n1\n1\n0\n1\n0\n0\n0\n1\n0\n\n\n\n\n\n\n\n\n\nTable 2: Expression levels for a set of genes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample -&gt;\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nGene A\n2\n1\n1\n2\n2\n0\n2\n1\n1\n2\n\n\nGene B\n1\n1\n2\n1\n0\n0\n0\n2\n0\n0\n\n\nGene C\n1\n1\n3\n1\n2\n2\n3\n0\n3\n0\n\n\nGene D\n0\n0\n2\n1\n3\n3\n2\n1\n1\n1\n\n\nGene E\n1\n3\n3\n1\n3\n1\n2\n1\n3\n2\n\n\n\n\n\n\n\n\n          1. Think about the problem on your own for 5 minutes.\n          2. In your groups, discuss and create different visualizations to highlight underlying patterns\n          3. Summarize the group’s approach\n          4. Elect/volunteer a spokesperson to present the solution\n\n\nConsider the following concepts when creating your visualizations\n\n\n\n\nPatterns\nPatterns are the essence of data exploration. What kinds of representation will produce the most meaningful insights?\n   \n\n\nEncodings\nSome visual estimations are easier to make than others. How might you use encodings that are less accurate but otherwise better at conveying overall trends?\n  \n\n\n\n\nColor\nColor is a powerful encoding that presents several challenges. Have you chosen a color scale that is optimal for that data type?\n   \n\n\nSalience and Relevance\nPop-out effects enable quick recognition. Are the most noticeable elements of your visualizations also the most relevant?"
  },
  {
    "objectID": "materials/3-workshop3/7-best-practices-and-send-off/slides.html#goals-for-this-workshop",
    "href": "materials/3-workshop3/7-best-practices-and-send-off/slides.html#goals-for-this-workshop",
    "title": "Workshop summary and path forward",
    "section": "Goals for this workshop",
    "text": "Goals for this workshop\nWhat happens to the instances?\nDiscussion of best practices\nWhat to do next?"
  },
  {
    "objectID": "materials/3-workshop3/7-best-practices-and-send-off/slides.html#practicalities",
    "href": "materials/3-workshop3/7-best-practices-and-send-off/slides.html#practicalities",
    "title": "Workshop summary and path forward",
    "section": "Practicalities",
    "text": "Practicalities\n\nWiFi:\nNetwork: KTB Free Wifi (no password needed)\nNetwork AHRI\nPassword: @hR1W1F1!17\nNetwork AHRI Internet only\nPassword: AHRI twenty three!\nNetwork CAPRISA-Corp Password: corp@caprisa17\nBathrooms are out the lobby to your left"
  },
  {
    "objectID": "materials/3-workshop3/7-best-practices-and-send-off/slides.html#lets-take-a-poll",
    "href": "materials/3-workshop3/7-best-practices-and-send-off/slides.html#lets-take-a-poll",
    "title": "Workshop summary and path forward",
    "section": "Let’s take a poll",
    "text": "Let’s take a poll\nGo to the event on wooclap\n\n\n\n\nback to module"
  },
  {
    "objectID": "past_workshops.html",
    "href": "past_workshops.html",
    "title": "Data Science for Biology Workshop Series",
    "section": "",
    "text": "Workshop 1: October 16th-20th 2023 at UKZN School of Medicine in Durban, SA"
  },
  {
    "objectID": "past_workshops.html#workshop-i---r-rstudio-and-quarto-for-clinical-research",
    "href": "past_workshops.html#workshop-i---r-rstudio-and-quarto-for-clinical-research",
    "title": "Data Science for Biology Workshop Series",
    "section": "Workshop I - R, RStudio, and Quarto for clinical research",
    "text": "Workshop I - R, RStudio, and Quarto for clinical research\nThank you for participating in the workshop!\nOur week together was amazing, thank you so much for making the workshop a success by participating all week long.\n\nIf you haven’t already, please fill out this survey\nIf you have any photos of the workshop you’d like to share, go to #photos on the workshop discord\nPlease post on Discord or email us if you have any questions or comments!\n\n\n\n\n\nTable 1: Schedule\n\n\n\n\n\n\n\n\n\n\nDate/Time\nTopic\nInstructor\n\n\n\n\nMonday, October 16th\n–\n–\n\n\nAM\nCourse Introduction & Introduction to data types\nSalina Hussain and Suuba Demby (Ragon Institute)\n\n\nPM\nIntro to R, Rstudio, and Quarto\nChandani Desai and Joseph Elsherbini (Ragon Institute)\n\n\nTuesday, October 17th\n–\n–\n\n\nAM\nIntro to data visualizaion with the tidyverse\nScott Handley (Wash U School of Medicine)\n\n\nPM\nData wrangling and more data visualization\nJoseph Elsherbini (Ragon Institute)\n\n\nWednesday, October 18th\n–\n–\n\n\nAM\ntableone and statistics for clinical data\nMarothi Letsoalo (CAPRISA)\n\n\nPM\nSmall group worktime\nParticipants\n\n\nThursday, October 19th\n–\n–\n\n\nAM\nHypothesis testing and data transformations\nLaura Symul (UCLouvain)\n\n\nPM\nSmall group worktime\nParticipants\n\n\nEvening\nReception\n\n\n\nFriday, October 20th\n–\n–\n\n\nAM\nKeynote\nLenine Liebenberg (CERI Stellenbosch)\n\n\n\nSmall group work time\nParticipants\n\n\nPM\nSmall group presentations"
  }
]